[{"content":"引子 经过一周的 vibe coding 之后，有些感悟，随便聊聊。\n这周的 vibe coding 中我一开始计划着尽量以一个完全小白的角度去进行，即只告诉 AI 自己想要什么，对话过程中出现的一切有关于代码或者文档的修改全盘接受。\n算是一种放养式编程。一周下来我也是这么做的。最终开发出了一个蛮符合我预期的小工具并进行了上线。\n当然现在看来这个小工具功能是挺齐全的，但是里面是否做到了优雅高效的开发我并不清楚，并且最终上线的过程也遇到了些错误导致出现了种种不顺利。这些都是我发现的算是不足之处吧。\n接下来从几个方面聊聊当前的 vibe coding:\n模型、中转商 如何编码 带来什么影响 模型\u0026amp;中转商 虽然有 Cursor、Augment 珠玉在前，但是由于我个人的原因都没有尝试过，相当长的一段时间里我所谓的 vibe coding 还处在使用 chatbot 例如 Gemini2.5Pro 或者 GPT 等对话模型进行一问一答式地交流。\n直到某天我在 L 站中看到有很多关于 Claude Code（以下简称 CC） 的推广贴，点进去发现是做 CC 的中转商的抽奖，于是乎我也参与了一下，没想到在银河录像局这里中奖了，那我就抱着试一试的心态用了用。\n当时抽中了 2000 积分，估计也就是个 20 刀左右的样子吧。 我拿到自己的项目中让 CC 进行了一系列的总结和梳理，得到的内容有模有样，虽然带着一股明显的 AI 味，但是内容的准确性确实很高。\n这让我觉得很好玩，既然这么好玩，不如多尝试一下？于是找了几家中转商，发现 DuckCoding 这里比较便宜，暂时使用了 DuckCoding.\n写到这里，我还是忍不住比较一下各家中转商的价格，就我用的感受来讲，Duck 这里是最便宜的，但是不保证稳定，有时候会因为号池的问题断掉（这也是用的人比较多的一种侧面反应吧）；Packy 价格稍贵，但是胜在稳定，并且限流开放购买；Fox 我还在使用中，听说最近降价了我才购入的，等我试用一阵子再来评价。\n关于模型，CC 一开始当仁不让是最好用的模型，但是中途经历过一次大规模的降智和 A 社对于中国的歧视性公告，导致之后的使用体验并没有之前那样顺滑。\n恰好 Codex(GPT)在此时出现，GPT5-high 模型带来的更高的准确性（特别是在调 bug 方面），甚至是更少的 token 消耗：给我的感觉与 CC 的 opus 相近的能力但更少的花费；都让 Codex 成为我们争相使用的对象。\n","permalink":"http://localhost:1313/posts/051vibecoding/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e经过一周的 vibe coding 之后，有些感悟，随便聊聊。\u003c/p\u003e\n\u003cp\u003e这周的 vibe coding 中我一开始计划着尽量以一个完全小白的角度去进行，即只告诉 AI 自己想要什么，对话过程中出现的一切有关于代码或者文档的修改全盘接受。\u003c/p\u003e","title":"051vibeCoding"},{"content":"","permalink":"http://localhost:1313/posts/050lt%E7%9A%84%E5%A5%87%E6%80%9D%E4%B9%B1%E6%83%B3ep04%E5%85%B3%E4%BA%8E%E7%A7%8B%E6%8B%9B/","summary":"","title":"050LT的奇思乱想EP04——关于秋招"},{"content":"引子 在之前的部署私有镜像仓库项目中，我们遇到过一次跨域请求问题，在 nginx 中添加了有关 CORS 的限制内容，那么这篇文章就来填个坑，梳理一下跨域的原理。\n从下面这段简单的跨域中间件代码开始吧\n// CORSMiddleware 跨域中间件 func CORSMiddleware() gin.HandlerFunc { return func(c *gin.Context) { c.Writer.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) c.Writer.Header().Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;GET, POST, OPTIONS\u0026#34;) c.Writer.Header().Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Origin, Content-Type, Content-Length, Accept-Encoding, X-CSRF-Token, Authorization\u0026#34;) if c.Request.Method == \u0026#34;OPTIONS\u0026#34; { c.AbortWithStatus(204) return } c.Next() } } 为什么会有跨域限制 CORS 全称是跨域资源共享，它是一个设定好的标准，起源于浏览器实现的一项安全策略——同源策略。\n两个 URL 的协议、域名、端口必须完全相同 例如 http://example.com:8080和http://example.com:8080/users是同源的 那么他限制了什么呢？不同源的脚本不能直接读取或者操作另一个源的 DOM 或者网络请求。\n目的呢？防止 **CSRF（跨站请求伪造）和 XSS（跨站脚本攻击）**等。\nCSRF 拿一个 AI 的举例来看：\n假设登录了银行网站并且登录状态存在了 cookie 中，此时如果浏览另一个恶意网站，这个网站页面包含了一行访问银行网站并取钱的请求。\n浏览器就会发送这个请求带上以之前的 cookie 内容，这样银行就会把钱转给攻击者。\n所以浏览器就会保证同源访问策略，有了同源访问策略，恶意网站就拿不到银行网站的 cookie 信息了，自然也无法取到钱。\n实际上还使用到了 X-CSRF-Token 来防护 CSRF，这个不是今天的重点。\n问题 这是一个很好地策略，为什么我们还需要来进行跨域呢？因为在日常开发中，前后端分离环境下，前端和后端开发出的内容并不是同源的，所以需要跨域；\n又或者在微服务架构中，服务 A 调用服务 B 的 API，二者不同源的话，需要在 B 的网关处配置 CORS 策略，允许服务 A 的域名。\n而既然要跨域，那必须得保证安全，所以 CORS 就允许服务器主动声明哪些外部源可以访问它的资源，这样既能够保证安全又能够放宽同源策略。\n主要是通过 HTTP 响应头来告知。\n分类 具体来说，服务器通过设置一系列Access-Control-*开头的响应头，告知浏览器，“我允许这个源，用这种方法，带这些头信息来访问我”，否则拦截响应。\n回到上面的代码中，我们一行一行来看\nc.Writer.Header().Set(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;) 设置允许访问的源为全部，这里是开发环境下，生产环境更加严格。\nc.Writer.Header().Set(\u0026quot;Access-Control-Allow-Methods\u0026quot;, \u0026quot;GET, POST, OPTIONS\u0026quot;)设置允许的请求方法。\nc.Writer.Header().Set(\u0026quot;Access-Control-Allow-Headers\u0026quot;, \u0026quot;Origin, Content-Type, ... Authorization\u0026quot;)允许浏览器的实际请求允许带哪些自定义的请求头。\nif c.Request.Method == \u0026#34;OPTIONS\u0026#34; { c.AbortWithStatus(204) return } 处理预检请求。\n这里我们就要对 CORS 请求进行分类：简单请求+预检请求。\n简单请求：例如一个简单的 GET，POST,且 Content-Type 为 text/plain、multipart/form-data、application/x-www-form-urlencoded 预检请求(OPTIONS)：PUT，DELETE，PATCH，Content-Type为application/json，带有自定义的请求头等。 本中间件的预检流程 浏览器向服务器发送一个OPTIONS请求。这个请求会带上两个特殊的头：\nAccess-Control-Request-Method: 实际请求将要使用的方法 (例如: POST)。 Access-Control-Request-Headers: 实际请求将要携带的自定义头 (例如: Authorization, Content-Type) 服务器捕获预检请求：\n服务器在响应中设置好上面提到的 Allow-Methods 和 Allow-Headers 等许可头 c.AbortWithStatus(204):服务器返回一个 204 No Content 的状态码，表示“许可信息已在头部，没有正文内容”。Abort会中断后续的中间件和处理函数，因为这只是一个许可问询，不需要执行真正的业务逻辑 浏览器 (决策)：收到204响应后，检查响应头里的许可信息。\n如果当前请求的方法和头部都在许可范围内，浏览器就会发送真正的、实际的业务请求（比如那个POST请求） 否则，就在控制台报CORS错误 总结 所以说到这里，跨域资源保护很简单，核心就是从服务器的角度出发，让其规定谁可以如何访问它的资源，保证安全的同时做到跨域访问。\n最后再回顾一下构建私有镜像仓库时我们用到了命令,其中就指定了具体的源可以访问。\ndocker run -d \\ --name registry \\ -p 5000:5000 \\ -v /etc/docker/registry/auth:/auth \\ -v /var/lib/registry:/var/lib/registry \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \\ -e REGISTRY_AUTH=htpasswd \\ -e REGISTRY_AUTH_HTPASSWD_REALM=\u0026#34;Registry Realm\u0026#34; \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin=\u0026#39;[\u0026#34;https://registryui.bfsmlt.top\u0026#34;]\u0026#39; \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods=\u0026#39;[\u0026#34;GET\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;OPTIONS\u0026#34;]\u0026#39; \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers=\u0026#39;[\u0026#34;Authorization\u0026#34;, \u0026#34;Accept\u0026#34;, \u0026#34;Cache-Control\u0026#34;, \u0026#34;Content-Type\u0026#34;]\u0026#39; \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials=\u0026#39;[\u0026#34;true\u0026#34;]\u0026#39; \\ --restart=always \\ registry:2 在实际后端开发中，通常会从配置文件或者环境变量中加载allowedOrigins列表。\n而 CORS 通常也会和一些认证机制 JWT，OAuth2 结合，下一篇文章我打算谈谈 JWT 等认证机制。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/049cors/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在之前的部署私有镜像仓库项目中，我们遇到过一次跨域请求问题，在 nginx 中添加了有关 CORS 的限制内容，那么这篇文章就来填个坑，梳理一下跨域的原理。\u003c/p\u003e\n\u003cp\u003e从下面这段简单的跨域中间件代码开始吧\u003c/p\u003e","title":"兴趣八股之计算机网络EP03——CORS浅探"},{"content":"引子 记录 TCP握手挥手过程 sudo tcpdump -i lo -S -v tcp port 7070\nsudo tcpdump -i lo -S -v tcp port 7070\ncurl http://127.0.0.1:7070\nsudo tcpdump -i lo -S -v tcp port 7070 tcpdump: listening on lo, link-type EN10MB (Ethernet), snapshot length 262144 bytes 18:39:31.097639 IP (tos 0x0, ttl 64, id 19239, offset 0, flags [DF], proto TCP (6), length 60) localhost.46136 \u0026gt; localhost.7070: Flags [S], cksum 0xfe30 (incorrect -\u0026gt; 0xe79e), seq 3564383388, win 65495, options [mss 65495,sackOK,TS val 3523724659 ecr 0,nop,wscale 7], length 0 18:39:31.097671 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60) localhost.7070 \u0026gt; localhost.46136: Flags [S.], cksum 0xfe30 (incorrect -\u0026gt; 0xa381), seq 2807956798, ack 3564383389, win 65483, options [mss 65495,sackOK,TS val 3523724659 ecr 3523724659,nop,wscale 7], length 0 18:39:31.097702 IP (tos 0x0, ttl 64, id 19240, offset 0, flags [DF], proto TCP (6), length 52) localhost.46136 \u0026gt; localhost.7070: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xca3d), ack 2807956799, win 512, options [nop,nop,TS val 3523724659 ecr 3523724659], length 0 18:39:31.097927 IP (tos 0x0, ttl 64, id 19241, offset 0, flags [DF], proto TCP (6), length 129) localhost.46136 \u0026gt; localhost.7070: Flags [P.], cksum 0xfe75 (incorrect -\u0026gt; 0x6e9b), seq 3564383389:3564383466, ack 2807956799, win 512, options [nop,nop,TS val 3523724659 ecr 3523724659], length 77 18:39:31.097956 IP (tos 0x0, ttl 64, id 46227, offset 0, flags [DF], proto TCP (6), length 52) localhost.7070 \u0026gt; localhost.46136: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xc9f1), ack 3564383466, win 511, options [nop,nop,TS val 3523724659 ecr 3523724659], length 0 18:40:32.082157 IP (tos 0x0, ttl 64, id 19242, offset 0, flags [DF], proto TCP (6), length 52) localhost.46136 \u0026gt; localhost.7070: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xdbb7), ack 2807956799, win 512, options [nop,nop,TS val 3523785644 ecr 3523724659], length 0 18:40:32.082178 IP (tos 0x0, ttl 64, id 46228, offset 0, flags [DF], proto TCP (6), length 52) localhost.7070 \u0026gt; localhost.46136: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xdbb6), ack 3564383466, win 512, options [nop,nop,TS val 3523785644 ecr 3523724659], length 0 18:41:33.523205 IP (tos 0x0, ttl 64, id 19243, offset 0, flags [DF], proto TCP (6), length 52) localhost.46136 \u0026gt; localhost.7070: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xfd7b), ack 2807956799, win 512, options [nop,nop,TS val 3523847085 ecr 3523785644], length 0 18:41:33.523229 IP (tos 0x0, ttl 64, id 46229, offset 0, flags [DF], proto TCP (6), length 52) localhost.7070 \u0026gt; localhost.46136: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xebb4), ack 3564383466, win 512, options [nop,nop,TS val 3523847085 ecr 3523724659], length 0 18:41:35.723669 IP (tos 0x0, ttl 64, id 19244, offset 0, flags [DF], proto TCP (6), length 52) localhost.46136 \u0026gt; localhost.7070: Flags [F.], cksum 0xfe28 (incorrect -\u0026gt; 0x04e0), seq 3564383466, ack 2807956799, win 512, options [nop,nop,TS val 3523849285 ecr 3523847085], length 0 18:41:35.723808 IP (tos 0x0, ttl 64, id 46230, offset 0, flags [DF], proto TCP (6), length 52) localhost.7070 \u0026gt; localhost.46136: Flags [F.], cksum 0xfe28 (incorrect -\u0026gt; 0xfc46), seq 2807956799, ack 3564383467, win 512, options [nop,nop,TS val 3523849285 ecr 3523849285], length 0 18:41:35.723868 IP (tos 0x0, ttl 64, id 19245, offset 0, flags [DF], proto TCP (6), length 52) localhost.46136 \u0026gt; localhost.7070: Flags [.], cksum 0xfe28 (incorrect -\u0026gt; 0xfc46), ack 2807956800, win 512, options [nop,nop,TS val 3523849285 ecr 3523849285], length 0 ^C 12 packets captured 24 packets received by filter 0 packets dropped by kernel 这是一次抓包所产生的结果，分析如下：\n三次握手：\nFlags [S] 代表着 SYN，请求建立连接，第一次握手 Flags [S.]多了一个.,这个点就是 ACK 的简写，表示 SYN+ACK,第二次握手；且注意 ack= seq+1 Flags [.]只有 ACK 的确认报文，第三次握手 传输数据：\nFlags [P.]代表 Push，seq x:y从x 开始到 y 长度为 77 字节数据 之后服务器回复 ACK 确认，等于y 表示完美匹配 Keep Alive:\n定时发送一些空的 ACK 包探测包，确认在线 四次挥手:\nFlags[F.] 代表着 Fin，第一次挥手 当服务端也没有数据要发送时，可以直接把 ACK 和 FIN 一起发出去，合并了二三次挥手 第四次挥手结束 HTTPS过程 在服务端显式:\nCertificate chain：服务器发来的证书链 Server certificate: 证书详细内容（server.crt) SSL-session Protocal:TLSv1.3 Cipher:协商的加密套件 AES Session-ID Broker(Server):\ninitial high watermark 0 high watermark 17 ","permalink":"http://localhost:1313/posts/048%E8%B7%9F%E7%9D%80ai%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003ch2 id=\"记录\"\u003e记录\u003c/h2\u003e\n\u003ch3 id=\"tcp握手挥手过程\"\u003eTCP握手挥手过程\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003esudo tcpdump -i lo -S -v tcp port 7070\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo tcpdump -i lo -S -v tcp port 7070\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecurl http://127.0.0.1:7070\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo tcpdump -i lo -S -v tcp port \u003cspan class=\"m\"\u003e7070\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etcpdump: listening on lo, link-type EN10MB \u003cspan class=\"o\"\u003e(\u003c/span\u003eEthernet\u003cspan class=\"o\"\u003e)\u003c/span\u003e, snapshot length \u003cspan class=\"m\"\u003e262144\u003c/span\u003e bytes\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:39:31.097639 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19239, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 60\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eS\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe30 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xe79e\u003cspan class=\"o\"\u003e)\u003c/span\u003e, seq 3564383388, win 65495, options \u003cspan class=\"o\"\u003e[\u003c/span\u003emss 65495,sackOK,TS val \u003cspan class=\"m\"\u003e3523724659\u003c/span\u003e ecr 0,nop,wscale 7\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:39:31.097671 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 0, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 60\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.7070 \u0026gt; localhost.46136: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eS.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe30 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xa381\u003cspan class=\"o\"\u003e)\u003c/span\u003e, seq 2807956798, ack 3564383389, win 65483, options \u003cspan class=\"o\"\u003e[\u003c/span\u003emss 65495,sackOK,TS val \u003cspan class=\"m\"\u003e3523724659\u003c/span\u003e ecr 3523724659,nop,wscale 7\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:39:31.097702 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19240, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xca3d\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 2807956799, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523724659\u003c/span\u003e ecr 3523724659\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:39:31.097927 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19241, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 129\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eP.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe75 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0x6e9b\u003cspan class=\"o\"\u003e)\u003c/span\u003e, seq 3564383389:3564383466, ack 2807956799, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523724659\u003c/span\u003e ecr 3523724659\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e77\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:39:31.097956 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 46227, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.7070 \u0026gt; localhost.46136: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xc9f1\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 3564383466, win 511, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523724659\u003c/span\u003e ecr 3523724659\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:40:32.082157 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19242, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xdbb7\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 2807956799, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523785644\u003c/span\u003e ecr 3523724659\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:40:32.082178 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 46228, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.7070 \u0026gt; localhost.46136: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xdbb6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 3564383466, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523785644\u003c/span\u003e ecr 3523724659\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:41:33.523205 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19243, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xfd7b\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 2807956799, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523847085\u003c/span\u003e ecr 3523785644\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:41:33.523229 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 46229, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.7070 \u0026gt; localhost.46136: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xebb4\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 3564383466, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523847085\u003c/span\u003e ecr 3523724659\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:41:35.723669 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19244, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eF.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0x04e0\u003cspan class=\"o\"\u003e)\u003c/span\u003e, seq 3564383466, ack 2807956799, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523849285\u003c/span\u003e ecr 3523847085\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:41:35.723808 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 46230, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.7070 \u0026gt; localhost.46136: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eF.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xfc46\u003cspan class=\"o\"\u003e)\u003c/span\u003e, seq 2807956799, ack 3564383467, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523849285\u003c/span\u003e ecr 3523849285\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18:41:35.723868 IP \u003cspan class=\"o\"\u003e(\u003c/span\u003etos 0x0, ttl 64, id 19245, offset 0, flags \u003cspan class=\"o\"\u003e[\u003c/span\u003eDF\u003cspan class=\"o\"\u003e]\u003c/span\u003e, proto TCP \u003cspan class=\"o\"\u003e(\u003c/span\u003e6\u003cspan class=\"o\"\u003e)\u003c/span\u003e, length 52\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    localhost.46136 \u0026gt; localhost.7070: Flags \u003cspan class=\"o\"\u003e[\u003c/span\u003e.\u003cspan class=\"o\"\u003e]\u003c/span\u003e, cksum 0xfe28 \u003cspan class=\"o\"\u003e(\u003c/span\u003eincorrect -\u0026gt; 0xfc46\u003cspan class=\"o\"\u003e)\u003c/span\u003e, ack 2807956800, win 512, options \u003cspan class=\"o\"\u003e[\u003c/span\u003enop,nop,TS val \u003cspan class=\"m\"\u003e3523849285\u003c/span\u003e ecr 3523849285\u003cspan class=\"o\"\u003e]\u003c/span\u003e, length \u003cspan class=\"m\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e^C\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"m\"\u003e12\u003c/span\u003e packets captured\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"m\"\u003e24\u003c/span\u003e packets received by filter\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e packets dropped by kernel\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这是一次抓包所产生的结果，分析如下：\u003c/p\u003e","title":"048跟着AI学习计算机网络"},{"content":"引子 在使用私有镜像仓库时突然出现了认证问题，跟着 AI 来回改，在此记录一下改的过程。\n首先回顾一下之前的那篇构建私有镜像仓库的文章,我们建立了两个容器，registry用来存放镜像,registry-UI用来给前端显示。\n同时在服务器上使用 nginx 进行了反向代理，对浏览器传来的 https 请求进行处理\n两个80 端口用作 Let’s Encrypt 的 HTTP 验证 两个443 端口：一个用作 Registry,转发到本地:5000端口;一个用作 UI 界面，转发到本地的:8080端口，这两个本地端口其实都是容器服务所暴露的端口。 错误过程 CORS 一开始访问前端 UI 界面，显示了红色错误，显示Access-Control-Allow-Origin header must be set to https://registryui.bfsmlt.top\n这是一个 CORS 问题,为什么呢？因为浏览器前端页面https://registryui.bfsmit.top,会使用 js 向镜像仓库后端发送请求。\n而镜像后端在https://jimlt.bfsmlt.top上，二者不同源。\n关于Cross-Origin Requests,简单描述一下这个概念:\n协议，端口，域名三者有一个不同就不是同源 浏览器默认采用同源策略，防止不同源的网站读取当前网站的敏感信息 在 CORS 下允许服务器声明哪些源可以访问其资源 令我惊讶的是之前使用中一直没有发现问题，而且我在之前那篇文章中在启动 registry 容器时也已经指定了REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin等相关参数,按理来说应该不会出现 CORS 问题。\n说到这里我觉得有必要梳理一下整个流程\n浏览器访问https://registryui.bfsmlt.top 服务器中的 nginx 将这个请求转发给 ui 容器，即http://localhost:8080 容器返回给浏览器所需的 HTML,CSS,JS；浏览器借此组成界面 浏览器中 JS 根据容器设定的参数会去访问 https://jimlt.bfsmlt.top/v2/_catalog来获取镜像列表 而此时浏览器发现当前网页是在https://registryui.bfsmlt.top加载的，与要访问的https://jimlt.bfsmlt.top/v2/_catalog不同源;发生 CORS 问题 浏览器会发送一个 OPTIONS 预检请求，询问jimlt.bfsmlt.top 是否同意这次跨域请求 所以关键就在这里了，可能我们的容器运行时参数不够，得去修改 nginx,让jimlt.bfsmlt.top允许这次跨域请求。\n为 jimlt.bfsmlt.top 的 server 区域添加如下的预检请求，本质就是增加头部信息。\n# 处理浏览器的预检请求 if ($request_method = \u0026#39;OPTIONS\u0026#39;) { add_header \u0026#39;Access-Control-Allow-Origin\u0026#39; \u0026#39;https://registryui.bfsmlt.top\u0026#39; always; add_header \u0026#39;Access-Control-Allow-Methods\u0026#39; \u0026#39;HEAD, GET, OPTIONS, DELETE, PUT\u0026#39; always; add_header \u0026#39;Access-Control-Allow-Headers\u0026#39; \u0026#39;Authorization, Content-Type, Accept\u0026#39; always; add_header \u0026#39;Access-Control-Allow-Credentials\u0026#39; \u0026#39;true\u0026#39; always; add_header \u0026#39;Access-Control-Max-Age\u0026#39; 1728000; add_header \u0026#39;Content-Type\u0026#39; \u0026#39;text/plain; charset=utf-8\u0026#39;; add_header \u0026#39;Content-Length\u0026#39; 0; return 204; } # 为实际的 API 也加上标头 add_header \u0026#39;Access-Control-Allow-Origin\u0026#39; \u0026#39;https://registryui.bfsmlt.top\u0026#39; always; add_header \u0026#39;Access-Control-Allow-Credentials\u0026#39; \u0026#39;true\u0026#39; always; 502 Bad Gateway 这样修改之后又出现了新问题\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;502 Bad Gateway\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;502 Bad Gateway\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\n这意味着 Nginx 接收到了浏览器的请求，但是将请求转发给 registry 容器时失败了。\n我试着直接访问这个容器curl -I http://127.0.0.1:5000/v2/,显示无法连接。\n于是重启 docker 服务，再次尝试时不再是 502 问题了。终端出现认证问题是 401，因为我们之前设置过密码；\n但是，令人不解的是，浏览器访问页面时又出现了 CORS 问题。\n试着去掉缓存，去掉 cookie，不起作用。问题在于，此时浏览器应该给我出现填写用户名和密码的表单才对。\n更神奇的来了，我直接访问https://jimlt.bfsmlt.top/v2/_catalog成功了，可以看到镜像列表。\n所以我觉得还是从浏览器访问registry容器的这个过程出现了问题。\nUI升级 反思一下，当时我参考的自建镜像仓库的文章来源于 2020年的一篇博客，这个 registryUI 在 2021年进行了一次较大的改动。\n所以来到了改动页面，看看我们需要改什么参数。\n可以发现最大的变化在于以下几点:\nREGISTRY_URL 改为了 NGINX_PROXY_PASS_URL URL 改为了 REGISTRY_URL 从旧版本迁移时，SINGEL_REGISTRY = true 其中NGINX_PROXY_PASS_URL就是为了处理可能出现的 CORS 问题，这个字段表示 UI 容器内部的反向代理，至于到底发生了什么，见下方的最终修改。\n最终修改 到了这里，我感觉到应该是 UI 的版本更替问题，因为在之前使用过程中，好像出现过更新的提示。\n所以我决定将 registry 仓库和 UI 全部进行重建，并将他们放在同一个 dockerCompose 里面进行操作，这样可以充分利用 compose 的便利性。\n由于卷挂载的缘故，重建过程并不会影响之前已经存在的镜像和 TLS 信息 dokcer-compose 文件内容如下：\nversion: \u0026#39;3.8\u0026#39; services: # 後端 Registry 服務 registry: image: registry:2 container_name: registry restart: always ports: - \u0026#34;5000:5000\u0026#34; volumes: - /var/lib/registry:/var/lib/registry - /etc/docker/registry/auth:/auth # 如果您的憑證也想在這裡管理，也可以加上，但目前您的 Nginx 在使用，保持現狀即可。 # - /etc/docker/registry/certs:/certs environment: REGISTRY_HTTP_ADDR: 0.0.0.0:5000 REGISTRY_AUTH: htpasswd REGISTRY_AUTH_HTPASSWD_REALM: \u0026#34;Registry Realm\u0026#34; REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd networks: - registry-net # 前端 UI 服務 (使用我們最終確定的 v2 版本配置) registry-ui: image: joxit/docker-registry-ui:main container_name: registry-ui restart: always ports: - \u0026#34;8080:80\u0026#34; environment: SINGLE_REGISTRY: \u0026#34;true\u0026#34; NGINX_PROXY_PASS_URL: \u0026#34;http://registry:5000\u0026#34; REGISTRY_TITLE: \u0026#34;My Private Registry\u0026#34; DELETE_IMAGES: \u0026#34;true\u0026#34; depends_on: - registry networks: - registry-net networks: registry-net: driver: bridge 可以看到在 UI 服务中这两个参数SINGLE_REGISTRY: \u0026quot;true\u0026quot;,NGINX_PROXY_PASS_URL: \u0026quot;http://registry:5000\u0026quot;\nJS的请求此时不会直接打到 registry 容器，而是访问 UI 容器GET https://registryui.bfsmlt.top/v2/_catalog UI 容器的内部 nginx 会将发来的请求反向代理到 http://registry:5000 而 registry 正是镜像容器的服务名称，通过 bridge 网络结构直接发送了过来。 最终后端容器接收请求，处理后再通过原路返回给前端。 注意，这个过程就不会出现 CORS 问题，因为SINGLE_REGISTRY: \u0026quot;true\u0026quot;的缘故，浏览器访问的还是同源下的路径，到了我们的服务器 Docker 环境中才转换了访问。\n这里用到了 compose 中的 bridge网络结构，利用了其内建 DNS 的特性——将服务名作为主机名来访问容器。 最后成功在浏览器进行访问，获取了镜像列表。\nGithub issues 一开始遇到问题询问 AI 我觉得无可厚非，但是在尝试过程中发现可能是 UI 出现问题，早该去对应的仓库中找找有没有类似的问题可以参考。\n找到了最近的相关 issue，可以发现就是版本更新带来的参数定义重置问题。\n并且在 README 中，作者也提到了这个问题，他的建议是\nI suggest to have your UI on the same domain than your registry e.g. registry.example.com/ui/ or use NGINX_PROXY_PASS_URL or configure a nginx/apache/haproxy in front of your registry that returns 200 on each OPTIONS requests.\n不要跨域 设置NGINX_PROXY_PASS_URL参数 设置nginx/apache/haproxy 同时他也提到了如果不进行设置会导致 CORS 的最终原因:\nThis is caused by a bug in docker registry, it returns 401 status requests on preflight requests, this breaks W3C preflight-request specification.\n即 registry 在收到预检请求 Options 后不会进行预检，还是会强调身份认证，所以预检失败，自然触发 CORS.\n总结 有趣的一次经历，巩固了 nginx 和 CORS 的相关知识。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/047%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E5%87%BA%E9%97%AE%E9%A2%98%E5%95%A6/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在使用私有镜像仓库时突然出现了认证问题，跟着 AI 来回改，在此记录一下改的过程。\u003c/p\u003e\n\u003cp\u003e首先回顾一下之前的那篇构建私有镜像仓库的文章,我们建立了两个容器，\u003ccode\u003eregistry\u003c/code\u003e用来存放镜像,\u003ccode\u003eregistry-UI\u003c/code\u003e用来给前端显示。\u003c/p\u003e","title":"DebugEP02_私有镜像出问题啦"},{"content":" Value copy In GO,everything we assign is a copy.\n变量的值到底是什么？\n值类型，string,array,struct,是数据本身； 引用类型，slice,map,channel,pointer,function,是数据的引用——但仍是值拷贝，只是这个值中带着地址。 所以本质上二者都是值拷贝。\n值拷贝的意义：\n值传递意味着数据可以被分配到栈上，函数返回时，栈上的数据立即销毁，无需 GC； 指针数据分配在堆上，由 GC 管理 要践行通过通信共享内存，而不是共享内存来通信，传递值的副本可以避免数据竞争。 遍历一个很大的 slice 时，除了直接使用索引以外，还可以将 slice 变为指针类型，避免拷贝开销。（但是遍历指针类型对 CPU 来说效率低）\n循环控制器的求值时机 The provided expression is evaluated only once,before the beginning of the loop.\n只在循环开始前求一次值，之后会使用其副本进行迭代。\ns := []int{0, 1, 2} for range s { // 不需要索引和值，只要按照长度循环相应的次数即可。 s = append(s, 10) } 先进行拷贝，得到的s_copy作为循环控制器 后续操作，s 在不断变化，但是循环控制器始终不变，一直是开始的那个拷贝副本，所以只会执行三次。 为什么要有一个始终不变的循环控制器呢？\n为了保证循环的可预测性，防止意外的无限循环,减少对应的性能开销。\n举一个无限循环的错误示范：\ns := []int{0,1,2} for i := 0; i \u0026lt; len(s); i++ { s = append(s, 10) } 这会导致无限循环，因为 s 的长度一直在增长。\nchannels 和 Arrays 也有类似的行为，具体见下方所示：\nChannels 同样的，在处理多个 channel 时也会遇到类似问题。\nch1 := make(chan int, 3) go func() { ch1 \u0026lt;- 0 ch1 \u0026lt;- 1 ch1 \u0026lt;- 2 close(ch1) }() ch2 := make(chan int, 3) go func() { ch2 \u0026lt;- 10 ch2 \u0026lt;- 11 ch2 \u0026lt;- 12 close(ch2) }() ch := ch1 for v := range ch { fmt.Println(v) ch = ch2 } 这里打印出的 v 一直是 ch1，因为只在开始时确定一次。\n一开始就确定了一个循环控制器，这样以后在 range 中修改也不会修改定住的这个控制器。\n日常开发中通常会采用 select 语句来处理多个动态数据。\nArray a := [3]int{0,1,2} for i, v := range a { a[2] = 10 if i == 2 { fmt.Println(v) } } 这段代码最终会输出 2 还是 10 呢？答案是 2，循环中的 v 来自于拷贝，而只有a[i]访问的是原始的数组。\n与最开始的 slice 类似，都有拷贝作为循环控制器:\n一开始求值得到一个拷贝副本值a_copy，接下来的循环都将在a_copy上进行 i,v 都来源于a_copy 明确使用变量名a[2]才是使用原数组 pointer 当结构体较大时，建议使用指针存入 map，否则会带来两个严重的后果：\n只是操作副本 大的结构体在 map 存取过程中会涉及到数据拷贝 所以使用指针 mapPointer map[string]*LargeStruct,可以直接对结构体内的属性值进行操作mapPointer[id].foo = \u0026quot;bar\u0026quot;\n在 Go1.22 之前，以下代码会出现循环变量的复用问题:\nfor _, customer := range customers { s.m[customer.ID] = \u0026amp;customer } 循环变量的复用:customer只有一个实例，每次只操作这一个实例。 循环赋值 获取被复用的地址 如何解决这个问题：\n每次循环创建一个新的局部变量 通过索引获取原始切片的元素地址，而不是循环变量的地址 在 Go1.22 之后，修改了 for 循环的语义,让每次循环背后都做了一次局部变量声明操作：\ncustomer := customers[i] 这样就不会出现循环变量的复用问题了。\n在 Java 中 for-each 循环每次都是一个新的变量。\n对比 Java 可以看到，首先 Go 中一切皆值拷贝，而 Java，对象变量存的就是引用，拷贝的是引用的值——地址。\n在传参过程中，Java 传递的是引用的值（地址），这样不会带来拷贝的开销。\n","permalink":"http://localhost:1313/posts/046golangep07-range%E9%81%8D%E5%8E%86/","summary":"\u003ch2\u003e\u003c/h2\u003e\n\u003ch2 id=\"value-copy\"\u003eValue copy\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn GO,everything we assign is a copy.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e变量的值到底是什么？\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e值类型，\u003ccode\u003estring,array,struct\u003c/code\u003e,是数据本身；\u003c/li\u003e\n\u003cli\u003e引用类型，\u003ccode\u003eslice,map,channel,pointer,function\u003c/code\u003e,是数据的引用——但仍是值拷贝，只是这个值中带着地址。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e所以本质上二者都是值拷贝。\u003c/p\u003e","title":"GolangEP07-Range遍历(上)"},{"content":"引子 上一篇文章中留下了两个问题没有解决\nmap 如何扩容 为什么原生的 map 线程不安全 其实这两个问题是有联系的，所以这篇文章来解决这两个问题。\nmap 的扩容机制 本节还是需要用到上一篇文章中的底层 hmap 结构\ntype hmap struct { count int // map 中存储的元素个数 B uint8 // 当前 bucket 的数量为 2^B 个 buckets unsafe.Pointer // 指向 bucket 数组 oldbuckets unsafe.Pointer // 指向旧的 buckets（在扩容时使用） ... } 为什么扩容 装载因子超限（主要条件） 像 Java 中的 ArrayList 等结构，都有一个装载因子来限制结构的长度，当超过某个限制时就会发生扩容，Go 也不例外。 Go 硬编码（默认）的装载因子是 6.5；当 map.count / (2^B) \u0026gt; 6.5时，就会发生翻倍扩容。 举个例子，之前我们说一个 bucket默认存 8 个键值对，那么如果 map 此时已经存放了 14个键值对了，即14/2 \u0026gt; 6.5，那么两个 bucket 就不够了，为了减少哈希冲突，需要进行扩容。 溢出桶过多(次要条件) 上一篇提到的*overflows指针，就是指向溢出桶，这里先讲讲什么是溢出桶。\n之前提到过一个 bucket 容量默认为 8，那么当这个 bucket满了，而下一个 key又要进入这个 bucket 时该怎么办？\n创建一个新的 bmap(buckect) 将原来的 bucket 的 overflows 指针指向新的溢出桶 最终形成了一个单向链表 这虽然可以在严重的哈希冲突下保证正确的数据读取，但是将 map 的性能降低了，最坏情况下退化到链表查询，复杂度接近 N\n好了，那么话说回来，溢出桶过多为什么导致扩容？\n因为此时 Go 会认为哈希冲突非常严重，对性能产生了很大的影响，需要重新整理数据，让 key 重新分布，减少溢出桶的使用。具体的扩容规则如下：\n当B 小于 15 时，溢出桶数量超过了 $2^B$，触发扩容 当B 大于 15 时，数量超过 $2^15$时，触发扩容 如何扩容 扩容的核心是数据迁移，将旧的数据迁移到新的 bucket 中,并且这个过程是渐进式的，不是一蹴而就的。\n翻倍扩容 创建新的bucket B++ 分配一个两倍于旧 bucket 数组大小的新 bucket 数组。例如图中长度为 2 的数组变为 4，底层就会有 4 个 bucket hmap 结构中指向 buckets 的指针也要发生变化 渐进式迁移 真正的迁移发生在后续的 map 写入或删除操作中 每当读写一次map，就会顺便迁移 1～2 个旧的 bucket 中的数据到新的 bucket 中 新位置需要 rehash 读操作每次都会去新的 bucket 中查找，找不到再去旧的。这样可以保证每次读到的都是正确的。 迁移完成后，最终oldbuckets指向 nil 等量扩容 创建等量的新桶 B 不变，map 会分配一个和旧 bucket 数组同样大小的新 bucket 数组 渐进式迁移 创建新的哈希种子 将之前高度冲突的 key 重新散列 rehash 扩容带来的问题 The number of bucktes in a map cannot shrink\n扩容不是一劳永逸的，想象这样一个场景：\n你用 map 存储最近一个小时的用户数据，促销开始时百万级用户量的涌入，map 会疯狂扩容；当促销结束后用户逐渐离线，map 中删除了数据但是 map 底层的结构不会被删除，其在堆上的内存不会被回收。\n为什么？Go 的运行时认为，一个曾经达到过如此规模的 map，未来很可能再次需要这么大的容量，因此保留这部分内存以避免未来再次扩容的开销。\n那么如何解决这种潮汐的流量模式带来的问题呢？我们需要主动打破 map 的生命周期，让 GC 可以回收它。\n定时重建 启动 goroutine，使用定时器周期性地用新的小的 map 来替换旧的 map 旧 map 没有引用后就可以被 GC 回收了 分片/分时 map 不要使用一个大 map 来存储数据，而用一个 map 切片来存，每个 map 负责存储一个时间段内的数据 根据数据时间戳来写入，定时清理 这样数据洪峰只会撑大其中几个分钟的 map，当洪峰过去后，这些 map 会被自动回收。 线程安全问题 最后，我们来回答本系列最开始的问题，为什么原生的 map 线程不安全。\n原生 map在设计之处就是为了追求性能，而忽略了并发环境的使用。如果在并发环境下使用会发生什么呢？\n原生 map 以我们上面的扩容过程举例，如果一个 goroutine触发了扩容，而另一个 goroutine正在读 map，那么可能会读到：\n旧 bucket 数据 新 bucket 数据 读操作访问内存被释放，触发 panic 当然，并发写导致的数据竞争就更显而易见了。\n总结一下就是 map 的操作有很多小步骤，并不是原子性的，任何一个步骤都可能被其他 goroutine 打断插入。\nsync.Map 这个包中的 map 结构可以通过读写分离来避免并发冲突。\n具体结构如下所示：\ntype Map struct { mu Mutex // 无锁，用于缓存，读取时先从read读；如果未命中再去dirty读 // 原子操作read read atomic.Pointer[readonly] // 带锁，写入时，键值对首先存在dirty中;所有新增，删除，修改操作 dirty map[any]*entry // 缓存未命中次数，当达到阈值，read缓存会被替换成dirty中的最新数据 misses int } // 存储可无锁访问的键值对 type readOnly struct { m map[any]*entry amended bool // true if the dirty map contains some key not in m. } 仔细看就像是 os 中的缓存系统。\n读操作 首先从只读的 read map 中查找，原子读取。 如果没有找到，加锁然后去 dirty map 中查找;找到后解锁，misses计数器+1 写操作 先尝试更新 read map ，但是 key 必须存在于read map 中 如果不满足，加锁去 dirty map 写入；解锁 数据同步 当 misses计数器达到一定的阈值时，说明read map 命中率过低 此时会进行一次升级（数据同步），将dirty map的内容提升为新的read map 与操作系统中的缓存操作有异曲同工之妙啊。\n总结 那么关于 map 的有关知识就到这里结束了，上下两篇，从底层结构到初始化再到扩容最后到并发安全问题。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/045golangep06%E5%85%B3%E4%BA%8Emap-%E4%B8%8B/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e上一篇文章中留下了两个问题没有解决\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emap 如何扩容\u003c/li\u003e\n\u003cli\u003e为什么原生的 map 线程不安全\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e其实这两个问题是有联系的，所以这篇文章来解决这两个问题。\u003c/p\u003e\n\u003ch2 id=\"map-的扩容机制\"\u003emap 的扩容机制\u003c/h2\u003e\n\u003cp\u003e本节还是需要用到上一篇文章中的底层 hmap 结构\u003c/p\u003e","title":"GolangEP06:关于map-下"},{"content":"引子 在做两数之和时，操作 map 遇到了问题，可以看我下面的代码。\nfunc twoSum(nums []int, target int) []int { sMap := make(map[int]int) for i, x := range nums { sMap[x] = i } var ans []int for i, y := range nums { // 如果另一个值在 map 中存在的话即下标大于等于 0;且要保证不能使用两次相同的元素 p := sMap[target-y] if p \u0026gt;= 0 \u0026amp;\u0026amp; p != i { ans = append(ans, i) ans = append(ans, p) break } } return ans } 像极了一个 golang 新手的操作，特别是在判断 key 是否存在于哈希表中时，用值（下标）是否大于 0 来判断。（当然前面的哈希表的定义也有问题）\n而 go 中判断某 key 是否存在于哈希表中使用的是如下语法\nif val, ok := map[key]; ok { ... } 用这个 ok 来判断是否存在。\n还有一点是，就拿我上面错误的判断方法来说，我进行 debug 时发现：为什么不在 map 中的 key 它们的值都返回的是 0 呢？0在题目中的含义可是下标 0啊。\n这就又牵扯到了 go 中 map 的原理了，直接访问一个不存在的 key 会返回该值类型的零值，对于 int 来说就是 0 了。\nMap底层原理 使用 go 时，一开始刷到的八股就是 map 类型线程不安全，多个 goroutine 不能同时安全读写同一个 map。\n但是 why？老规矩，先从其底层结构看起吧。\n底层结构 type hmap struct { count int // map 中存储的元素个数 B uint8 // 当前 bucket 的数量为 2^B 个 buckets unsafe.Pointer // 指向 bucket 数组 oldbuckets unsafe.Pointer // 指向旧的 buckets（在扩容时使用） ... } 看了这些结构内容后，很明显，bucket 数组很重要(其实就是 hash table),这是我们剖析 map 的核心。\n下面这张图可以粗略地展示 bucket数组以及其指向的的 bucket空间。\n可以看到 bucket数组中存放的是指针，指向底层的 bucket 空间，而 bucket 空间由四部分组成\ntopHash存储哈希值的高八位,用于快速查找——不会立即比较key，而是先根据 topHash 进行快速比较，因为字符串比较十分昂贵 keys，要求类型必须是可比较的，有哪些不能比较呢：切片，函数等类型 values 没有特殊要求 *overflows指向溢出桶 初始化 默认一个 bucket 最多可以存储 8 个键值对。与 slice 类似，其初始化也分为不带初始容量和带初始容量。\nmap1 := make(map[string]int) map2 := make(map[string]int, 10) 面对不带容量的，go 会对其进行懒加载，要用的时候才去分配 bucket 对于带容量的，go 会根据容量来计算出一个合适的 B 值（上面提到过的 $2^B$ 桶数量） 显然，带有初始容量会更高效，减少扩容次数（这一点在 slice 中多次提到）。\n最后需要注意一点，如果不用 make 进行初始化而是 var，则会遇到问题\nvar map3 map[string]int map3[\u0026#34;l\u0026#34;]=20 这里会发生 panic，因为这是一个 nil map，不能直接存入键值对。为什么呢？\n在 slice 文章中我们提到过 nil slice 和 empty slice 的区别——nil slice 等于 nil，而empty 是长度为 0 的。nil 是 empty 的子集。\n同样的 nil Map 和 empty Map。我们将二者进行输出，看看结果如何。\nfunc main() { map1 := make(map[string]int) var map3 map[string]int println(map1) println(map3) } // 结果如下 0x14000052708 0x0 nil 输出为0x0，意味着 go 未分配任何底层内存结构给它，只是一个 nil 变量 empty 输出了具体地址，已经分配了 hmap 结构的内存。 所以我们对 nil 进行写入数据会引发 panic。\n总结 那么关于 Map 的剖析上篇就到这里结束了，我们着重分析了其底层结构 hmap 的内容、初始化的方式以及可能出现的问题。\n下一篇文章我们进入底层的扩容机制以及回答开始的问题，为什么 map 并发不安全。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/044golangep06%E5%85%B3%E4%BA%8Emap/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在做两数之和时，操作 map 遇到了问题，可以看我下面的代码。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003etwoSum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003enums\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003etarget\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003esMap\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"nb\"\u003emake\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kd\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"nx\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"k\"\u003erange\u003c/span\u003e \u003cspan class=\"nx\"\u003enums\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"nx\"\u003esMap\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nx\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003ei\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003eans\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"nx\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ey\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"k\"\u003erange\u003c/span\u003e \u003cspan class=\"nx\"\u003enums\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"c1\"\u003e// 如果另一个值在 map 中存在的话即下标大于等于 0;且要保证不能使用两次相同的元素\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"nx\"\u003ep\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"nx\"\u003esMap\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nx\"\u003etarget\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"nx\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"nx\"\u003ep\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan class=\"nx\"\u003ep\u003c/span\u003e \u003cspan class=\"o\"\u003e!=\u003c/span\u003e \u003cspan class=\"nx\"\u003ei\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\t\u003cspan class=\"nx\"\u003eans\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eans\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\t\u003cspan class=\"nx\"\u003eans\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eans\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ep\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\t\u003cspan class=\"k\"\u003ebreak\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nx\"\u003eans\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e像极了一个 golang 新手的操作，特别是在判断 key 是否存在于哈希表中时，用值（下标）是否大于 0 来判断。（当然前面的哈希表的定义也有问题）\u003c/p\u003e","title":"GolangEP06:关于map-上"},{"content":"引子 说起 Git，正如标题所述，让我欢喜让我忧。回想起本科学材料写毕业论文的时候，根本没有版本控制的概念，改了之后不满意就 ctrl+z 或者再回来 ctrl+shift+z。\n甚至大三的时候我连 github 都不知道怎么用，还问过别人怎么用，搜过 B 站的教程。现在看来，哈哈，成长的必经之路。\n跨考后研一上学期我才学习了 Git，最开始接触的时候学的是廖雪峰的 Git，我还记了一个笔记叫《LT 学 Git》。\n哈哈，结果发现自己最常用的场景居然是在我的 linux 和 macos 之间同步代码，无非就是下面几条指令。\ngit pull git st git add git ci -m \u0026#34;\u0026#34; git push origin 有时候老会忘记在执行操作前先进行 git pull 同步上游的代码变更，导致写着写着最后出现了冲突，然后终端出现诸多的信息自己也看不太懂。\n给 chatgpt 进行处理也是云里雾里，并没有很好的理解其中的原理。\n一转眼又到了另一个时间节点，我在尝试一生一芯时学到《计算机缺失的一课》这系列课程，从中学到不少好东西，例如 vim，tmux 还有 git。\n但是从那里我只是比之前更懂 git 操作的底层原理罢了，例如 git 中的对象，树，引用等。\n后来我去了熊厂实习，代码没有写几行，大多数的时间都是去偷文档和看代码，唯一写过的几行代码无非就是修改几个脚本罢了。\n但是这个过程也需要进行 git，我已经记不清当时 git 出现了什么问题了，只记得出过两次代码不一致的问题，但都影响的只是自己。\n好像是我自己在开发的过程中并没有及时去看主分支的代码更新，导致和另一位实习生对代码的过程中发现有的地方他有我没有，哈哈。还让他教了我几个操作。\nAnyway，直到最近，我偶然间发现了朱双印的 git 博文，六七年前的老文章了，我抱着这次我一定要搞懂 git 的心态去看了看，嗯，果真收获颇丰。\n虽然他只更新到了git pull命令，没有后面的 rebase,stash 等操作，但是仍然价值连城。直到最后我还是觉得想要真正掌握 git 操作，必须实践，就像找工作一样，实习的重要性不言而喻。（又开始后悔熊厂的实习了，But 只能向前看了）\n简单梳理 我不想去阐述整个 git 的底层原理然后又去罗列出一堆操作（好吧，也许我还是会稍微列举），就拿一些例子来看吧。\n首先，在上一篇文章中我们对 g 这个 golang 版本管理工具提出了 PR，那么整个过程中 git 操作是什么呢？\n从源仓库 fork 出代码后，我将其 clone 到本地，接着对其进行操作。\n创建新分支: 创建新分支是开发的重要一步，相当于在进行解耦操作，产生不同的逻辑线。 # 查看所有分支 git branch -a * feature/add-riscv64-support master remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/feature/add-riscv64-support remotes/origin/master 可以看到一共有 5 个分支，前两个分支是本地分支；后两个分支其实是“代表”分支——Remoting-tracking,是本地仓库对远程仓库状态的一个快照 现在 HEAD 指针意味着 origin 仓库默认的分支是 master git branch -v * feature/add-riscv64-support e37e4db fix:Complete riscv64 support in pacakge.sh script master a82e89c chore: upgrade version number 这回就只有前两个本地分支了，并且还有各自分支上的最新提交信息 编写代码 在这个过程中我之前一直有一个疑惑，什么时间点我该进行提交？是完成一个模块的开发？还是完成这个模块中某个函数的开发？\n由于没有太多的实际经验，我询问了 ai，得到的回复大概是这样：\n完成一个函数 修复一个 bug 重构一部分代码 一天工作结束时，提交当期进度 这些都算是一种小的频繁的提交，所以提交信息可以写的不那么规范。但是当我们想要提出一个 Pull Request 的时候，我们就需要进行rebase 操作了。\ngit rebase -i HEAD~2 rebase 操作可以将多个相同目的的零碎提交合并成一个，例如要完成一个登录功能，我们开发了相关的多个函数，每个都进行了提交，开发完成测试成功后，进行汇总。 上面的rebase 操作指定了整理最近的 2 个提交；我们可以对其进行提交信息的重写和提交顺序的排列 通常，rebase 命令后呈现的页面类似这样：\npick a1b2c3d commit message one pick e4f5g6h commit message two # Rebase ... # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # ... 有多种操作，每个操作的作用在注释中都有提示，这里我们想要将第二条提交信息合并到第一条中 将第二条的 pick 改为 squash 即可 接下来会自动打开第二个编辑窗口，为合并后的提交编写信息，这会包含之前的提交信息，我们将其全部删除然后写一个全新的最终的提交信息。\n提交信息 关于提交信息，昨天我才第一次看到这个规范\n\u0026lt;类型\u0026gt;[可选 范围]: \u0026lt;描述\u0026gt; fix 代表 bug 相关提交；这会引起 PATCH 补丁版本更新 feat 代表新增功能提交；这会引起 MINOR 版本更新 BREAKING CHANGE 代表包含了不兼容变更 docs,test 等等 包含!提醒注意破坏性变更 更多具体的信息见上面的链接。\n所以对于这次提交，我们 rebase 之后总结一个提交信息为 feat: Add support for linux/riscv64,in build and installation.\n这里跑个题，略微比较一下 git merge 和 git rebase，二者都是用来合并分支的，但有所不同。\n主要的区别是 merge 操作会产生一个新的提交，解决完冲突（如果有冲突）后需要对其进行 add,commit；而 rebase 只是在修改，所以只需要 add rebase 会让提交的历史记录变为直线，比较适用于私人分支； 所以通常个人分支上对于很多提交信息我们用 rebase 来清理，到了 main 分支上我们使用 merge 来合并其他分支 注意，当要进行合并操作时，先 checkout 到你的目的分支上，比如要把 A 往 main 上合并，就需要先 checkout 到 main 上 推送内容 当前我们已经将代码修改完成并暂存提交，同时添加了提交信息。下一步我们就要将这次修改推送到远程仓库去了。\ngit push -u origin feature/add-riscv64-support 平时最常用的是 git push origin，但是很明显这个指令不完整,完整的 push 命令是这样的\ngit push \u0026lt;远程仓库名\u0026gt; \u0026lt;本地分支名\u0026gt;:\u0026lt;远程上游分支名\u0026gt; 这里远程仓库名一般都是 origin\n再回看刚开始的 push 命令，我们使用了 -u 参数，意思是将本地分支推送到远程仓库的同时为其创建上游分支，名称相同；这通常在第一次推送该分支时进行。\nBtw,当本地分支与远程命名相同时，只要在此分支下可以直接执行git push,git pull不用指明后面的参数了。\n通过git branch -vv更直观展现本地分支与上游分支的关系。\ngit branch -vv * feature/add-riscv64-support e37e4db [origin/feature/add-riscv64-support] fix:Complete riscv64 support in pacakge.sh script master a82e89c [origin/master] chore: upgrade version number 既然说到 push，再聊聊对应的 pull 操作吧；pull 其实是 fetch 和 merge 操作的结合：\nfetch将远程分支同步到本地的“代表”分支中 “代表”分支还需要与本地分支进行 merge pull 默认将当前分支对应的远程分支进行拉取 所以之前我遇到的问题：自己开发过程中没有及时与 main 分支进行同步导致混乱，我当时应该先 git fetch origin 获取远程仓库所有最新的更改，然后切换到 main 分支进行git merge origin/main\n在日常的开发中，推送都是从当期分支往其对应的上游分支推送，接着发起 Pull Request.\nPull Request 完成了上面的推送，来到 github 的 fork 页面，我们执行最后一步 PR.\n在 PR 中写清楚这次推送是做什么的即可，向代码的审查人员描述清楚。如果有后续的修改，还是一样的套路，最后这次只需要git push即可。\n如果审查通过了，我们的分支就会被 merge 到 main 分支中，最后我们切换到 main 分支并更新代码。\ngit checkout main git pull origin main 没提到的 还有一些没有提到的操作，例如\ngit stash #保留暂时的更改 git stash pop # 将工作区和暂存区全部回到上一次提交的状态 git reset --hard HEAD git reset -mixed commit git reset -hard commit # 回滚提交但是保存提交的历史记录,回滚后会产生一个新的提交 git revert 最后还有分离 HEAD 操作，可以让 HEAD 指向某个提交，借此可以进行实验性的更改\n丢弃这个更改：回到某个分支 根据这个更改创建新的分支 Anyway，还有很多没有提到的操作，但是我觉得 git 的学习绝对要通过真实的企业级的开发案例来锻炼，上面梳理中其实只是一个向开源项目提出 PR 的过程 :)\nSo,干中学!\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/043git-%E8%AE%A9%E6%88%91%E6%AC%A2%E5%96%9C%E8%AE%A9%E6%88%91%E5%BF%A7/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e说起 Git，正如标题所述，让我欢喜让我忧。回想起本科学材料写毕业论文的时候，根本没有版本控制的概念，改了之后不满意就 ctrl+z 或者再回来 ctrl+shift+z。\u003c/p\u003e\n\u003cp\u003e甚至大三的时候我连 github 都不知道怎么用，还问过别人怎么用，搜过 B 站的教程。现在看来，哈哈，成长的必经之路。\u003c/p\u003e","title":"Git——真是让我欢喜让我忧"},{"content":"引子 在使用 Go 语言开发过程中，我发现经常会遇到想使用的项目的 Go 版本与当前本机的 Go 版本不一致的情况，通常是本地的版本较低。\n所以每次都需要去手动更新版本，而手动更新的过程是比较繁琐的，需要下载新版本并替换旧版本（听起来也没什么是吧）但是 Go 版本的更新还算是比较频繁的，特别是各种小版本。\n所以我寻思着一定有什么自动化的更新或者可以管理本地的 Go 版本的工具吧，于是在浏览 Reddit 的过程中找到了 gvm 和 g 这两个 Go 版本工具。\n在进入 gvm 之前，我想了想，自己的 Go 是怎么安装的呢？使用命令 go env | grep GOROOT 查看路径发现我是通过 Homebrew 安装的。\n所以执行 brew upgrade go应该是可行的，但是怎么说呢，速度感人啊,一共花了 41s\n故之后的 gvm 测试我决定拿到开发板上进行测试，之前我们在开发板环境搭建中不就遇到了 Golang 版本的切换问题吗？🧐\ngvm 听名字有点像 Java 中的 jvm 不是吗？Anyway,我使用开发板进行这个测试。\n安装\n# 安装bison sudo apt-get install bison # 安装gvm bash \u0026lt; \u0026lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer) Cloning from https://github.com/moovweb/gvm.git to /home/debian/.gvm Created profile for existing install of Go at /usr/local/go Installed GVM v1.0.22 Please restart your terminal session or to get started right away run `source /home/debian/.gvm/scripts/gvm` 测试命令\ngvm list all # 安装1.24.5 gvm install 1.24.5 额，发现其下载速度感人，最后下载还显示错误ERROR: Unrecognized Go version\ng 安装时就遇到问题，我发现其在 riscv64 架构上并不适配。\ncurl -sSL https://raw.githubusercontent.com/voidint/g/master/install.sh | bash [1/3] Downloading https://github.com/voidint/g/releases/download/v1.8.0/g1.8.0.linux-.tar.gz 可以看到 linux- 后面缺了一块，在其 release 中也发现暂未支持 RiscV 架构，但是用 Go 写的，那自己编译试试呗。\n手动编译适配 其实对于 Golang 项目，其对于 RiscV 架构的适配是比较简单的，一方面因为 RiscV 架构比较新，没人用啊，哈哈；另一方面 Go 本身是适配 RiscV 架构的，所以这并不是一件难事。\n先手动编译一下\ngit clone https://github.com/voidint/g.git cd g GOOS=linux GOARCH=riscv64 go build 这会编译出一个 g 二进制文件，接下来将其移动到 bin 中\nmkdir -p ~/.g/bin mv g ~/.g/bin/g export PATH=\u0026#34;$HOME/.g/bin:$PATH\u0026#34; source ~/.bashrc 测试使用，注意使用之前先卸载掉之前已经安装好的 Go,或者修改 bashrc 中关于 Go 的路径设置。\ng ls-remote g install 1.24.5 Oh,居然没有遇到架构问题？damn，成功了，看来这两个 Go 系统管理系统在安装逻辑这里有区别哦。\ng install 1.24.5 Downloading 100% [===============] (76/76 MB, 9.0 MB/s) Computing checksum with SHA256 Checksums matched Now using go1.24.5 linux/riscv64 可以发现 go 文件夹安装到了 ~/.g 路径下,所以我们来修改 bashrc 中的路径。\n# g: Go 版本管理器 export PATH=\u0026#34;$HOME/.g/bin:$PATH\u0026#34; # 设置 g 当前版本的 GOROOT 和 PATH export GOROOT=\u0026#34;$HOME/.g/go\u0026#34; export PATH=\u0026#34;$GOROOT/bin:$PATH\u0026#34; # 设置 GOPATH（你的 go mod 或 go install 会安装到这里） export GOPATH=\u0026#34;$HOME/go\u0026#34; export PATH=\u0026#34;$GOPATH/bin:$PATH\u0026#34; 至此 g 可以在开发板上顺利切换版本。\n# 列出当前已安装的版本 g ls * 1.24.2 1.24.5 # 使用某个已经安装的具体版本 g use 1.24.5 Now using go1.24.5 linux/riscv64 不同点 g install 流程 首先来到 g 的源码中关于 install 的部分，大致梳理一遍下载流程。\n获取要下载的版本名称 构建 NewCollector 根据 url 选择构建何种的 Collector 将 url 预解析为 pURL,内容更加丰富，带有 Host,Path 等字段 对 url 发送 Get 请求，使用 goquery 读取响应的 HTML 获取所有的版本信息保存在 items 中 根据版本名称，os，arch 信息找到 items 中对应的信息 拼凑出将要安装到 g 的路径,例如 /Users/lutao/.g/versions/1.22.5，并检查其是否已存在 从 items中找到符合 ArchiveKind, os, arch 的包文件(可能不止一个) checkSum 拼接 fileName 作为要从远程下载的文件名称 /Users/lutao/.g/downloads/go1.22.5.darwin-arm64.tar.gz 下载return httppkg.Download(pkg.URL, dst, os.O_CREATE|os.O_WRONLY, 0644, true) 解压下载的文件 切换版本 最终在~/.g的目录下会有\ndownloads：不同版本的 go 源文件 versions:解压源文件得到的文件 go：软链接，指向versions中的某个版本的文件 gvm install 流程 与 g 不同的是，gvm 是以一个完全的 shell 脚本运行的，但是底层逻辑差不多都是从官方仓库拉取源码进行安装，设置路径，文件目录等。\n在download_binary的部分对架构和 os 进行了判断，显然其中没有 RiscV 架构，所以我们之前安装失败。\n向g项目学习 因为自己做的毕设可能也要开发命令行工具，所以 g 项目对我而言吸引力还是蛮大的。\n我顺便看了看 ls,ls_remote 等功能是如何实现的，下面简单谈谈吧。\nls 本质是os.ReadDir()方法的运用，读出的文件夹格式如下\nparent name typ info 接着利用 name 构造 version 结构体：namej,sv,pkgs,并按升序排序。\nVersion represents a Go language distribution version.\n调用 render，根据传来的版本信息打印出结果。值得学习的是，color.New().Fprintf()这样的着色式打印信息。\n做出适配贡献 着眼于 Makefile,打开后发现其对于 linux 的构建规则中没有 riscv64，所以我们进行添加\nbuild-linux: build-linux-386 build-linux-amd64 build-linux-arm build-linux-arm64 build-linux-s390x build-linux-riscv64 build-linux-riscv64: GOOS=linux GOARCH=riscv64 $(GO) build $(GO_FLAGS) -o bin/linux-riscv64/g 同时，看到其安装方式是通过安装install.sh脚本实现，所以也要修改这个脚本。\n原本脚本中是这样的，没有 riscv 架构，所以会出现最开始那种情况 linux- 缺了一块关于架构的。\nfunction get_arch() { a=$(uname -m) case ${a} in \u0026#34;x86_64\u0026#34; | \u0026#34;amd64\u0026#34;) echo \u0026#34;amd64\u0026#34; ;; \u0026#34;i386\u0026#34; | \u0026#34;i486\u0026#34; | \u0026#34;i586\u0026#34;) echo \u0026#34;386\u0026#34; ;; \u0026#34;aarch64\u0026#34; | \u0026#34;arm64\u0026#34;) echo \u0026#34;arm64\u0026#34; ;; \u0026#34;armv6l\u0026#34; | \u0026#34;armv7l\u0026#34;) echo \u0026#34;arm\u0026#34; ;; \u0026#34;s390x\u0026#34;) echo \u0026#34;s390x\u0026#34; ;; *) echo ${NIL} ;; esac } 添加关于 riscv64 的字段即可。\n最后我们将其汇总为一个 Pull Request 提交到了仓库中。\n总结 Golang 的版本更新需要一个管理工具，反观 Java，在学习的时候好像很少有人提到其版本的更新，听到最多的是 Java 使用的版本要不然是 Java8 要不然就是 17，选一个很稳定的一直用。\nJava 的版本工具最常用的是 SDKMAN（AI 回答）。\n最终我们选择了 g 这个命令行项目作为我在 RiscV 开发板上的 Go 版本管理工具，并且这个项目有很多关于写命令行操作中值得学习的地方，最后我们提出了相应的 PR 🫡\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/042golang%E7%89%88%E6%9C%AC%E6%9B%B4%E6%96%B0/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在使用 Go 语言开发过程中，我发现经常会遇到想使用的项目的 Go 版本与当前本机的 Go 版本不一致的情况，通常是本地的版本较低。\u003c/p\u003e\n\u003cp\u003e所以每次都需要去手动更新版本，而手动更新的过程是比较繁琐的，需要下载新版本并替换旧版本（听起来也没什么是吧）但是 Go 版本的更新还算是比较频繁的，特别是各种小版本。\u003c/p\u003e","title":"Golang版本更新工具"},{"content":"引子 在进行 k3s 的适配过程中，Traefik 作为系统组件镜像之一，我们最终使用了大佬构建好的镜像作为私有镜像，并没有进行深究。\n恰好在昨天的一个集群测试中网络出现了问题，随着这个问题，我觉得我非常有必要去掌握 Traefik 这个组件。\n问题描述 集群环境是1 Server 2 Agent 的模式，运行了一个简单地 Nginx 应用，整体结构如下：\nDeployment 中配置3个 Replica Pod,容器是 Nginx，挂载了本地存储内容是一个简单的前端内容。 Deployment 前配置 Service,匹配上面 Pod 的标签，配置为 ClusterIP，暴露 Pod 给集群 Service 前配置 Ingress ，将特定路径映射到 Service，为流量提供路由。 IngressController 是这个结构最前面的部分，在 k3s 中默认是 Traefik 就当我访问 Ingress 中定义好的路径时，返回 404 Pag Not Found,显然这中间有地方出问题了。\n使用 ping \u0026lt;url\u0026gt; 发现这个 url 的 IP 地址就是 Server 的地址且可以 ping 通，说明笔记本与节点的通信没问题。 检查 Deployment, Pod, Service, Ingress 的状态，都是正常运行，没有错误信息。 检查 Traefik 的日志，出现问题，于是检查这个问题。 问题所在 # 获取Traefik Pod 信息 sudo kubectl get pods -o wide -n kube-system -l app.kubernetes.io/name=traefik # 查看日志信息 sudo kubectl logs \u0026lt;traefik_pod_name\u0026gt; -n kube-system 查看日志才发现，错误就在这里。\nW0602 12:19:21.743160 1 reflector.go:561] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: failed to list *v1.ConfigMap: configmaps is forbidden: User \u0026#34;system:serviceaccount:kube-system:traefik\u0026#34; cannot list resource \u0026#34;configmaps\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope E0602 12:19:21.743322 1 reflector.go:158] \u0026#34;Unhandled Error\u0026#34; err=\u0026#34;k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps is forbidden: User \\\u0026#34;system:serviceaccount:kube-system:traefik\\\u0026#34; cannot list resource \\\u0026#34;configmaps\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; at the cluster scope\u0026#34; logger=\u0026#34;UnhandledError\u0026#34; 经过一番 google，发现一个与我遇到相同问题的人,他对于此问题的描述是 traefik 的配置文件中关于某些组件的定义有缺失，需要补充 traefik 的配置文件。\n解决如下：sudo kubectl edit clusterrole traefik-kube-system 在其中添加 discovery.k8s.io,nodes,endpointslices,serverstransporttcps等字段，最终结果如下:\nrules: - apiGroups: - extensions - networking.k8s.io - discovery.k8s.io resources: - ingressclasses - ingresses - endpointslices verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets - configmaps - nodes verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - traefik.io resources: - ingressroutes - ingressroutetcps - ingressrouteudps - middlewares - middlewaretcps - tlsoptions - tlsstores - traefikservices - serverstransporttcps - serverstransports verbs: - get - list - watch 删除这个 Pod 之后它会自动重新生成，之后的结果就正常了，可以成功访问定义的路径。\n具体原因 从 forbidden 可以看出，这其实是一个 RABC (Role-Based Access Control) 权限问题.\nRole 会定义有关于资源的操作权限（可以对哪些资源执行哪些操作） RoleBinding 会将这个定义好的权限给某个或某组用户(要注意特定命名空间) ServiceAccount: 集群内部的进程(例如 Pod 中运行的应用) 报错日志告诉我们，User \\\u0026quot;system:serviceaccount:kube-system:traefik\\\u0026quot; cannot list resource \\\u0026quot;configmaps\\\u0026quot; in API group \\\u0026quot;\\\u0026quot; at the cluster scope\u0026quot;即某个用户的对于某个资源的某个操作失败了。\n即我们的 Traefik 在读取某些资源的时候失败了，权限不够！这会造成什么后果呢，请继续往下看。\nTraefik 在 K8s/K3s 中的应用 Traefik 到底是做什么的呢？(类似于 Nginx,但是又有其特点)\n监视 (Watch) K8s/K3s 集群中的 Ingress、Service、EndpointSlice 等资源的变化 根据这些资源的信息，动态配置其内部的路由规则 将外部传入的流量路由到正确的后端 Service 和 Pod 在官网的这篇文档中可以看到 Traefik 在 K8s 中的应用。 值得一提的是，K3s 内置了 Traefik 作为 Ingress Controller,而 K8s 则是没有内置，默认维护 AWS,nginx 等 Controller，同时也可以采用第三方例如 Traefik.\n在 K8s 中使用 Traefik,我们需要进行以下操作:\n创建 role:ClusterRole 创建 ServiceAccount,将 role 绑定到 account 上； 部署 traefik deployment，设置 traefik 容器端口web 80,dashboard 8080 部署 traefik service,并在其中对应上面两个端口 而 K3s 中，观其源码可以发现，其在 /manifest 目录下定义了使用 Helm Controller 来管理例如 traefik 这样的预装组件，通过 Helm Chart 自动部署。\n接下来我们来验证一下 K3s 集群中的 Traefik 属性:\nrole \u0026amp; ServiceAccount\n# ServiceAccount,名称为 traefik sudo kubectl get serviceaccount -n kube-system traefik NAME SECRETS AGE traefik 0 18d # ClusterRole,名称为traefik-kube-system sudo kubectl get clusterrole | grep traefik traefik-kube-system 2025-05-15T06:59:41Z # 查看其 Role 信息： sudo kubectl get clusterrole traefik-kube-system -o yaml 这里查看到的信息就是我们上面修改好的“配置文件”,定义了 Traefik 可以访问哪些 apiGroups,groups 中的 resources,可以对这些 resources 进行哪些 verbs\nbinding\n# 查看 clusterrolebinding 绑定信息 sudo kubectl get clusterrolebinding | grep traefik helm-kube-system-traefik ClusterRole/cluster-admin 19d helm-kube-system-traefik-crd ClusterRole/cluster-admin 19d traefik-kube-system ClusterRole/traefik-kube-system 18d # 查看具体的绑定规则 sudo kubectl get clusterrolebinding traefik-kube-system -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: meta.helm.sh/release-name: traefik meta.helm.sh/release-namespace: kube-system creationTimestamp: \u0026#34;2025-05-15T06:59:41Z\u0026#34; labels: app.kubernetes.io/instance: traefik-kube-system app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: traefik helm.sh/chart: traefik-27.0.201_up27.0.2 name: traefik-kube-system resourceVersion: \u0026#34;4041\u0026#34; uid: 905de4bb-8aab-42c6-a84c-169252444073 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-kube-system subjects: - kind: ServiceAccount name: traefik namespace: kube-system 在最后的部分可以发现就是把 traefik-kube-system 绑定到了 traefik 上，这与官网给出的示例几乎一致。\nDeployment \u0026amp; Service\n# Deployment sudo kubectl get deployment -n kube-system traefik NAME READY UP-TO-DATE AVAILABLE AGE traefik 1/1 1 1 18d # Deployment 内部 Port 信息 ports: - containerPort: 9100 name: metrics protocol: TCP - containerPort: 9000 name: traefik protocol: TCP - containerPort: 80 name: web protocol: TCP - containerPort: 8443 name: websecure protocol: TCP # Service sudo kubectl get service -n kube-system traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.144.33 192.168.31.70 80:32194/TCP,443:31405/TCP 18d # Service 内部 Port 信息，与上面 Deployment 要对应 ports: - name: web nodePort: 32194 port: 80 protocol: TCP targetPort: web - name: websecure nodePort: 31405 port: 443 protocol: TCP targetPort: websecure 其实从这里与之前的集群图片对照，可以看出 Traefik Deployment 作为 Ingress Controller,其 Service 在前面作为 LoadBalancer,让外部的流量可以到达 Traefik Deployment.\n与 Nginx 对比 从前面的介绍可以看出，Traefik 最大的卖点就是实时监测集群内的路由情况并自动进行服务发现并为其动态配置路由。\n而传统的 Nginx 想要完成这样的自动化是不可能的，需要手动更改 nginx.conf文件，向其中添加路由规则，所以其并不适合动态服务。其通常用作静态文件服务和反向代理。\n但是 Nginx 也像 Traefik 一样，提供了动态监听的 Ingress Controller: Nginx Ingress Controller,虽然 K3s 默认嵌入 Traefik,但如果你想，也可以替换成 Nginx.\n总结 Traefik 作为集群中的 Ingress Controller,需要调用集群中的资源来为我们后方 Deployment Pod 中的应用自动构建路由规则，将外部的请求转发过来。\n所以我们需要保证其有访问某些资源的权限，例如 nodes,endpointslices 等资源。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/041traefik/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在进行 k3s 的适配过程中，Traefik 作为系统组件镜像之一，我们最终使用了大佬构建好的镜像作为私有镜像，并没有进行深究。\u003c/p\u003e\n\u003cp\u003e恰好在昨天的一个集群测试中网络出现了问题，随着这个问题，我觉得我非常有必要去掌握 Traefik 这个组件。\u003c/p\u003e","title":"K3sEP13——遇到的Traedfik问题"},{"content":"引子 对于 Pod 的一切规则\n暂时以这些字段进行测试\nPod:\n容器 卷 调度 生命周期 主机名，主机名字空间 服务账号 安全上下文 容器：\n镜像 Entrypoint 端口 环境变量 卷 资源 生命周期 安全上下文 Lifetime 生命周期 UID\n指数退避延迟\n资源分配 为容器分配资源 requests \u0026amp; limits\n如果要直接为 Pod 指定分配的资源，需要启用 PodLevelResources feature gate;所以还是为每个容器分配资源，最终资源总和就是这个 pod 资源总和。\n调整资源 传统上，更改 Pod 的资源需求需要删除现有的 Pod 并创建一个替换品，这通常由工作负载控制器管理。\nkubectl edit deploy xxx # 修改 YAML 文件 kubectl rollout restart xxx # 触发滚动重启 原地 Pod 调整大小允许在运行中的 Pod 内更改容器（s）的 CPU/内存分配，从而可能避免应用程序中断 只能调整 CPU 和内存，无法降低内存限制 但是只有高版本支持调整操作，所以调整操作在日常的使用中并不常见？\nQos 服务质量 Guaranteed:limits == requests Burstable:不等于 BestEffort:没有任何要求和限制 查看指定字段信息\n卷 empty volumes volumeMounts 持久卷 pv 持久卷 pvc 持久卷声明:找到符合的 pv 进行绑定并独占 Pod -\u0026gt; pvc -\u0026gt; pv -\u0026gt; 实际存储后端(NFS,hostPath,CephFS) storageClassName 动态供应\nProjected Volume 投影卷，聚合多种资源，统一挂载路径\nsecret configMap downardAPI serviceAccountToken 权限 Security Context securityContext 控制容器中运行用户、组、文件系统权限\nrunAsUser：容器进程的用户ID runAsGroup：容器进程的主要组ID fsGroup：卷的文件系统组ID supplementalGroups：附加组 Service Accounts Pod 中的应用程序与 K8s API 交互；每个 namespace 都有一个 default 用户\n为 Pod 提供精细的 API 访问权限(特定权限)\n","permalink":"http://localhost:1313/posts/040k3sep13%E5%85%B3%E4%BA%8Epod%E7%9A%84%E4%B8%80%E5%88%87/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e对于 Pod 的一切规则\u003c/p\u003e\n\u003cp\u003e暂时以这些字段进行测试\u003c/p\u003e\n\u003cp\u003ePod:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e容器\u003c/li\u003e\n\u003cli\u003e卷\u003c/li\u003e\n\u003cli\u003e调度\u003c/li\u003e\n\u003cli\u003e生命周期\u003c/li\u003e\n\u003cli\u003e主机名，主机名字空间\u003c/li\u003e\n\u003cli\u003e服务账号\u003c/li\u003e\n\u003cli\u003e安全上下文\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e容器：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e镜像\u003c/li\u003e\n\u003cli\u003eEntrypoint\u003c/li\u003e\n\u003cli\u003e端口\u003c/li\u003e\n\u003cli\u003e环境变量\u003c/li\u003e\n\u003cli\u003e卷\u003c/li\u003e\n\u003cli\u003e资源\u003c/li\u003e\n\u003cli\u003e生命周期\u003c/li\u003e\n\u003cli\u003e安全上下文\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lifetime-生命周期\"\u003eLifetime 生命周期\u003c/h2\u003e\n\u003cp\u003eUID\u003c/p\u003e","title":"040K3sEP13关于Pod的一切"},{"content":"引子 本文记录完成 Lab01 的过程,跟着 AI 照猫画虎.\nMapReduce 首先我们需要简单了解一下 MapReduce 这篇论文,其提出了分布式的思想,具体如下:\nMap:将一个大的问题分解为小问题,应用用户定义的 Map 函数,生成中间键值对——文本分割为单词,输出 {world, 1} 这样的键值对 Reduce: 将中间键值对按 key 分组,应用用户定义的 Reduce 函数,生成最终输出——Reduce 对同一 key 进行计数求和 {word, count} 系统可以自动做到并行,任务调度,负载均衡,容错,让用户只需要开发 Map 和 Reduce 这两个函数 这样的一个系统由三部分组成\nMaster: 主节点,类似于 K3s 中的 server 节点 Worker: 类似于 K3s 中的 agent 节点 分布式文件系统,例如 GFS: 存储输入数据,中间结果和最终输出 工作流程如下:\n输入数据被分割成固定大小的分片 Master 将 Map 任务分配给 Worker，Worker 读取分片，执行 Map 函数，生成中间键值对，写入本地磁盘。 中间键值对按键分区（partitioned），传输到 Reduce Worker Reduce Worker 对键值对排序、分组，执行 Reduce 函数，生成最终输出，写入分布式文件系统 而本 lab 的目标就是实现一个简化的 MapReduce 系统,模拟 Master 和 Worker. 处理任务分配、并行执行和故障容错.\nLab 实现 在本 Lab 中,我们只需要编写 src/main/mr 下的三个代码即可.\nCoordinator.go 对于 Master 节点,我们需要让他来进行调度,如何确保 Worker 们按照我们想要的方式来执行任务.\nCoordinator 首先,我们先要定义 Coordiantor 本身\ntype Coordinator struct { // Your definitions here. mu sync.Mutex // 互斥锁 files []string // 输入文件列表 nReduce int // Reduce任务数 mapTasks []Task // Map任务列表,以taskID作为下标 reduceTasks []Task // Reduce任务列表 completedMaps int // 已完成的Map任务数量 completedReds int // 已完成的Reduce任务数量 } 我们作为控制者,要获取输入的文件集合,保存 Map 和 Reduce 两种不同任务,并统计二者的初始数量与已完成的数量——用来看某类任务是否已经全部完成.\n对于这些变量,为了防止数据冲突,自然要处理并发;在这里我们使用了 mutex 直接上锁,具体操作见后文.\nTask 其次,我们需要定义有关 Task 的信息,分别是 Type 结构体和任务类型 TaskType\ntype TaskType string type Task struct { TaskID int // 任务ID TaskType TaskType // 是Map还是Reduce InputFile string // 对Map来说输入文件 OutputFile string // 对Reduce来说最终的输出文件 NReduce int // Reduce任务数 NMap int StartTime time.Time // 用于计时,看是否超时 Completed bool // } const ( Map TaskType = \u0026#34;Map\u0026#34; Reduce TaskType = \u0026#34;Reduce\u0026#34; Wait TaskType = \u0026#34;Wait\u0026#34; Exit TaskType = \u0026#34;Exit\u0026#34; ) 同时,我们还定义了一些常量表示不同任务类型.\nMakeCoordinator 这是一个初始化方法,入参已经给定,我们来初始化 Coordinator,Task切片.\nfunc MakeCoordinator(files []string, nReduce int) *Coordinator { c := Coordinator{ files: files, nReduce: nReduce, mapTasks: make([]Task, len(files)), reduceTasks: make([]Task, nReduce), completedMaps: 0, completedReds: 0, } // 初始化Map任务 for i, file := range files { c.mapTasks[i] = Task{ TaskID: i, TaskType: Map, InputFile: file, NReduce: nReduce, NMap: len(files), } } // 初始化Reduce任务 for i := 0; i \u0026lt; nReduce; i++ { c.reduceTasks[i] = Task{ TaskID: i, TaskType: Reduce, OutputFile: fmt.Sprintf(\u0026#34;mr-out-%d\u0026#34;, i), NReduce: nReduce, NMap: len(files), } } c.server() return \u0026amp;c } 最终启动了 server,server 是已经写好的代码,它用来作为 Coordinator 监听相关 RPC 请求的服务端.\nRPC方法 在调度框架中, Worker 会通过发送 RPC 请求向 Master 请求资源,例如请求任务,所以我们在 Master 端需要写相对应的请求任务的逻辑.\nRequestTask 优先分配未完成或者超时的 Map 任务,所有 Map 任务完成后分配 Reduce 任务(实现了 Reduce 对 Map 的依赖关系) 由于存在并发,所以我们一开始就用 mutex 保护共享状态.\nfunc (c *Coordinator) RequestTask(args *TaskArgs, reply *TaskReply) error { c.mu.Lock() defer c.mu.Unlock() // 优先分配Map for i := range c.mapTasks { // 如果任务未完成 if !c.mapTasks[i].Completed { // 检查任务是否超时 if !c.mapTasks[i].StartTime.IsZero() \u0026amp;\u0026amp; time.Since(c.mapTasks[i].StartTime) \u0026gt; 10*time.Second { // 若超时则重新分配 c.mapTasks[i].StartTime = time.Now() reply.Task = c.mapTasks[i] return nil } else if c.mapTasks[i].StartTime.IsZero() { // 任务未分配 c.mapTasks[i].StartTime = time.Now() reply.Task = c.mapTasks[i] return nil } } } // 如果所有Map任务都完成 if c.completedMaps == len(c.mapTasks) { // 分配Reduce 任务 for i := range c.reduceTasks { if !c.reduceTasks[i].Completed { if !c.reduceTasks[i].StartTime.IsZero() \u0026amp;\u0026amp; time.Since(c.reduceTasks[i].StartTime) \u0026gt; 10*time.Second { // 若超时则重新分配 c.reduceTasks[i].StartTime = time.Now() reply.Task = c.reduceTasks[i] return nil } else if c.reduceTasks[i].StartTime.IsZero() { // 任务未分配 c.reduceTasks[i].StartTime = time.Now() reply.Task = c.reduceTasks[i] return nil } } } // 所有 reduce 完成 if c.completedReds == len(c.reduceTasks) { reply.Task.TaskType = Exit reply.Finished = true return nil } } // 无可用任务, Wo 等待 reply.Task.TaskType = Wait return nil } TaskCompleted 同时, Worker 也要向 Master 发送对应的任务处理情况通知,为了让 Master 去管理全局状态,例如已经完成的任务数的更新.\nfunc (c *Coordinator) TaskCompleted(args *TaskCompletionArgs, reply *TaskCompletionArgs) error { c.mu.Lock() defer c.mu.Unlock() switch args.TaskType { case Map: if args.TaskID \u0026lt; len(c.mapTasks) \u0026amp;\u0026amp; !c.mapTasks[args.TaskID].Completed { c.mapTasks[args.TaskID].Completed = true // 在范围内且传回来未完成? c.completedMaps++ log.Printf(\u0026#34;Map任务 %d 已完成\u0026#34;, args.TaskID) } case Reduce: if args.TaskID \u0026lt; len(c.reduceTasks) \u0026amp;\u0026amp; !c.reduceTasks[args.TaskID].Completed { c.reduceTasks[args.TaskID].Completed = true c.completedReds++ log.Printf(\u0026#34;Reduce任务 %d 已完成\u0026#34;, args.TaskID) } } return nil } 如果报告的的是 Map 任务且状态为未完成,我们才会去更新任务状态为已完成.\n为了保证任务状态的幂等性——重复通知不改变任务状态,所以我们只有未完成才去更新(通知了一定是完成了) 之后的通知此任务状态为已完成,不会再更新 Done 最后,主函数需要调用 Done 来判断整个系统是否运行结束,我们根据已完成的任务数与预期的任务数对比来判断.\nfunc (c *Coordinator) Done() bool { //ret := false // Your code here. c.mu.Lock() defer c.mu.Unlock() return c.completedMaps == len(c.mapTasks) \u0026amp;\u0026amp; c.completedReds == len(c.reduceTasks) } rpc.go 在 lab 中, Master 和 Worker 是通过套接字 socket 来通信的,在 rpc.go 中我们需要定义 RPC 通信的请求和响应结构,可以携带一些状态或信息.\n通常以请求和响应参数成对出现,从 Worker 请求到 Master,从 Master 响应到 Worker\n与任务的请求有关的:\ntype TaskArgs struct{} type TaskReply struct { Task Task Finished bool } 与任务完成情况有关的:\ntype TaskCompletionArgs struct { TaskID int TaskType TaskType } type TaskCompletionReply struct{} Worker 需要向 Master 传递 TaskID 等信息供后者使用. Master 无需向 Worker 传递什么信息. 值得注意的是,在 RPC 调用中,我们要保证 RPC 的参数类型在调用双方是匹配的,这里我们都用了之前声明的结构体,没有此问题.\n后续更新,其实有关 Task 的定义也该放在这里.\nworker.go 在 worker 中,我们就要实际进行 Map 和 Reduce 的相关操作了. 为了本文的连续性,我们先写有关于 RPC 的相关代码.\nRPC相关 我们需要一个 call 方法来调用 Master 中的方法,具体就是靠 rpc.DialHTTP 打开一个连接,Worker 作为客户端去调用 Call 方法\nfunc call(rpcname string, args interface{}, reply interface{}) bool { // c, err := rpc.DialHTTP(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1\u0026#34;+\u0026#34;:1234\u0026#34;) sockname := coordinatorSock() c, err := rpc.DialHTTP(\u0026#34;unix\u0026#34;, sockname) if err != nil { log.Fatal(\u0026#34;dialing:\u0026#34;, err) } defer c.Close() err = c.Call(rpcname, args, reply) if err == nil { return true } fmt.Println(err) return false } 同时我们也要写与服务端,也就是 Master 中对应的 RPC 方法,本质上是通过上面的 call 方法调用服务端上的方法.\nrequestTask,请求任务.\nfunc requestTask() (Task, error) { // declare an argument structure. args := \u0026amp;TaskArgs{} reply := \u0026amp;TaskReply{} ok := call(\u0026#34;Coordinator.RequestTask\u0026#34;, args, reply) if !ok { return Task{}, fmt.Errorf(\u0026#34;RPC调用失败\u0026#34;) } // declare a reply structure. //reply := ExampleReply{} return reply.Task, nil } reportTaskCompletion,报告任务完成情况.\nfunc reportTaskCompletion(taskID int, taskType TaskType) { args := \u0026amp;TaskCompletionArgs{ TaskID: taskID, TaskType: taskType, } var reply struct{} ok := call(\u0026#34;Coordinator.TaskCompleted\u0026#34;, args, \u0026amp;reply) if !ok { log.Printf(\u0026#34;报告任务完成失败: TaskID=%d, Type=%s\u0026#34;, taskID, taskType) } } Worker本身 Worker 的主要的逻辑是请求任务,执行任务(如果没有任务则等待),所有任务执行完后退出.\nfunc Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { // Your worker implementation here. // 注册RPC客户端,连接Coordinator,持续请求任务直到收到退出信号 for { // 请求任务 task, err := requestTask() if err != nil { log.Printf(\u0026#34;获取任务失败: %v\u0026#34;, err) time.Sleep(2 * time.Second) continue } // 执行任务 switch task.TaskType { case Map: log.Printf(\u0026#34;开始处理Map任务: %d, 文件: %s\u0026#34;, task.TaskID, task.InputFile) executeMapTask(task, mapf) reportTaskCompletion(task.TaskID, task.TaskType) case Reduce: log.Printf(\u0026#34;开始处理Reduce任务: %d\u0026#34;, task.TaskID) executeReduceTask(task, reducef) reportTaskCompletion(task.TaskID, task.TaskType) case Wait: log.Printf(\u0026#34;没有可用任务,等待中...\u0026#34;) time.Sleep(2 * time.Second) case Exit: log.Println(\u0026#34;所有任务已完成,Worker退出\u0026#34;) return } } } 请求任务失败的话暂停2秒继续请求 根据请求到的任务类型来进行指定的操作,具体的操作见下文. executeMapTask 用于处理 Map 任务\n读取输入文件,将其内容保存在 content 中 调用 mapf 方法处理内容,得到中间结果 kv pairs 将这些对根据 reduceID(即相同的 key 组成一个集合)保存在一个二维切片中 将这些 pairs 编码为 JSON 格式并存储在临时文件中 要注意对文件的命名符合实验要求 将上述操作放在一个 goroutine 中执行,并且保证不发生冲突 func executeMapTask(task Task, mapf func(string, string) []KeyValue) { // 读取输入文件 file, err := os.Open(task.InputFile) if err != nil { log.Fatalf(\u0026#34;Cannot open %v\u0026#34;, task.InputFile) } defer file.Close() content, _ := io.ReadAll(file) // 调用Map kvs := mapf(task.InputFile, string(content)) // 按 Reduce 任务分区存储中间结果 nReduce := task.NReduce intermediate := make([][]KeyValue, nReduce) // reduceID -\u0026gt; kv ? for _, kv := range kvs { reduceID := ihash(kv.Key) % nReduce intermediate[reduceID] = append(intermediate[reduceID], kv) } // 写入临时文件并原子重命名 var wg sync.WaitGroup for i := 0; i \u0026lt; nReduce; i++ { wg.Add(1) go func(reduceID int, data []KeyValue) { defer wg.Done() tempFile, err := os.CreateTemp(\u0026#34;\u0026#34;, fmt.Sprintf(\u0026#34;mr-%d-%d.tmp\u0026#34;, task.TaskID, reduceID)) if err != nil { log.Fatalf(\u0026#34;Can\u0026#39;t create tempFile: %v\u0026#34;, err) } defer tempFile.Close() encoder := json.NewEncoder(tempFile) for _, kv := range data { if err := encoder.Encode(\u0026amp;kv); err != nil { log.Fatalf(\u0026#34;编码 JSON 失败: %v\u0026#34;, err) } } // 原子重命名为最终文件名 finalName := fmt.Sprintf(\u0026#34;mr-%d-%d\u0026#34;, task.TaskID, reduceID) if err := os.Rename(tempFile.Name(), finalName); err != nil { log.Fatalf(\u0026#34;重命名文件失败: %v\u0026#34;, err) } }(i, intermediate[i]) } wg.Wait() } executeReduceTask 几乎同理,我们处理刚才的 Map 过程产生的文件,读取其中信息并进行解码,最终保存在一个一维切片中.\n相同key作为一组调用 reducef 方法进行处理,处理完后的结果写入到输出文件中.\nfunc executeReduceTask(task Task, reducef func(string, []string) string) { // 收集所有Map任务的中间文件 var kvs []KeyValue for mapID := 0; mapID \u0026lt; task.NMap; mapID++ { fileName := fmt.Sprintf(\u0026#34;mr-%d-%d\u0026#34;, mapID, task.TaskID) file, err := os.Open(fileName) if err != nil { continue // 忽略未完成的 Map 任务文件 } decoder := json.NewDecoder(file) for { var kv KeyValue if err := decoder.Decode(\u0026amp;kv); err != nil { break // 读完所有记录 } kvs = append(kvs, kv) } file.Close() } // 按 Key 排序 sort.Slice(kvs, func(i, j int) bool { return kvs[i].Key \u0026lt; kvs[j].Key }) // 分组调用Reduce,找key相同的集合放到values中 var results []string i := 0 for i \u0026lt; len(kvs) { j := i for j \u0026lt; len(kvs) \u0026amp;\u0026amp; kvs[j].Key == kvs[i].Key { j++ } keys := kvs[i].Key values := []string{} for k := i; k \u0026lt; j; k++ { values = append(values, kvs[k].Value) } result := reducef(keys, values) results = append(results, fmt.Sprintf(\u0026#34;%v %v\\n\u0026#34;, keys, result)) i = j } // 写入 Reduce 输出文件 tempFile, err := os.CreateTemp(\u0026#34;\u0026#34;, fmt.Sprintf(\u0026#34;mr-out-%d.tmp\u0026#34;, task.TaskID)) if err != nil { log.Fatalf(\u0026#34;创建临时文件失败: %v\u0026#34;, err) } defer tempFile.Close() _, err = tempFile.WriteString(strings.Join(results, \u0026#34;\u0026#34;)) if err != nil { log.Fatalf(\u0026#34;写入文件失败: %v\u0026#34;, err) } finalName := fmt.Sprintf(\u0026#34;mr-out-%d\u0026#34;, task.TaskID) if err := os.Rename(tempFile.Name(), finalName); err != nil { log.Fatalf(\u0026#34;文件重命名失败: %v\u0026#34;, err) } } 至此, Worker.go 结束. 最后执行实验命令可以通过实验.\n总结 至此,就是我对于 lab1 的做法流程梳理,实现了一个小型的 MapReduce 分布式框架结构,这对于我自己目前在做的毕业设计项目也有很好的启发.\n特别是在 Task 如何定义,在 lab 中我们定义为了 go 中的 struct,在未来要写的分布式框架中我们对于不同的 Task 的定义是通过 json 文件进行定义,来确定其所占据的资源信息.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n引用 https://pdos.csail.mit.edu/6.824/labs/lab-mr.html ","permalink":"http://localhost:1313/posts/039mit6.824lab1/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e本文记录完成 \u003ca href=\"https://pdos.csail.mit.edu/6.824/labs/lab-mr.html\"\u003eLab01 \u003c/a\u003e的过程,跟着 AI 照猫画虎.\u003c/p\u003e\n\u003ch2 id=\"mapreduce\"\u003eMapReduce\u003c/h2\u003e\n\u003cp\u003e首先我们需要简单了解一下 MapReduce 这篇论文,其提出了分布式的思想,具体如下:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMap:将一个大的问题分解为小问题,应用用户定义的 Map 函数,生成中间键值对——文本分割为单词,输出 \u003ccode\u003e{world, 1}\u003c/code\u003e 这样的键值对\u003c/li\u003e\n\u003cli\u003eReduce: 将中间键值对按 key 分组,应用用户定义的 Reduce 函数,生成最终输出——Reduce 对同一 key 进行计数求和 \u003ccode\u003e{word, count}\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e系统可以自动做到\u003cstrong\u003e并行,任务调度,负载均衡,容错\u003c/strong\u003e,让用户只需要开发 Map 和 Reduce 这两个函数\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这样的一个系统由三部分组成\u003c/p\u003e","title":"Mit6.824Lab1流程梳理"},{"content":"引子 为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 metric-server; 在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.\nmetric-server 什么是 metric-server Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API for use by Horizontal Pod Autoscaler and Vertical Pod Autoscaler. Metrics API can also be accessed by kubectl top, making it easier to debug autoscaling pipelines.\n它会去与 kubelet 交互,作为 apiserver 的外部 api,向 kubelet 的 /metrics/resource 发起 HTTPS 请求;同时其还支持 HPA 自动扩缩容\n可以使用 kubectl get apiservices 查看其 api 状态\nNAME SERVICE AVAILABLE AGE v1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d22h 具体操作 首先,metric-server 在其 release 版本中并没有 riscv 的版本,所以我们先要进行适配工作. 不过好在之前的那位大佬也做了相关的支持,所以我们直接使用它的镜像作为我们的私有仓库镜像.\n之后写一个 yaml文件 用来部署 metric-server 到 server 上\n# metrics-server.yaml apiVersion: v1 kind: Namespace metadata: name: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: replicas: 1 selector: matchLabels: k8s-app: metrics-server template: metadata: labels: k8s-app: metrics-server spec: containers: - name: metrics-server image: jimlt.bfsmlt.top/metrics-server:v0.7.2 # 私有镜像 imagePullPolicy: IfNotPresent args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname ports: - containerPort: 4443 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: k8s-app: metrics-server ports: - port: 443 targetPort: 4443 这里包括了 Deployment 和对应的 Service,并且我们作为测试没有用到 tls --kubelet-insecure-tls\n写好之后进行部署 kubectl apply -f xxx.yaml\n部署结束后,我们进行测试 kubectl top nodes,发现此服务并没有启动成功.\n排查问题 我们自然地去查看容器的状态,即 kubectl describe,发现问题所在\nReadiness probe failed: Get \u0026#34;https://10.42.0.58:10250/readyz\u0026#34;: dial tcp 10.42.0.58:10250: connect: connection refused 这是个什么地址呢?我们先来看看之前 yaml 文件中的这段代码 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\nInternalIP: 节点 Node 在集群内部网络中的地址 ExternalIP: 公网 IP,供集群外部访问 Hostname: 节点的计算机名称 这些与 PodIP 不同,PodIP 是集群使用 CNI 分配的,关于 CNI 在 k3s 中的知识,请看这篇文章.\n所以,这个请求其实是发往了我的 server 节点上的一个 Pod,所以这个就很奇怪了,按道理来说应该发往 InternalIP 即192.168.1.198 才对\n执行 kubectl get apiservices 可以发现 MissingEndpoints\nv1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d22h 相关的 issue 可以找到些解决办法,最多的还是添加 - --kubelet-insecure-tls,但是我们已经有了不是吗?\n如果现在遇到同样问题的你找到了其他更好的相关问题或者解决方法,请在评论区告知.\n官方文档中的问题 stackoverflow上有关问题 我们在 yaml 配置中添加 HostNetwork\nspec: hostNetwork: true # ✅ 添加 hostNetwork，避免 CNI 中 10250 访问失败 dnsPolicy: ClusterFirstWithHostNet # ✅ 搭配 hostNetwork 使用 之后再次查看容器的运行状态:\nReadiness probe failed: HTTP probe failed with statuscode: 404 通过查看日志发现:\nsudo kubectl logs -n kube-system metrics-server-6b8f575f5d-56jm8 I0518 11:31:43.086131 1 serving.go:374] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0518 11:31:45.510186 1 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager I0518 11:31:45.658709 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController I0518 11:31:45.658803 1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController I0518 11:31:45.659011 1 configmap_cafile_content.go:202] \u0026#34;Starting controller\u0026#34; name=\u0026#34;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\u0026#34; I0518 11:31:45.659138 1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0518 11:31:45.659169 1 configmap_cafile_content.go:202] \u0026#34;Starting controller\u0026#34; name=\u0026#34;client-ca::kube-system::extension-apiserver-authentication::client-ca-file\u0026#34; I0518 11:31:45.659247 1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I0518 11:31:45.660929 1 dynamic_serving_content.go:132] \u0026#34;Starting controller\u0026#34; name=\u0026#34;serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key\u0026#34; I0518 11:31:45.662143 1 secure_serving.go:213] Serving securely on [::]:4443 I0518 11:31:45.662329 1 tlsconfig.go:240] \u0026#34;Starting DynamicServingCertificateController\u0026#34; I0518 11:31:45.759559 1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I0518 11:31:45.759641 1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0518 11:31:45.759584 1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController 其并没有直接显示一些访问地址失败的信息.\n接着我们尝试 kubectl edit deployment -n kube-system metrics-server修改配置 将 livenessProbe 和 readinessProbe 及其有关配置删除 相关解决方案来自 CSDN\nreadinessProbe: httpGet: path: /readyz port: 4443 scheme: HTTPS initialDelaySeconds: 20 periodSeconds: 10 再来看看 Pod 结果如何\nkube-system metrics-server-6c8f74f4c4-fktpw 1/1 Running 居然成功了,查看集群信息:\nsudo kubectl top no NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% agent197 40m 1% 774Mi 9% revyos-lpi4a 136m 3% 1502Mi 9% 所以最终我们只需要去掉 readinessProbe即可,不用添加 NetWork 等字段在配置文件中. 这对整体的运行时没有影响的,只是去掉了存活探针检测机制.\nProbe及可能原因 这些 Probe 是来自于 k3s 的系统探针\nReadiness Probe:检测某个容器是否就绪(就是我们常见的 Ready 字段),是否可以接受流量,如果没有就不能加入到 Service Liveness Probe:判断容器是否“健康”，未通过探针 → 被重启容器。 Startup Probe:判断容器启动是否完成，配合前两者避免“启动未完成就被探测失败”。 所以结合之前的两种失败报错, https://10.42.0.58:10250/readyz,是 metric-server 访问 kubelet 容器,因为 metrics-server 默认会通过 kubelet API 获取节点、容器、Pod 的资源指标数据\n后来返回404:\n可能metrics-server 镜像中并没有暴露 /readyz 这个 HTTP 路径 某些版本只支持 /healthz 或 / 路径 prometheus prometheus 在 k8s 的集群监控中也是十分常见的工具,可抓取 所有组件、应用、Exporter 的细粒度指标,并且可以做到报警,可视化,长期持久化等功能.\n所以我们测试一下在我们的集群中使用情况.\n操作流程 与 metric-server 不同的是,prometheus 支持 riscv 架构,所以我们可以直接使用.\n在官网下载好安装包传到开发板 server 节点上,进行解压,并将可执行文件和配置文件都放到对应目录中\nsudo cp ~/prometheus-2.53.4.linux-riscv64/prometheus /usr/local/bin/prometheus sudo cp ~/prometheus-2.53.4.linux-riscv64/prometheus.yml /etc/prometheus/prometheus.yml 为其编写 service 文件,交由 systemd 管理\n[Unit] Description=Prometheus Monitoring Wants=network-online.target After=network-online.target [Service] User=debian ExecStart=/usr/local/bin/prometheus \\ --config.file=/etc/prometheus/prometheus.yml \\ --storage.tsdb.path=/var/lib/prometheus \\ --web.listen-address=0.0.0.0:9090 \\ --web.enable-lifecycle Restart=on-failure [Install] WantedBy=multi-user.target 这里的 User 可以选择为 prometheus 独立创建一个用户;数据存储在 /var/lib/prometheus 下\n同时创建对应的目录并为其赋予对应权限.\nsudo mkdir -p /etc/prometheus /var/lib/prometheus sudo chown -R jimlt:jimlt /etc/prometheus /var/lib/prometheus 重新加载 systemd 并启动\nsudo systemctl daemon-reexec sudo systemctl daemon-reload sudo systemctl enable prometheus sudo systemctl start prometheus 验证其状态:\nsudo systemctl status prometheus # prometheus.service - Prometheus Monitoring Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; preset: enabled) Active: active (running) since Sun 2025-05-18 20:59:08 CST; 12min ago Invocation: a99f1274a13a4ae89ecb9726ece102a4 Main PID: 5337 (prometheus) Tasks: 10 (limit: 18020) Memory: 41.2M (peak: 41.9M) CPU: 5.667s CGroup: /system.slice/prometheus.service └─5337 /usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/var/lib/prometheus --web.listen-address=0.0.0.0:9090 --web.enable-\u0026gt; 此时我们就可以去浏览器中输入 localhost:9000 访问其监控页面.\n关于其安装部署流程就告一段落了,后续我们可以在其配置文件中添加 job,让其监控集群中的数据,关于其后续的更深入操作,见以后的文章吧!\n总结 本文尝试了 k3s 集群中两种监控方案的实现,并解决了 metric-server 运行过程中的探针问题.为后续集群的监控奠定了基础.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/038k3sep12%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 \u003ca href=\"https://github.com/kubernetes-sigs/metrics-server\"\u003emetric-server\u003c/a\u003e;\n在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.\u003c/p\u003e","title":"K3sEP12——监控集群的两种方式"},{"content":"引子 在与开发板打交道的过程中,网络成为了出现问题最多的地方,之前我们谈过 DNS 的事情,今天我们来说说 Ping 这个命令,每次网络出问题的时候最喜欢的命令就是\nping baidu.com # 最喜欢 ping 前司了 ping 8.8.8.8 那 ping 命令到底是用来干什么的呢?其中的原理是什么呢?接下来让我们一起探索一下.\n什么是 ping Ping 即是一个动词又是一个名词,名词在于他本身就是一个应用,动词在于我们常常使用 ping 命令.\n甚至,我们平时打游戏的时候常说的就是这个 ping,当你的 ping 值很高时,那代表你可能有些卡了!\n原理 当我们在主机上使用 ping IP/DomainName 时\n我们的主机就会向远程的服务器发送多个 ICMP echo requestes 这是 ICMP 协议中专门用于测试网络连通性的一种报文; 其对应的就是 ICMP echo reply,众所周知,ICMP 作用于网络层,IP所在的层. 这个报文会带着一个发送的时间戳和序号 其不依赖于 TCP/UDP,而是直接封装在 IP 包中,所以自然是一个不可靠传输. 如果目标主机可达,返回响应 Ping 工具靠着应答时间与发送时间计算时间差即 RTT,并统计包数量,计算丢包率,同时还提供 TTL 生存时间值(可以经过多少个路由器);如下所示 ping baidu.com PING baidu.com (110.242.68.66): 56 data bytes 64 bytes from 110.242.68.66: icmp_seq=0 ttl=47 time=68.472 ms 64 bytes from 110.242.68.66: icmp_seq=1 ttl=47 time=47.705 ms 64 bytes from 110.242.68.66: icmp_seq=2 ttl=47 time=30.750 ms 64 bytes from 110.242.68.66: icmp_seq=3 ttl=47 time=37.783 ms 64 bytes from 110.242.68.66: icmp_seq=4 ttl=47 time=43.311 ms 64 bytes from 110.242.68.66: icmp_seq=5 ttl=47 time=33.979 ms ^C --- baidu.com ping statistics --- 6 packets transmitted, 6 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 30.750/43.667/68.472/12.430 ms 关于数据包传递的细节,我们后续在其他文章中更新,这会涉及到网络层的内容.\n用途 知道了 Ping 的工作原理,我们来看看他有什么用途:\n最常见的就是验证远程的服务器,网站,网络设备能不能通过我们的网络访问 从结果可以看到,还可以测量数据包的 RTT(time字段) 和丢包率来评估网络的延迟情况 丢包率受多个因素影响,可能是防火墙,可能是网络拥塞 还可以测试本地环回地址以此来验证本地的 TCP/IP 栈是否正常 所以回到上面游戏里的 ping 值,当 ping 值过高,就表示着我们与当前连接的远程服务器之间的网络并不畅通,数据包的传输比较慢,或者丢包严重,自然就造成游戏操作变卡.\ntraceroute 看完了上面 ping 的作用,可以顺便思考一下,Ping 不能看到什么?\n没错,ping 看不到数据包在网络中的路径——从哪里到哪里的. 所以我们需要另外一个工具来查看路由中的每个跳转,以此来跟踪整条路径,判断出在哪里出现了丢包,超时等情况.\ntraceroute 的工作过程如下:\n发送第一个 TTL = 1 的数据包 → 第一个路由器返回 “TTL exceeded” → 记录第 1 跳 IP 和 RTT 然后发送 TTL = 2 的数据包 → 第二个路由器返回 → 记录第 2 跳 一直增加 TTL，直到目标主机响应，或者达到最大 TTL traceroute to google.com (172.217.31.142), 64 hops max, 40 byte packets 1 * * * 2 * * * 3 * * * 从这例子来看,很可惜,应该是 google 的防火墙屏蔽了我的请求.但这并不意味着 google 不能访问.\n那再试试我的路由器.\ntraceroute 192.168.1.1 traceroute to 192.168.1.1 (192.168.1.1), 64 hops max, 40 byte packets 1 192.168.1.1 (192.168.1.1) 30.071 ms 2.634 ms 2.095 ms 很明显,从我的主机到路由器就只需要一跳.\n所以,我们日后在排查的时候可以将 ping 和 traceroute 结合起来,前者用于检测是否可以连通,后者来检查如果有问题哪里出了问题.\nmtr 这是一个更强的 traceroute,mtr 会实时显示每跳的 RTT 和丢包情况，比 traceroute 更直观.推荐使用.\n可以看到,分为前几跳,中间几跳和最后.\n前几跳可能在局域网中 中间几跳是具体的 IP 地址,这些是电信的骨干网,特别是最后一个是电信的网络出海口. 有些运营商路由器默认不回应 mtr 的 ICMP/UDP 包，这不代表一定有问题 Ping spoofing where the main goal is to overwhelm the victim\u0026rsquo;s server by sending it an extremely large number of echo request packets within a short period of time.\n具体而言,攻击者可以伪造 ping 命令的 ICMP 包,将一个被害者的 IP 作为 源 IP,目标主机随意; 之后对一批目标主机发起 ping 命令,那么就会造成数不胜数的 ICMP reply 返回给被害者,形成反射攻击,是 DDoS 攻击的一种,称为 DDos放大.\nhping3 -1 --spoof 10.0.0.2 8.8.8.8 # 被害者为10.0.0.2 如何避免呢?这里仅仅列出几个常见做法,没有深究具体过程.\n过去的方法是采用防火墙即 iptables 限制 ping 的速率和来源 现在常用的有入侵检测系统,会检查异常的 ICMP 活动 对源地址进行验证(反向路径过滤) 实际应用 在实际的开发板操作过程中,有时遇到 ping 不通的情况,但实际情况就是自己的网卡,网络不太行.(因为我的随身wifi不太好)\n在网络正常的情况下,我 ping 8.8.8.8, baidu.com 都是成功的. 这里就要多嘴一句,你认为 ping 命令与 DNS 有关吗?一个网络层,一个应用层,好像\u0026hellip;\n其实是可能有关的,例如 ping google.com 我们需要用到 DNS 来解析此域名为 IP 地址.\n在这个命令中会先触发一次 DNS 得到 IP 地址,然后通过 socket 接口传给内核网络栈,最终传给网络层去封装,再层层递进\u0026hellip;\n总结 总结一下,这篇文章还是比较简单的,因为 ping 命令可能从小时候的微机课都或多或少地接触过了,一个人人都能上手的检测网络连通性的好办法.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n引用 https://www.techtarget.com/searchnetworking/definition/ping ChatGPT ","permalink":"http://localhost:1313/posts/037%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9Cping%E5%91%BD%E4%BB%A4/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在与开发板打交道的过程中,网络成为了出现问题最多的地方,之前我们谈过 DNS 的事情,今天我们来说说 Ping 这个命令,每次网络出问题的时候最喜欢的命令就是\u003c/p\u003e","title":"兴趣八股之计算机网络EP03——Ping命令"},{"content":"引子 继上一篇文章中我们成功迁移了 k3s 到 riscv64 上之后,我们使用另外一个 riscv 开发板作为 agent 节点进行集群化测试.\n环境初始化 仍然按照 EP09 中的步骤进行环境的初始化.\n执行到安装时,请按照下面的步骤进行,因为 agent 与 server 的步骤略有不同.\n部署agent 首先,我们将 k3s 的可执行文件放入 agent 中.\nsudo cp /home/debian/k3s /usr/local/bin/k3s sudo chmod +x /usr/local/bin/k3s 接着,下载安装脚本并写好配置文件(这次我们采用预先配置的方法进行安装)\ncurl -sfL https://get.k3s.io -o k3s-install.sh # 编写config sudo vi /etc/rancher/k3s/config.yaml # 内容如下: server: https://\u0026lt;SERVER_IP\u0026gt;:6443 token: \u0026lt;YOUR_TOKEN\u0026gt; node-name: agent197 这里的 SERVER_IP 就是我们的 server 节点的 IP 地址; token 需要在 server 节点中获取 值得注意的是，如果我们切换了整个集群的网络，这里的 SERVER_IP 需要进行手动更改并重启 agent。\n# 来到 server 节点 cat /var/lib/rancher/k3s/server/token # 复制得到的结果 接着,继续配置 agent 上的镜像拉取规则,即 registries.yaml;这里我们设置与 server 相同的配置\nsudo vi /etc/rancher/k3s/registries.yaml # 填充内容如下 mirrors: docker.io: endpoint: - \u0026#34;https://jimlt.bfsmlt.top\u0026#34; # 这里根据上 rewrite: \u0026#34;^library/(.+)$\u0026#34;: \u0026#34;${1}\u0026#34; \u0026#34;^library/pause$\u0026#34;: \u0026#34;pause\u0026#34; \u0026#34;^rancher/mirrored-library-traefik$\u0026#34;: \u0026#34;traefik\u0026#34; \u0026#34;^rancher/mirrored-metrics-server$\u0026#34;: \u0026#34;metrics-server\u0026#34; \u0026#34;^rancher/klipper-helm$\u0026#34;: \u0026#34;klipper-helm\u0026#34; \u0026#34;^library/klipper-lb$\u0026#34;: \u0026#34;klipper-lb\u0026#34; \u0026#34;^rancher/local-path-provisioner$\u0026#34;: \u0026#34;local-path-provisioner\u0026#34; \u0026#34;^rancher/mirrored-coredns-coredns$\u0026#34;: \u0026#34;coredns/coredns\u0026#34; \u0026#34;^busybox$\u0026#34;: \u0026#34;riscv64/busybox\u0026#34; # 注意,因为我们采用了 TLS 的私有仓库,所以这里需要添加 configs configs: \u0026#34;jimlt.bfsmlt.top\u0026#34;: # 你的私有仓库地址 auth: username: xxx password: yyyyyyyy 最后,我们使用命令部署 agent,跳过下载,并设置为 agent 节点\nINSTALL_K3S_SKIP_DOWNLOAD=\u0026#34;true\u0026#34; INSTALL_K3S_EXEC=\u0026#34;agent\u0026#34; bash -x k3s-install.sh 至此,我们完成了 agent 部署,用以下步骤来验证其成功运行.\n# 在 agent 上 sudo systemctl status k3s-agent # 在 server 中 sudo kubectl get nodes 得到结果如下:\nNAME STATUS ROLES AGE VERSION agent197 Ready \u0026lt;none\u0026gt; 175m v1.31.2+k3s-4c017da7 revyos-lpi4a Ready control-plane,master 41h v1.31.2+k3s-4c017da7 值得注意的是, agent 只是工作节点,我们的那些系统组件例如 coredns 只会在 server 节点执行;除了 pause 镜像以外,这是一个最最最基础的镜像,无论如何都需要.\n同样的, agent 上无法进行 kubectl 操作,因为 kubectl 需要与 API Server 通信,而 API Server 默认运行在 Server 节点上.\n最后我们再来查看一下集群的信息.\nsudo kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy 测试 redis 应用 在 Server 上写好两个 yaml 文件,一个作为服务端,一个座位客户端.\n服务端,redisTest-deployment.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis # 设置 pod 标签为 app: redis spec: containers: - name: redis image: jimlt.bfsmlt.top/redis:v7.2.4 ports: - containerPort: 6379 --- apiVersion: v1 kind: Service metadata: name: redis-service spec: selector: app: redis # 将流量转发给 app: redis 的 pod ports: - port: 6379 targetPort: 6379 protocol: TCP type: ClusterIP 客户端,redisTest-client.yaml:\napiVersion: v1 kind: Pod metadata: name: redis-client spec: containers: - name: redis-client image: jimlt.bfsmlt.top/redis:v7.2.4 # 自建的 RISC-V 客户端镜像 command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] 在服务端执行kubectl apply -f redis-deployment.yaml和kubectl apply -f redis-client.yaml\n可以查看各自的 pod 信息,查看 service 信息与实际转发的信息\n# pod 信息 sudo kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES hello-7ff6b9f9bc-2kfff 1/1 Running 4 (12m ago) 21h 10.42.0.40 revyos-lpi4a \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; redis-7c667cb775-d72lw 1/1 Running 1 (12m ago) 109m 10.42.0.42 revyos-lpi4a \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; redis-client 1/1 Running 0 9m30s 10.42.1.9 agent197 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # service 信息 sudo kubectl get svc redis-service -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR redis-service ClusterIP 10.43.184.199 \u0026lt;none\u0026gt; 6379/TCP 5h59m app=redis # 实际转发的信息 sudo kubectl get endpoints redis-service NAME ENDPOINTS AGE redis-service 10.42.0.42:6379 6h 查看 redis-client 的构建过程,可以发现其被 API_Server 调度到了 agent197 上\nsudo kubectl describe pod redis-client # Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 70s default-scheduler Successfully assigned default/redis-client to agent197 Normal Pulled 69s kubelet Container image \u0026#34;jimlt.bfsmlt.top/redis:v7.2.4\u0026#34; already present on machine Normal Created 69s kubelet Created container redis-client Normal Started 68s kubelet Started container redis-client 进入客户端 Pod 并连接 Redis,指定连接的 redis 服务器地址为 redis-service,这在 yaml 文件中指明了,会自动解析成 Redis Pod 的 ClusterIP 地址\nsudo kubectl exec -it redis-client -- sh # 在容器内执行 redis-cli -h redis-service 执行一些命令进行测试,因为我的 redis 配置文件中配置了密码,所以执行 auth {password}时视自己情况而定\nredis-service:6379\u0026gt; get name (nil) redis-service:6379\u0026gt; set name jimlt OK redis-service:6379\u0026gt; get name \u0026#34;jimlt\u0026#34; 至此,这个简单的 redis 测试应用就部署成功了.\n构建多架构镜像(可选) 如果我们后期将其他架构的机器加入到集群里面,我们在拉取镜像时可能遇到架构不兼容问题,所以我们需要构建一个多架构的 List 在私有仓库中.\n在 macos 上执行下面的命令,其中 Dockerfile 来自于这篇文章.\ndocker build --platform linux/amd64,linux/arm64,linux/riscv64 --network host -f Dockerfile.redis -t jimlt.bfsmlt.top/redis:multiarch . 但是却遇到 riscv64 有关的错误,这在我们最初构建时并没有发生,目前暂时没有找到好的处理方法.\n5.934 CC adlist.o 5.934 cc: error: unrecognized argument in option \u0026#39;-mabi=lp64d\u0026#39; 5.934 cc: note: valid arguments to \u0026#39;-mabi=\u0026#39; are: ilp32 lp64; did you mean \u0026#39;lp64\u0026#39;? 5.934 make[1]: *** [Makefile:436: adlist.o] Error 1 5.934 make[1]: Leaving directory \u0026#39;/redis/src\u0026#39; 5.934 make: *** [Makefile:6: all] Error 2 总结 k3s 已经成功地在一个由 riscv 开发板所组成的集群中运行起来了,可以创建,调度 Pod,可以做到 Pod 之间的通信.\n在这里,需要对 Antony Chazapis 对 k3s for riscv 所作出的贡献致以最高的敬意!\n当然,这只是长征路上的第一步,我们才刚刚起步而已,后续围绕 k3s 进行的分布式框架编写,分布式应用的依赖分析以及最后要形成的一键部署工具都还是任重道远啊.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/036k3sep11%E5%8A%A0%E5%85%A5agent%E8%8A%82%E7%82%B9%E8%BF%9B%E8%A1%8C%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e继上一篇文章中我们成功迁移了 k3s 到 riscv64 上之后,我们使用另外一个 riscv 开发板作为 agent 节点进行集群化测试.\u003c/p\u003e\n\u003ch2 id=\"环境初始化\"\u003e环境初始化\u003c/h2\u003e\n\u003cp\u003e仍然按照 EP09 中的步骤进行环境的初始化.\u003c/p\u003e\n\u003cp\u003e执行到安装时,请按照下面的步骤进行,因为 agent 与 server 的步骤略有不同.\u003c/p\u003e","title":"K3sEP11——加入agent节点进行集群测试"},{"content":"引子 在前面的一系列文章中,我们知道了要想在 riscv 开发板上适配 k3s,我们要做以下两件事\n系统级别的重要镜像进行适配并可以成功拉取到开发板 执行 make 或者 download,build,package-cli 脚本 对于第一件事,已经有人做过了,我们只需要使用他做好的镜像,并将其推送到我们的私有仓库.\n对于第二件事,在操作过程中我们可能会遇到大大小小的问题,其实在之前的文章操作中我们都遇到过,这里汇总一下.\n构建过程 由于 make 过程会执行 dapper 的容器构建,会涉及到下载,由于众所周知的网络问题(我不可能给开发板上代理吧),所以我们放弃 make 过程.\n直接采用执行三个脚本的方法,在此过程中我们需要修改一些部分的代码.\n避免Golang版本检测 由于在脚本执行过程中,会去检测 当前代码中 k8s 的 Golang 的版本,具体代码在 /scripts/validate 如下:\necho Running: go version if ! go version | grep -s \u0026#34;go version ${VERSION_GOLANG} \u0026#34;; then echo \u0026#34;Unexpected $(go version) - Kubernetes ${VERSION_K8S} should be built with go version ${VERSION_GOLANG}\u0026#34; exit 1 fi 这个 VERSION_GOLANG 来自于 /scripts/version.sh\nDEPENDENCIES_URL=\u0026#34;https://raw.githubusercontent.com/kubernetes/kubernetes/${VERSION_K8S}/build/dependencies.yaml\u0026#34; VERSION_GOLANG=\u0026#34;go\u0026#34;$(curl -sL \u0026#34;${DEPENDENCIES_URL}\u0026#34; | yq e \u0026#39;.dependencies[] | select(.name == \u0026#34;golang: upstream version\u0026#34;) 故它会去使用 curl 命令查找这个 yaml 文件,接着 yq 读取其中 golang 的字段,具体内容如下:\n# Golang - name: \u0026#34;golang: upstream version\u0026#34; version: 1.22.8 refPaths: - path: .go-version - path: build/build-image/cross/VERSION - path: staging/publishing/rules.yaml match: \u0026#39;default-go-version\\: \\d+.\\d+(alpha|beta|rc)?\\.?(\\d+)?\u0026#39; - path: test/images/Makefile match: GOLANG_VERSION=\\d+.\\d+(alpha|beta|rc)?\\.?\\d+ 而这个检查 golang 版本的行为会出现在我们运行 k3s 的过程中,源码中 server 和 agent 的启动第一步就是 cmds.MustValidateGolang()\n但是,由于众所周知的网络问题, curl 不动,😅;如果你不管这个问题就会出现以下现象:\nMay 14 18:16:01 revyos-lpi4a k3s[1447]: time=\u0026#34;2025-05-14T18:16:01+08:00\u0026#34; level=fatal msg=\u0026#34;Failed to validate golang version: incorrect golang build version - kubernetes v1.31.2 should be built with go, runtime version is go1.23.6\u0026#34; 可以发现,should built with go这是错误的,至少后面也应该是 goxxx,而不是 go——证明其没有读取到 yaml 文件中的内容.\n这也就导致运行 k3s 时第一步的检查版本代码过不去,即下方这里的代码: pkg/cli/agent/agent.go\nfunc ValidateGolang() error { k8sVersion, _, _ := strings.Cut(version.Version, \u0026#34;+\u0026#34;) if version.UpstreamGolang == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;kubernetes golang build version not set - see \u0026#39;golang: upstream version\u0026#39; in https://github.com/kubernetes/kubernetes/blob/%s/build/dependencies.yaml\u0026#34;, k8sVersion) } if v, _, _ := strings.Cut(runtime.Version(), \u0026#34; \u0026#34;); version.UpstreamGolang != v { return fmt.Errorf(\u0026#34;incorrect golang build version - kubernetes %s should be built with %s, runtime version is %s\u0026#34;, k8sVersion, version.UpstreamGolang, v) } return nil } 所以我们最后干了什么:\n将 validate 中关于检验 Golang 版本的代码注释掉 将 server 与 agent 中的 cmds.MustValidateGolang()注释掉 在 yaml 文件中找到适合的 golang 版本作为我们开发板上要用的版本,见上一篇文章环境配置. 避免下载过程过慢 在执行 download 过程中,其实并不需要在开发板上执行,这一点我们在这篇文章中提过,具体做法如下:\n# 在我们的主机上执行 export GOARCH=riscv64 export ARCH=riscv64 export OS=linux ./scripts/download # 然后拷贝build目录到开发板的k3s源码目录中 scp build/ root@ip:/root/k3s 开发板 airgap 安装 在开发板上执行 build 与 package-cli 脚本,build过程中可能会遇到网络问题,速度较慢,但是只要我们的环境,工具都正确,不会出现其他问题.\n执行完两个脚本后会在 /dist/artifacts目录下产生一个 k3s-riscv 可执行二进制文件,将作为我们后续安装的文件.\n接着我们进行 airgap 安装 server 节点,利用上一篇文章中的安装脚本即可完成安装.\n安装后检查是否成功:\nsudo systemctl status k3s sudo kubectl get nodes 看到自己的节点即为成功.\n手动拉取镜像 在 2025-05-14 实验版本中,系统镜像并没有自动拉取,这点我们还在尝试,因为官网还有一种为其设置配置文件的方式 /etc/rancher/k3s/config.yaml\n所以我们使用 sudo kubeclt get pods -A 来查看这些系统镜像,并用sudo kubectl describe pod \u0026lt;podName\u0026gt; -n kube-system 来查看其错误原因\n错误原因大部分都来自于镜像拉取失败,因为它还在拉取默认的 rancher 镜像. 所以我们需要修改 registries.yaml 文件,为其添加 rewrite,使其将默认镜像改为我们私有仓库中的镜像.\nmirrors: docker.io: endpoint: - \u0026#34;https://jimlt.bfsmlt.top\u0026#34; # 这里根据上 rewrite: \u0026#34;^library/pause$\u0026#34;: \u0026#34;pause\u0026#34; \u0026#34;^rancher/mirrored-library-traefik$\u0026#34;: \u0026#34;traefik\u0026#34; \u0026#34;^rancher/mirrored-metrics-server$\u0026#34;: \u0026#34;metrics-server\u0026#34; \u0026#34;^rancher/klipper-helm$\u0026#34;: \u0026#34;klipper-helm\u0026#34; \u0026#34;^library/klipper-lb$\u0026#34;: \u0026#34;klipper-lb\u0026#34; \u0026#34;^rancher/local-path-provisioner$\u0026#34;: \u0026#34;local-path-provisioner\u0026#34; \u0026#34;^rancher/mirrored-coredns-coredns$\u0026#34;: \u0026#34;coredns/coredns\u0026#34; \u0026#34;^busybox$\u0026#34;: \u0026#34;riscv64/busybox\u0026#34; # 注意,因为我们采用了 TLS 的私有仓库,所以这里需要添加 configs configs: \u0026#34;jimlt.bfsmlt.top\u0026#34;: # 你的私有仓库地址 auth: username: xxx password: yyyyyyyy 然后需要手动拉取这些镜像,利用 sudo crictl pull 命令拉取. 最终结果如下:\nsudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-56f6fc8fd7-28mn7 1/1 Running 1 (18h ago) 19h kube-system helm-install-traefik-crd-98v8n 0/1 Completed 0 19h kube-system helm-install-traefik-j7czp 0/1 Completed 0 19h kube-system local-path-provisioner-5cf85fd84d-hznms 1/1 Running 0 19h kube-system metrics-server-5985cbc9d7-v7dch 1/1 Running 0 19h kube-system svclb-traefik-699324d1-48lvn 2/2 Running 0 93m kube-system traefik-57b79cf995-xnv79 1/1 Running 0 93m 这些系统镜像都已经在成功运行.\n测试pod 在主机上构建一个小的镜像推送到私有仓库中去\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func handler(w http.ResponseWriter, r *http.Request) { message := os.Getenv(\u0026#34;MESSAGE\u0026#34;) if message == \u0026#34;\u0026#34; { message = \u0026#34;Hello from RISC-V!\u0026#34; } fmt.Fprintf(w, \u0026#34;%s\\n\u0026#34;, message) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) fmt.Println(\u0026#34;Listening on :7080\u0026#34;) http.ListenAndServe(\u0026#34;:7080\u0026#34;, nil) } FROM golang:1.22.12-alpine3.20 AS builder WORKDIR /app COPY . . RUN go build -o hello-k8s main.go FROM alpine:latest WORKDIR /root/ COPY --from=builder /app/hello-k8s . EXPOSE 7080 ENTRYPOINT [\u0026#34;./hello-k8s\u0026#34;] 构建并推送,注意设置 platform 与 network\ndocker build --platform=linux/riscv64 --network host -t jimlt.bfsmlt.top/hello-kubernetes:1.0.1 . docker push jimlt.bfsmlt.top/hello-kubernetes:1.0.1 这里看着 push 的结果可以思考一下为什么 push 的时候显示 traefik?\nThe push refers to repository [jimlt.bfsmlt.top/hello-kubernetes] f8b25a4c3a38: Pushed ecbb5a20f6d5: Pushed 7df33f7ad8be: Mounted from traefik 4f4fb700ef54: Layer already exists 1.0.1: digest: sha256:78107dc8255d5b7eb09031d1c26fee9b10ee896237ac8b8acc1914e83b704325 size: 857 因为私有仓库中的 traefik 镜像已经有了 7d 这个镜像层(Docker 镜像的基本概念,镜像由层组成) 故这里复用了这个镜像层,就不用再重新构建这层了 在开发板上写一个 hello-deploy.yaml 进行部署测试\napiVersion: v1 kind: Service metadata: name: hello spec: type: ClusterIP ports: - port: 7080 targetPort: 7080 selector: app: hello --- apiVersion: apps/v1 kind: Deployment metadata: name: hello spec: replicas: 1 selector: matchLabels: app: hello template: # 注意这里在 spec 下，不是在 selector 下 metadata: labels: app: hello spec: containers: - name: hello-kubernetes image: jimlt.bfsmlt.top/hello-kubernetes:1.0.0 env: - name: MESSAGE value: \u0026#34;Hello RISC-V!\u0026#34; ports: - containerPort: 7080 运行此 deployment 并执行测试\nsudo kubectl apply -f hello-deploy.yaml sudo kubectl port-forward svc/hello 7080:7080 curl http://localhost:7080 # 输出结果 Hello RISC-V! 总结 至此,我们的移植过程暂时告一段落了,可以运行起一个简单的小应用,接下来我们会将另一个开发板作为 agent 加入其中进行集群测试.\n不过其中对源码的修改方式(避免检测)也是一种无奈的尝试,后续看看有没有什么其他的好的方式;还有手动拉取系统镜像这一步也不应该这样做,再看吧.\n最后问自己一句:问题解决了吗?这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/035k3sep10%E6%88%90%E5%8A%9F%E8%BF%81%E7%A7%BBk3s%E8%87%B3riscv64%E5%BC%80%E5%8F%91%E6%9D%BF%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在前面的一系列文章中,我们知道了要想在 riscv 开发板上适配 k3s,我们要做以下两件事\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e系统级别的重要镜像进行适配并可以成功拉取到开发板\u003c/li\u003e\n\u003cli\u003e执行 make 或者 download,build,package-cli 脚本\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于第一件事,已经有人做过了,我们只需要使用他做好的镜像,并将其推送到我们的私有仓库.\u003c/p\u003e","title":"K3sEP10——迁移k3s至riscv64开发板的过程记录"},{"content":"前瞻 本文为了总结在拿到一个新的开发板后具体该如何操作,包括烧录镜像,配置 k3s 所需环境等操作.\n烧录镜像 关于烧录操作,无需多言,请关注这篇文章.本文主要说一下镜像问题.\n在选择 OS 镜像的时候,由于 riscv 的实验性,往往在阿里云、清华源等知名镜像源并没有提供,通常是中科院自己的镜像源,而有时有些镜像并没有进行稳定的维护; 甚至有的镜像质量并不好,例如 openEuler24 LTS 竟然不提供 wifi 模块.😇\n所以经过我的多次尝试(重新烧录镜像配置环境的折磨)我找到了一款适合实验稳定(暂时稳定)操作的 OS 镜像,RevyOS\n其适配了 LicheePi 4A 和 Milk-V Meles 正好都是我手上有的设备;虽然同样使用的是中科院的镜像管理,但是据我在其 github 页面的观察,其 issue 的时间在最近较多,回复也蛮及时; 同时 ChangeLog 也有稳定的更新,故以此为后续的实验镜像.\n环境搭建 与我们之前想的一样,就像宝塔这样的软件提供了一键部署,一键配置这样的操作,这对重新构建一个开发板上的系统来说真的是很方便;有机会我们可以把下面的操作全部放在一个脚本中,也做到一键操作.\n在第二篇文章中我们说过了如何配置适合容器的 OS 环境, 但是后面在实验中还是遇到了些问题,所以这里重新汇总一下.(并且在实验过程中本文会一直更新)\n更改 DNS 配置 如果你的 DNS 服务器地址无法正确解析,请修改 DNS 设置为 8.8.8.8,1.1.1.1等静态公用地址\n# 换成自己的网络名称 sudo nmcli connection modify \u0026#34;niuma\u0026#34; ipv4.ignore-auto-dns yes ipv4.dns \u0026#34;8.8.8.8 1.1.1.1\u0026#34; # 重启连接 sudo nmcli connection down \u0026#34;niuma\u0026#34; \u0026amp;\u0026amp; nmcli connection up \u0026#34;niuma\u0026#34; 注意,如果你是在 ssh 或者其他远程连接环境下进行这步操作,那么后续会因为 wifi 断开而无法连接,所以我们推荐执行这步操作在有显示器的环境下进行.\nApt 截止2025.05.13,中科院仓库存在 KEY 过期问题,官方给出了解决方案\n我采用的是 ChatGPT 给出的方案,在源地址前面加上[trusted=yes],\nsudo vi /etc/apt/sources.list 最终结果如下:\ndeb [trusted=yes] https://fast-mirror.isrc.ac.cn/revyos/revyos-gles-21 revyos-gles-21 main deb [trusted=yes] https://fast-mirror.isrc.ac.cn/revyos/revyos-addons revyos-addons main deb [trusted=yes] https://fast-mirror.isrc.ac.cn/revyos/revyos-kernels revyos-kernels main deb [trusted=yes] https://fast-mirror.isrc.ac.cn/revyos/revyos-base sid main 之后再进行 apt 操作就可以了.(更新,后续还是建议使用官方的解决方案)\nsudo sh -c \u0026#39;gpg --keyserver keyserver.ubuntu.com --recv-keys 2FB3A9E77911527E \u0026amp;\u0026amp; \\ gpg --export 2FB3A9E77911527E \u0026gt; /etc/apt/trusted.gpg.d/revyos-keyring.gpg\u0026#39; sudo apt update; sudo apt upgrade -y 在这个过程中如果遇到 gpg: keyserver receive failed: Server indicated a failure这个问题,通常是网络问题,我们需要先更改 DNS,具体见上方 DNS 操作.\nGCC等构建所需工具 我们写一个脚本来一键检测并安装可能所需的工具.\n#!/bin/bash set -e echo \u0026#34;🔍 检测当前系统包管理器...\u0026#34; if command -v apt \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo \u0026#34;✅ 使用 apt 安装依赖...\u0026#34; sudo apt update sudo apt install -y \\ build-essential \\ make \\ gcc \\ git \\ curl \\ pkg-config \\ libseccomp-dev \\ btrfs-progs \\ iptables \\ libapparmor-dev \\ libgpgme-dev \\ libdevmapper-dev \\ libprotobuf-dev \\ protobuf-compiler \\ libnl-3-dev \\ libnl-route-3-dev \\ libnet-dev \\ libudev-dev \\ libsystemd-dev \\ libtool \\ autoconf \\ automake \\ rsync \\ unzip \\ zstd \\ socat elif command -v dnf \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo \u0026#34;✅ 使用 dnf 安装依赖...\u0026#34; sudo dnf install -y \\ gcc \\ gcc-c++ \\ git \\ make \\ curl \\ pkgconf-pkg-config \\ libseccomp-devel \\ btrfs-progs-devel \\ iptables \\ libapparmor-devel \\ gpgme-devel \\ device-mapper-devel \\ protobuf-devel \\ protobuf-compiler \\ libnl3-devel \\ libnet-devel \\ systemd-devel \\ libudev-devel \\ libtool \\ autoconf \\ automake \\ rsync \\ unzip \\ socat else echo \u0026#34;❌ 无法识别的包管理器，请手动安装依赖。\u0026#34; exit 1 fi echo \u0026#34;✅ 所有依赖安装完毕。你现在可以构建 K3s。\u0026#34; 此脚本来自于 ChatGPT.\nGit apt 安装 Git,并配置镜像地址\ngit config --global url.\u0026#34;https://gh-proxy.com/github.com/\u0026#34; .insteadOf \u0026#34;https://github.com/\u0026#34; cat .gitconfig 得到我们所配置的地址即为正确.\nDocker 第一种安装方式:\nsudo apt install docker.io sudo usermod -aG docker $USER 如果这种不行,请使用这种:(后面的加入 root 组操作是相同的)\nsudo apt-get install docker docker-compose sudo groupadd docker sudo usermod -aG docker $USER newgrp docker 最终执行 docker images hello-world 检测是否成功.\n配置代理镜像:()\n# 修改 vi /etc/docker/daemon.json # 如果没有这个文件，自己手动新建（我当时只有一个key.json文件） # 将下面内容复制进去 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.1ms.run\u0026#34;, \u0026#34;https://docker.linkedbus.com\u0026#34; ] } # 加载配置并重启 sudo systemctl daemon-reload sudo systemctl restart docker # 尝试拉取镜像 docker pull busybox docker run hello-world 注意,这里的代理是对 dockerhub 的代理;不会影响到我们的私有仓库. 🥸 对了,fk GFW ! 如果影响到了私有仓库,请暂时放弃代理.\nGolang 这里我们统一采用 go1.22.8 linux/riscv64 这个版本.原因见这篇文章,根据当前你的 k8s 版本进行对应变化.\n这里默认你已经有了此版本的 tar 文件(我是通过 scp 传过去的)\nsudo tar -C /usr/local -xzf go1.22.8.linux-riscv64.tar.gz sudo vi ~/.bashrc # Go environment export GOROOT=/usr/local/go export PATH=$GOROOT/bin:$PATH export GOPATH=$HOME/go export PATH=$GOPATH/bin:$PATH export GOPROXY=https://goproxy.cn,direct # source ~/.bashrc 使用 go version 检查得到 go1.22.8 这个正确版本.\nyq 用来解析 YAML 的工具,之前文章中也提到过.\n这里我们与 Golang 一样,从源码构建,因为包管理中的版本太老.\nwget https://github.com/mikefarah/yq/releases/download/v4.44.6/yq_linux_riscv64.tar.gz tar -zvxf yq_linux_riscv64.tar.gz mv yq_linux_riscv64.tar.gz yq chmod +x yq mv yq /usr/local/bin yq --version 如果你之前用 apt 安装了 yq,可能会存在错误的缓存,可以用hash -r清理缓存.\n权限问题 为了方便和避免可能出现的权限问题,我们为源码目录赋予 root 权限\nsudo chown -R $(whoami):$(whoami) . 访问仓库网络问题 如果我们访问自己的私有仓库时遇到网络问题,特别是 DNS 解析问题,通常是因为 Network Management 自动为我们设置好了 DNS 服务器地址.\n而这篇文章中我们谈到了这些 DNS 服务器很难不出问题,特别是在解析国外的服务器时.\n所以我们采用一个最简单的方法,修改 /etc/hosts 文件,将服务器的 IP 加入其中.\n# 请换成自己的 ip 与 域名 IP Domain 安装与卸载脚本 实验过程中可能出现问题导致 k3s 不能运行,在此情况下如果我们需要删除 k3s 或者重新安装 k3s,这里给出两种操作的脚本.\n卸载脚本 将其从 systemd 管理中移除.\n#!/bin/bash set -e echo \u0026#34;🔧 停止 k3s 服务（如在运行）...\u0026#34; sudo systemctl stop k3s || true echo \u0026#34;🚫 禁用 k3s 服务开机自启...\u0026#34; sudo systemctl disable k3s || true echo \u0026#34;🧹 删除 k3s systemd 服务文件...\u0026#34; sudo rm -f /etc/systemd/system/k3s.service sudo rm -f /etc/systemd/system/k3s.service.env echo \u0026#34;🔄 重新加载 systemd 守护进程...\u0026#34; sudo systemctl daemon-reload echo \u0026#34;✅ 验证 k3s 服务是否已清除...\u0026#34; sudo systemctl status k3s || echo \u0026#34;✅ k3s.service 已被成功移除。\u0026#34; echo \u0026#34;🎉 清理完成。\u0026#34; 安装脚本 将其重新加入 systemd 管理,前提是脚本所在路径与 k3s 二进制文件路径一致,不一致请修改脚本.\n#!/bin/bash set -e echo \u0026#34;📦 拷贝本地 k3s 到 /usr/local/bin ...\u0026#34; sudo cp /home/debian/k3s /usr/local/bin/k3s sudo chmod +x /usr/local/bin/k3s echo \u0026#34;🔧 配置私有镜像仓库 registries.yaml ...\u0026#34; sudo mkdir -p /etc/rancher/k3s cat \u0026lt;\u0026lt;EOF | sudo tee /etc/rancher/k3s/registries.yaml mirrors: docker.io: endpoint: - \u0026#34;https://jimlt.bfsmlt.top\u0026#34; EOF echo \u0026#34;📥 获取 k3s 安装脚本 ...\u0026#34; curl -sfL https://get.k3s.io -o k3s-install.sh echo \u0026#34;🚀 执行安装脚本，使用本地可执行文件 ...\u0026#34; INSTALL_K3S_SKIP_DOWNLOAD=\u0026#34;true\u0026#34; bash -x k3s-install.sh 对kubectl命令进行优化 以下两种方法对我们日常使用 kubectl 命令带来了方便。\n去sudo化 由于默认的 k3s 配置文件在 /etc/rancher/k3s/k3s.yaml，这是一个只有 root 权限才可以访问的路径，这导致每次我们使用 kubectl 命令需要在前面加上 sudo.\n所以我们执行以下步骤将配置文件复制到主用户目录下，并配置好环境变量，使 kubectl 命令默认查找主用户目录下的文件。这样就可以不用再加上 sudo\nmkdir -p $HOME/.kube sudo cp /etc/rancher/k3s/k3s.yaml $HOME/.kube/config sudo chmod 600 $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=$HOME/.kube/config # 将上面的 export 命令添加到您的 shell 配置文件 (例如 ~/.bashrc 或 ~/.zshrc) 中，使其永久生效 echo \u0026#39;export KUBECONFIG=$HOME/.kube/config\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # 如果您使用 bash source ~/.bashrc # 使更改立即生效 使用kubectl-aliases简化 这是一个别名操作，可以让我们不再输入繁杂的 kubectl 命令，例如将 kubectl get pod 可以简化为 kgpo,具体操作也很简单，按照官网的步骤进行即可。\n后续更新 后续更新会根据构建 k3s 的进展以及可能的其他应用例如 Mobilenet 进行补充.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/034k3sep09%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%80%E4%B8%AA%E5%BC%80%E5%8F%91%E6%9D%BF%E7%8E%AF%E5%A2%83%E6%B1%87%E6%80%BB/","summary":"\u003ch2 id=\"前瞻\"\u003e前瞻\u003c/h2\u003e\n\u003cp\u003e本文为了总结在拿到一个新的开发板后具体该如何操作,包括烧录镜像,配置 k3s 所需环境等操作.\u003c/p\u003e\n\u003ch2 id=\"烧录镜像\"\u003e烧录镜像\u003c/h2\u003e\n\u003cp\u003e关于烧录操作,无需多言,请关注这篇\u003ca href=\"https://www.bfsmlt.top/posts/003li_riscv01/\"\u003e文章\u003c/a\u003e.本文主要说一下镜像问题.\u003c/p\u003e\n\u003cp\u003e在选择 OS 镜像的时候,由于 riscv 的实验性,往往在阿里云、清华源等知名镜像源并没有提供,通常是中科院自己的镜像源,而有时有些镜像并没有进行稳定的维护;\n甚至有的镜像质量并不好,例如 openEuler24 LTS 竟然不提供 wifi 模块.😇\u003c/p\u003e","title":"K3sEP09——初始化开发板环境操作汇总"},{"content":"引子 从 EP 07 中,我们可以看到已经有了成功的 riscv 移植工作,见此仓库. 发现其构建 riscv 采用了以下命令\n# Build rm -rf bin dist build ARCH=riscv64 SKIP_IMAGE=true SKIP_VALIDATE=true SKIP_AIRGAP=true make # Split cd dist/artifacts gzip \u0026lt; k3s-riscv64 | split -b 20M - k3s-riscv64.gz. 在此 pull request 中他也提到了这一路上的适配工作过程:开始他是使用qemu模拟,之后在硬件上测试并适配我们之前提到过的各种镜像.\n所以这篇文章中我们要做的是,弄明白它的构建过程,包括其 Makefile 与 Dockerfile.dapper\nMakefile 源码 分段解释此 Makefile\nTARGETS := $(shell ls scripts | grep -v \\\\.sh) GO_FILES ?= $$(find . -name \u0026#39;*.go\u0026#39; | grep -v generated) 通过 shell 命令列出 scripts 目录下所有不以 .sh 结尾的文件 查找项目中所有 .go 文件,排除 generated目录 .dapper: @echo Downloading dapper @curl -sL https://releases.rancher.com/dapper/v0.6.0/dapper-$$(uname -s)-$$(uname -m) \u0026gt; .dapper.tmp @@chmod +x .dapper.tmp @./.dapper.tmp -v @mv .dapper.tmp .dapper 下载 dapper 工具,根据系统类型下载对应版本 添加执行权限并验证版本 重命名为 .dapper $(TARGETS): .dapper DAPPER_DEBUG=1 DAPPER_DOCKER_BUILD_ARGS=\u0026#34;--network host\u0026#34; ./.dapper $@ 为上面不以 .sh 结尾的文件创建目标 每个目标依赖 .dapper 执行时使用 dapper 运行脚本 .PHONY: deps deps: go mod tidy 清理整理 go 依赖 release: ./scripts/release.sh 执行发布脚本 .DEFAULT_GOAL := ci .PHONY: $(TARGETS) build/data: mkdir -p $@ 设置默认目标为 ci 声明伪目标 创建 build/data 目录 .PHONY: binary-size-check binary-size-check: scripts/binary_size_check.sh .PHONY: image-scan image-scan: scripts/image_scan.sh $(IMAGE) format: gofmt -s -l -w $(GO_FILES) goimports -w $(GO_FILES) 检查二进制文件大小 扫描指定的镜像 格式化 Go 代码 .PHONY: local local: DOCKER_BUILDKIT=1 docker build \\ --build-arg=\u0026#34;REPO TAG GITHUB_TOKEN GOLANG GOCOVER DEBUG\u0026#34; \\ -t k3s-local -f Dockerfile.local --output=. . 使用Docker BuildKit 构建本地开发镜像 可以发现,我们可以在 make 后跟local,format,build/data,release这些参数,执行不同的运行逻辑.\n执行过程 当我们执行 make 时,默认执行 make ci,而 ci 是 Target 中的目标, 正是 script/ 下的一个文件,内容如下\n#!/bin/bash set -e SCRIPT_DIR=$(dirname $0) pushd $SCRIPT_DIR ./download ./validate ./build ./package popd $SCRIPT_DIR/binary_size_check.sh 又因为 Target 依赖于 .dapper,故如果 .dapper不存在,它会执行 .dapper 的逻辑(下载dapper)\n最终回到 DAPPER_DEBUG=1 DAPPER_DOCKER_BUILD_ARGS=\u0026quot;--network host\u0026quot; ./.dapper ci, 关于dapper我们下面来讲.\nDockerfile.dapper 什么是 dapper Dapper is a tool to wrap any existing build tool in an consistent environment. This allows people to build your software from source or modify it without worrying about setting up a build environment. The approach is very simple and taken from a common pattern that has adopted by many open source projects. Create a file called Dockerfile.dapper in the root of your repository. Dapper will build that Dockerfile and then execute a container based off of the resulting image. Dapper will also copy in source files and copy out resulting artifacts or will use bind mounting if you choose.\n.dapper 是一个可执行的构建工具，它会运行一个构建容器，然后在容器里执行 scripts/ci; 而看到上面 ci 文件中的内容会发现,其执行了 download、validate、build、package 这几个脚本.\n这不就和之前我们在 issue 中看到的情况有点像, 他直接自己本地执行的 download,build,package-cli\ndapper 源码 分段解释一下 Dockerfile.dapper 的源码\nARG GOLANG=golang:1.23.3-alpine3.20 FROM ${GOLANG} 基于此镜像作为基础镜像 # Set proxy environment variables ARG http_proxy ARG https_proxy ARG no_proxy ENV http_proxy=${http_proxy} \\ https_proxy=${https_proxy} \\ no_proxy=${no_proxy} 传递代理设置,这里可能会引发网络问题,除非你可以保证代理地址可以从容器内访问 RUN apk -U --no-cache add \\ bash git gcc musl-dev docker vim less file curl wget ca-certificates jq linux-headers \\ zlib-dev tar zip squashfs-tools npm coreutils python3 py3-pip openssl-dev libffi-dev libseccomp \\ libseccomp-dev libseccomp-static make libuv-static sqlite-dev sqlite-static libselinux \\ libselinux-dev zlib-dev zlib-static zstd pigz alpine-sdk binutils-gold btrfs-progs-dev \\ btrfs-progs-static gawk yq pipx \\ \u0026amp;\u0026amp; [ \u0026#34;$(go env GOARCH)\u0026#34; = \u0026#34;amd64\u0026#34; ] \u0026amp;\u0026amp; apk -U --no-cache add mingw-w64-gcc || true 安装编译和工具链所需的依赖 ENV TRIVY_VERSION=\u0026#34;0.56.2\u0026#34; RUN case \u0026#34;$(go env GOARCH)\u0026#34; in \\ arm64) TRIVY_ARCH=\u0026#34;ARM64\u0026#34; ;; \\ amd64) TRIVY_ARCH=\u0026#34;64bit\u0026#34; ;; \\ s390x) TRIVY_ARCH=\u0026#34;s390x\u0026#34; ;; \\ *) TRIVY_ARCH=\u0026#34;\u0026#34; ;; \\ esac RUN if [ -n \u0026#34;${TRIVY_ARCH}\u0026#34; ]; then \\ wget --no-verbose \u0026#34;https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-${TRIVY_ARCH}.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; tar -zxvf \u0026#34;trivy_${TRIVY_VERSION}_Linux-${TRIVY_ARCH}.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mv trivy /usr/local/bin; \\ fi 安装 Trivy 安全扫描工具 RUN GOPROXY=https://goproxy.cn,direct go install golang.org/x/tools/cmd/goimports@latest RUN rm -rf /go/src /go/pkg RUN if [ \u0026#34;$(go env GOARCH)\u0026#34; = \u0026#34;amd64\u0026#34; ]; then \\ curl -sL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s v1.55.2; \\ fi ARG SELINUX=true ENV SELINUX=${SELINUX} 安装 goimports 工具 清理缓存 仅对于 amd64 进行静态分析 控制是否启用 SELinux ENV DAPPER_RUN_ARGS=\u0026#34;--privileged -v k3s-cache:/go/src/github.com/k3s-io/k3s/.cache -v trivy-cache:/root/.cache/trivy\u0026#34; \\ DAPPER_ENV=\u0026#34;REPO TAG DRONE_TAG IMAGE_NAME SKIP_VALIDATE SKIP_IMAGE SKIP_AIRGAP AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID GITHUB_TOKEN GOLANG GOCOVER DEBUG\u0026#34; \\ DAPPER_SOURCE=\u0026#34;/go/src/github.com/k3s-io/k3s/\u0026#34; \\ DAPPER_OUTPUT=\u0026#34;./bin ./dist ./build/out ./build/static ./pkg/static ./pkg/deploy\u0026#34; \\ DAPPER_DOCKER_SOCKET=true \\ CROSS=true \\ STATIC_BUILD=true 设置 Dapper 运行时配置 ENV HOME=${DAPPER_SOURCE} WORKDIR ${DAPPER_SOURCE} ENTRYPOINT [\u0026#34;./scripts/entry.sh\u0026#34;] CMD [\u0026#34;ci\u0026#34;] 设置工作目录为源码中的路径 执行 ./scripts/entry.sh ci,启动构建流程 entry.sh #!/bin/bash set -e mkdir -p bin dist if [ -e ./scripts/$1 ]; then ./scripts/\u0026#34;$@\u0026#34; else exec \u0026#34;$@\u0026#34; fi chown -R $DAPPER_UID:$DAPPER_GID . 如果 scripts/ci 存在，那就执行它(因为 $1 是传给 entry.sh 的第一个参数 ci) $@ 是所有参数 总结 所以当我们在终端按下make 的时候,发生的是:\nmake ci ./.dapper ci dapper 启动一个 Docker 容器，挂载当前代码目录，并在容器中执行构建命令（如 ci） 容器内执行 ./scripts/entry.sh ci 容器内执行 download,validate,build,package 在开发板上构建的时候,因为下载 dapper 这个过程可能会遇到网络问题(我没找到怎么解决)\n所以我们可以跳过 dapper 这个过程,直接在网络好的本机进行 download,然后将 download 得到的 build 目录传给开发板,在开发板上执行 build、package-cli 命令. 又回到了之前这篇文章中说过的过程了,只是这次我们修改了内部的镜像并使用了私有镜像仓库.\n在此之前我们 fork 出一个仓库上传到 gitee 然后在开发板上拉取源码进行构建.git clone https://gitee.com/ltxworld/k3s-riscv64.git\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/033k3sep08%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BA%E8%A7%A3%E6%9E%90/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e从 EP 07 中,我们可以看到已经有了成功的 riscv 移植工作,见此\u003ca href=\"https://github.com/CARV-ICS-FORTH/kubernetes-riscv64\"\u003e仓库\u003c/a\u003e.\n发现其构建 riscv 采用了以下命令\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Build\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003erm -rf bin dist build\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eARCH\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003eriscv64 \u003cspan class=\"nv\"\u003eSKIP_IMAGE\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003etrue\u003c/span\u003e \u003cspan class=\"nv\"\u003eSKIP_VALIDATE\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003etrue\u003c/span\u003e \u003cspan class=\"nv\"\u003eSKIP_AIRGAP\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003etrue\u003c/span\u003e make\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Split\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e dist/artifacts\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egzip \u0026lt; k3s-riscv64 \u003cspan class=\"p\"\u003e|\u003c/span\u003e split -b 20M - k3s-riscv64.gz.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在此 \u003ca href=\"https://github.com/k3s-io/k3s/pull/7778\"\u003epull request\u003c/a\u003e 中他也提到了这一路上的适配工作过程:开始他是使用qemu模拟,之后在硬件上测试并适配我们之前提到过的各种镜像.\u003c/p\u003e","title":"K3sEP08——make后会发生什么?从源码解析构建过程"},{"content":"引子 根据前面几篇 k3s 相关的文章,我们得到了一个结论——k3s 在上的适配首先要适配几个系统镜像.\n之前在 issues 中看到一位大佬为 k3s-root 提供了 riscv 的支持. 跟随他的足迹,我找到了他已经做出了的适配工作.\n适配工作中可以发现,早在2023年他就对这些系统镜像的源码进行了 commit,提供了 riscv 的支持.\n在翻阅其 fork 的有关 riscv 分支上的提交时. 可以发现他的修改主要是将系统镜像的 yaml 文件中的镜像名称修改为了其私有镜像仓库,所以我们本节先来构建一个私有镜像仓库. 目的是可以实现开发板拉取镜像从私有仓库拉取适合的跨架构镜像,而不是默认的 rancher 仓库.\n大佬用的是 dockerhub 中自己的仓库,由于某些原因我们的网络无法访问 dockerhub, 并且为了方便后续我们要自定义某些镜像,例如之前的 redis , nginx 等镜像.\n最终暂时要达到的效果是从我的 MacBook 上构建跨架构镜像,推送至私有仓库中,开发板可以从仓库中拉取对应镜像.\n搭建私有镜像仓库 在之前的这篇文章中我们曾经尝试过私有镜像仓库. 本文中采用的是云服务器+域名+SSL 的方式,比之前本地的仓库更加灵活.(不用写死IP,不再用http)\n云服务器+域名 云服务器我们采用雨云提供的美国服务器,目的是后续与域名结合时无需备案. 毕竟备案比较复杂,我们只是学习实验用途,少一些麻烦.(如果大家想要租云服务器可以点击上面的链接) 服务器配置随意,我这里选择的是 debian 的宝塔配置.购买后会得到一个公网 IP,后面要用.\n关于域名,我之前在阿里云购买了一年的域名,所以继续使用这个域名. 我们需要对域名进行 dns 解析,即将域名与上面得到的 IP 地址结合起来. 这样在别人访问此域名的时候就会解析出我们服务器的 IP 地址,具体配置如下:\n主机记录处填写你想起的域名名称,记录值填写服务器的 IP 地址.\n至此我们完成了域名的绑定,接下来我们配置 SSL .\nSSL 关于 SSL的概念,在这篇文章中有讨论,感兴趣的朋友可以去看看.这里我们采用 acme+Let\u0026rsquo;s Encrypt 的组合. 根据 acme 官方的教程我们执行以下步骤:\n登陆服务器.首先,安装 acme 脚本\ncurl https://get.acme.sh | sh -s email=my@example.com source ~/.bashrc 由于我直接在 root 目录下执行的,所以他会将脚本安装在其下.\n接着,创建必要目录,由于 acme 后面的命令需要某些特定的目录,所以执行:\nsudo mkdir -p /home/wwwroot/jimlt.bfsmlt.top sudo chown -R www-data:www-data /home/wwwroot/jimlt.bfsmlt.top # 确保Nginx有读写权限 查看 nginx 的配置情况,我的 nginx 的 server 部分原始监听的是 phpmyadmin,所以我们需要添加一个 server 用来使 acme 后续可以访问80端口.\nnginx -t # 检查并获取nginx配置,可以得到配置文件路径 sudo vi /path/to/nginx.conf # 添加如下server块 server { listen 80; server_name jimlt.bfsmlt.top ; root /home/wwwroot/jimlt.bfsmlt.top; location /.well-known/acme-challenge/ { allow all; } # 可选静态资源优化等配置... location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$ { expires 30d; } location ~ .*\\.(js|css)?$ { expires 12h; } location ~ /\\. { deny all; } access_log /www/wwwlogs/access.log; } 重载 nginx 并检查配置\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx nginx -t 看到最后的 success 即代表配置无误.\n接下来我们生成证书,并使用 Let\u0026rsquo;s Encrypt 作为我们的 CA 服务器.\nacme.sh --set-default-ca --server letsencrypt acme.sh --issue -d mydomain.com -d www.mydomain.com --webroot /home/wwwroot/mydomain.com/ 最终会得到证书,结果类似下方:\nYour cert is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/jimlt.bfsmlt.top.cer [Fri May 9 09:53:29 AM CST 2025] Your cert key is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/jimlt.bfsmlt.top.key [Fri May 9 09:53:29 AM CST 2025] The intermediate CA cert is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/ca.cer [Fri May 9 09:53:29 AM CST 2025] And the full-chain cert is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/fullchain.cer 接下来我们复制证书给 registry 容器使用.\nsudo mkdir -p /etc/docker/registry/certs acme.sh --install-cert -d jimlt.bfsmlt.top \\ --key-file /etc/docker/registry/certs/jimlt.bfsmlt.top.key \\ --fullchain-file /etc/docker/registry/certs/jimlt.bfsmlt.top.crt \\ --reloadcmd \u0026#34;docker restart registry\u0026#34; sudo chmod 600 /etc/docker/registry/certs/* # 验证证书的安装结果 ls -l /etc/docker/registry/certs/ # 应看到： # - jimlt.bfsmlt.top.key # - jimlt.bfsmlt.top.crt 部署 registry 容器 使用 htpasswd 模式,这里我们直接让 registry 容器自己监听了443,是一种简单直接的方式. 但是并不推荐,后续考虑改为 nginx 统一处理443端口,让不同的服务监听其他端口.\ndocker run -d \\ --name registry \\ -p 443:443 \\ -v /etc/docker/registry/certs:/certs \\ -v /etc/docker/registry/auth:/auth \\ -v /var/lib/registry:/var/lib/registry \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/jimlt.bfsmlt.top.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/jimlt.bfsmlt.top.key \\ -e REGISTRY_AUTH=htpasswd \\ -e REGISTRY_AUTH_HTPASSWD_REALM=\u0026#34;Registry Realm\u0026#34; \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ --restart=always \\ registry:2 验证容器状态docker ps -f name=registry\n接着我们需要知道私有仓库的账号和密码,通过 htpasswd 进行设置.\nsudo mkdir -p /etc/docker/registry/auth docker run --rm \\ --entrypoint htpasswd \\ httpd:2 -Bbn \u0026lt;username\u0026gt; \u0026lt;password\u0026gt; | sudo tee /etc/docker/registry/auth/htpasswd docker restart registry 至此我们就完成了带有 SSL 的私有镜像仓库搭建,接着我们可以在其他机器上进行登陆验证 docker login https://jimlt.bfsmlt.top 输入设置的账号与密码即可.\n如果遇到认证问题,在需要登陆的机器上从服务器上复制证书 但是 Docker 默认信任 Let’s Encrypt 的有效证书,所以一般无需复制.\nsudo mkdir -p /etc/docker/certs.d/jimlt.bfsmlt.top sudo scp root@你的服务器IP:/etc/docker/registry/certs/jimlt.bfsmlt.top.crt \\ /etc/docker/certs.d/jimlt.bfsmlt.top/ca.crt 总结流程 安装 acme.sh 脚本 修改 nginx 关于80端口配置,使 acme 可以访问 使用 acme 的 webroot 模式生成证书 复制证书给 registry 容器使用 测试搭建效果 还记得我们最开始的目标吗? Macos-服务器-开发板 这样一个工作流. 所以我们在 macos 上构建一个之前的 pause 镜像并推送和拉取.\n这里我们仿照大佬的构建思路, Dockerfile.pause 如下:\nFROM ubuntu:latest AS builder RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y wget gcc \u0026amp;\u0026amp; \\ wget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.31.1/build/pause/linux/pause.c \u0026amp;\u0026amp; \\ gcc -Os -Wall -Werror -static -DVERSION=v3.10-v1.31.1 -o pause pause.c FROM scratch COPY --from=builder /pause /pause USER 65535:65535 ENTRYPOINT [\u0026#34;/pause\u0026#34;] 在 mac 上进行构建并推送到私有仓库中(记得先登陆)\ndocker build --network host --platform linux/riscv64 \\ -f Dockerfile.pause \\ -t jimlt.bfsmlt.top/pause:v3.10-v1.31.1 \\ --push . 这里标签必须写作私有仓库的域名,否则会推送到 dockerhub 中.\n在开发板上拉取该镜像 docker pull jimlt.bfsmlt.top/pause:v3.10-v1.31.1 检查是否存在\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE jimlt.bfsmlt.top/pause v3.10-v1.31.1 d98ad82e7eed 14 minutes ago 593kB 至此,整条流程打通,暂时告一段落.\n接下来的想法就是仿照大佬的 commit 中的修改,对 k3s 的源码进行修改进行镜像的自定义. 我觉得这个过程一定会遇到很多问题,例如我们不仅要修改镜像名称还要修改 docker.io 这部分;例如可能遇到的网络等问题.\n更新:添加 WebUI 查看仓库内的镜像 推荐使用 WebUI 来查看仓库内的镜像,并作为一个简要的可视化界面;同时采用 nginx 的反向代理功能来管理不同的 https 请求. 最终达到的目的是客户端发来的 https 请求由服务器上的 nginx 处理,处理后转发到 registry 和 WebUI 容器内. 容器与 nginx 的交互通过 http.\nWebUI 需要注意的是,由于 Let\u0026rsquo;s Encrypt 给出的证书是一个非泛域名的证书,所以我们需要为每个子域名都申请一个证书.\n这里我们同样申请了一个新的子域名来作为可视化界面的地址:registryui.bfsmlt.top\n与上面步骤相同,我们修改 nginx 关于80端口配置\nserver { listen 80; server_name registryui.bfsmlt.top; root /home/wwwroot/registryui.bfsmlt.top; location /.well-known/acme-challenge/ { allow all; } } # 记得检查并重载 nginx 配置 新建对应目录 sudo mkdir -p /home/wwwroot/registryui.bfsmlt.top 生成证书acme.sh --issue -d registryui.bfsmlt.top --webroot /home/wwwroot/registryui.bfsmlt.top/\n因为要让 nginx 反向代理,所以将证书复制给 nginx 使用 sudo mkdir -p /etc/nginx/certs\nacme.sh --install-cert -d registryui.bfsmlt.top \\ --key-file /etc/nginx/certs/registryui.bfsmlt.top.key \\ --fullchain-file /etc/nginx/certs/registryui.bfsmlt.top.crt \\ --reloadcmd \u0026#34;systemctl reload nginx\u0026#34; 再次修改 nginx 配置,使 nginx 为我们代理传来的 https 请求\nserver { listen 443 ssl; server_name registryui.bfsmlt.top; ssl_certificate /etc/nginx/certs/registryui.bfsmlt.top.crt; ssl_certificate_key /etc/nginx/certs/registryui.bfsmlt.top.key; location / { proxy_pass http://localhost:8080; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } 部署 WebUI 容器,指明要访问的私有仓库地址\ndocker run -d \\ --name registry-ui \\ -p 8080:80 \\ -e REGISTRY_TITLE=\u0026#34;My Private Registry\u0026#34; \\ -e REGISTRY_URL=https://jimlt.bfsmlt.top \\ -e DELETE_IMAGES=true \\ --restart=always \\ joxit/docker-registry-ui:static registry 同样的,我们也要修改之前的 registry 容器;删除并以下面的方式启动(不再显示带有 https,交给 nginx)\ndocker run -d \\ --name registry \\ -p 5000:5000 \\ -v /etc/docker/registry/auth:/auth \\ -v /var/lib/registry:/var/lib/registry \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \\ -e REGISTRY_AUTH=htpasswd \\ -e REGISTRY_AUTH_HTPASSWD_REALM=\u0026#34;Registry Realm\u0026#34; \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin=\u0026#39;[\u0026#34;https://registryui.bfsmlt.top\u0026#34;]\u0026#39; \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods=\u0026#39;[\u0026#34;GET\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;OPTIONS\u0026#34;]\u0026#39; \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers=\u0026#39;[\u0026#34;Authorization\u0026#34;, \u0026#34;Accept\u0026#34;, \u0026#34;Cache-Control\u0026#34;, \u0026#34;Content-Type\u0026#34;]\u0026#39; \\ -e REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials=\u0026#39;[\u0026#34;true\u0026#34;]\u0026#39; \\ --restart=always \\ registry:2 这是加了允许 CORS 跨域请求的,有时间我们出一期跨域请求的文章.\n(如果你没有配置证书,请仿照之前的步骤来)然后同理修改 nginx 配置文件,以达到我们让 nginx 来管理 https 命令的目的.\nserver { listen 443 ssl; server_name jimlt.bfsmlt.top; ssl_certificate /etc/docker/registry/certs/jimlt.bfsmlt.top.crt; ssl_certificate_key /etc/docker/registry/certs/jimlt.bfsmlt.top.key; location / { proxy_pass http://127.0.0.1:5000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Authorization $http_authorization; proxy_pass_request_headers on; } } 最后重启 nginx,再次进行测试\n进行 docker pull/push 检查私有镜像仓库是否可以访问 在浏览器中访问 WebUI 的地址看是否可以访问 最终效果如下,我们可以直观地看到自己的私有镜像仓库有哪些镜像:\n总结 本片文章中已经顺利完成了带有SSL的私有镜像仓库搭建,为我接下来的实验提供了有力的镜像保证.\n在这个过程中我们遇到了很多的概念,例如 SSL/TLS, CA, Nginx配置等,我们已经在其他文章中进行了总结.\n引用 https://github.com/CARV-ICS-FORTH/kubernetes-riscv64 http://github.com/acmesh-official/acme.sh/wiki/%E8%AF%B4%E6%98%8E https://hackmd.io/@neverleave0916/S1KhWswhv ","permalink":"http://localhost:1313/posts/032k3sep07%E9%85%8D%E7%BD%AE%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E5%87%86%E5%A4%87%E6%9E%84%E5%BB%BA%E8%B7%A8%E6%9E%B6%E6%9E%84%E9%95%9C%E5%83%8F/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e根据前面几篇 k3s 相关的文章,我们得到了一个结论——k3s 在上的适配首先要适配几个系统镜像.\u003c/p\u003e\n\u003cp\u003e之前在 issues 中看到一位大佬为 k3s-root 提供了 riscv 的支持.\n跟随他的足迹,我找到了他已经做出了的\u003ca href=\"https://github.com/CARV-ICS-FORTH/kubernetes-riscv64\"\u003e适配工作\u003c/a\u003e.\u003c/p\u003e","title":"K3sEP07——配置私有镜像仓库准备构建跨架构镜像"},{"content":"引子 之前的两篇文章关于context和channel，我们都从源码的角度去讨论了其各自的底层原理以及使用方法。\n但是我并没有从宏观的角度上来阐述为什么我们需要并发？为什么要设计这两种结构来处理并发，本着做到\u0026quot;what-how-why\u0026quot;的态度，我们今天来深入讨论一下 Go 中的并发。\n区分并行和并发 这个已经被说烂的话题为什么我还要拿出来说呢？（因为书里写了，bushi）因为自己对其的理解还不够深刻。英文中并行通常是 Parallelism,并发是 Concurrency.\nConcurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.\n从这句名言可以看出，并发是一种组织结构上的概念，而并行是一种执行上的概念。（这个思想很重要）\n同时我们抛出一个问题：同样的程序，并发就一定并行要快吗？\n在回答这个问题之前，我们应该先了解 Go 中关于并发和并行的常见概念，也就是 goroutine 与 GMP 调度模型。\ngoroutine 在我们学的操作系统课程中，通常用线程thread 描述并发，进程描述并行；但是作为 Go 程序员，我们通常用 goroutine 描述并发，thread描述操作系统线程。\ngoroutine 可以被视作应用级别的 threads，所以这是一种来自于用户态的调度 相当于又把 thread 往里“剥”了一层 那为什么要搞一个 goroutine 出来呢？\nthread 直接由 OS 管理，占用空间大，系统调用和线程调度过程中上下文切换资源消耗大，并且受到 CPU 资源限制，同步问题处理复杂 goroutine 由 Go runtime 管理，占用空间极小（2KB）；运行时的调度器可以将N个 goroutine 映射到 M 个 thread 上，消耗小；Go 提供 context,channel机制处理同步问题容易 GMP模型 上面提到了调度器的作用，其实就是 Go 的调度模型 GMP（这里简单提及，后续会出文章深究）\nG-Goroutine M-OS thread P-CPU core 形象的理解就是 M 作为服务员，通过 P 来服务许多的顾客 G，而管理员就是 Goruntime.\n每个 goroutine 都有三个状态（类似于 thread 的几个状态,但比其简单）\nExecuting Runnable Waiting 还有一个重要概念，这里简单提一嘴：GOMAXPROCS——代表着默认M的数量，同时也是最多有多少 goroutine 同时处于 Executing 状态。\nGOMAX-PROCS is by default equal to the number of available CPU cores；GOMAX-PROCS can be changed and can be less than the number of CPU cores\n最后再简单说说 GoRuntime 是怎么调度以保证并发效率的。\n采用队列保存 goroutine,有本地队列也有全局队列。 遇到自身没有 goroutine 的时候会进行 work stealing,从其他的队列偷一半过来保证并发效率。\n并发一定快吗？ 好了，补充了这么多了，希望大家没有忘记上面提出的问题，并发的写法一定比顺序的快吗？\n以书中的例子——归并排序举例。 因为归并排序每次将元素分为长度相同的两半，直到无法分，然后又对分出的这些内容进行排序，最后组合在一起（形成了一个树形结构）\n如果我们用 goroutine，大概是这样的。\nfunc MergesortV1(s []int) { if len(s) \u0026lt;= 1 { return } middle := len(s) / 2 var wg sync.WaitGroup wg.Add(2) go func() { defer wg.Done() MergesortV1(s[:middle]) }() go func() { defer wg.Done() MergesortV1(s[middle:]) }() wg.Wait() merge(s, middle) } 结果发现，并发反而要比顺序要慢，这是为什么呢？\n还记得我们一开始就说过，并发是一种组织结构，是需要调度的，调度是需要开销的。 尽管我们已经用了 goroutine 这样一个开销较小的结构，但是对于归并排序，每次把元素划分地太小了，导致每一个任务的开销甚至不如调度的开销，所以并发更加耗时。 如何解决这个问题？很明显是 workload 的缘故，所以我们可以设置一个 workload 的 threshhold,不够这个门槛的顺序执行，超过的并发执行。\n并发引起的数据竞争 在上面我们引入了 goroutine 并讨论了 golang 中的并发与并行。接下来讨论常见的数据竞争问题。\n首先要明确一点：goroutine 虽然已经创建，但不一定立刻调度运行（Go 的调度器不保证立即运行）也就是我们写的代码的顺序并不是真正的执行顺序。\n众所周知，数据竞争一直是并发问题下绕不开的话题。Go 中使用 goroutine 这种更小的结构更应该讨论如何面对数据竞争。 同时 Go 也为我们提供了一个运行时参数-race来检查我们的代码中是否有数据竞争问题。\n那我们就来看看 Go 是如何避免数据竞争的。 Go的标准库 sync 下就有一些准备好的方法。\natomic 从单词意思就可以看出来，原子化——不能被打断的操作。atomic.AddInt64(\u0026amp;i, 1) 需要注意的是，其只能作用于具体类型，不能用于像 slices,maps,structs这样的结构。（并且还得指明bit位数）\nmutex 这就类似于操作系统课程中的锁，mutex可以主动给数据区上锁打造临界区，这样自然可以处理数据冲突问题。mutex.Lock()\n但是可能会发生死锁问题——死锁发生的根本原因是所有 goroutine 都被阻塞，没有任何一个能推进程序的执行。 特别是 main 中的被阻塞，而其他的 goroutine还没来得及启动时，那一定会都堵在那里。\nchannel 同时，我们也可以用 channel处理数据竞争，关于其底层原理，见我的文章.\n这里我们使用的是无缓冲的 channel，因为无缓冲型有一个特点是其必须存在一对接收方与发送方，如果少了其中之一，这个通道就会阻塞。 所以无缓冲的通道具有很强的同步一致性。\n有缓冲也可以，但是需要我们进行额外设计。\n区分mutex与channel 区分mutex与channel,二者最本质的区别是前者强调上锁形成一个临界区，而后者强调不同 goroutine 之间的通信协作。\nmu.Lock(),mu.Unlock()保护共享的内存 ch \u0026lt;-1, \u0026lt;-ch发送给通道，通道接收 在 Golang 中有一个原则是：不要通过共享内存来通信，而应该通过通信来共享内存\n区分data race和race condition 解决了数据竞争程序就可以一定按照我们想要的顺序执行吗？不一定。例如\ni := 0 mutex := sync.Mutex{} go func() { mutex.Lock() defer mutex.Unlock() i = 1 }() go func() { mutex.Lock() defer mutex.Unlock() i = 2 }() 很明显，没有数据竞争问题，但是最终结果会是1还是2？得看两个协程的运行先后顺序了。\nA data race occurs when multiple goroutines simultaneously access the same memory location and at least one of them is writing. An application can be free of data races but still have behavior that depends on uncontrolled events (such as goroutine execution, how fast a message is published to a channel, or how long a call to a data-base lasts); this is a race condition.\n可见 mutex 无法解决执行顺序问题，但是 channel 可以，它的核心思想一直是通过通信来共享数据，而不是通过共享数据来通信。 所以我们可以通过channel解决，第二个操作需要等待第一个操作完成（对其阻塞）\ni := 0 done1 := make(chan struct{}) // 用于通知第一个 goroutine 已完成 done2 := make(chan struct{}) // 用于通知第二个 goroutine 已完成 go func() { i = 1 close(done1) // 通知第一个操作已完成 }() go func() { \u0026lt;-done1 // 等待第一个操作完成 i = 2 close(done2) // 通知第二个操作已完成 }() \u0026lt;-done2 // 等待所有操作完成 fmt.Println(i) // 保证输出 2 具体原理简单解释一下：如果 done1 通道阻塞了，第二个 goroutine会被挂起并加入到接收等待队列，调度器会将其移出可运行队列，并切换其他协程，直到被唤醒。\nGo内存模型 上面的执行顺序问题其实在 Go 的内存模型中就已经定义过了。称为多 goroutine 并发操作时的执行顺序规则。 关键在于 happens-before 关系。规定哪些操作必须发生在另一些操作之前。\n1.channel 的发送操作在接收完成之前 1.底层用锁和等待队列实现。 2.close 先于所有“因关闭而返回”的接收操作 1.这样就不会出现关闭后仍能接收的情况 3.无缓冲的 channel 接收操作先于发送完成（这一点有些难理解） 1.就是我们说的接收和发送是一对操作，如果没有准备好的发送，接收就会阻塞\ngoroutine泄漏问题 Context在并发中也是十分重要的一环，用于控制生命周期，具体见这篇文章.最后我们来说一下一个常见问题: goroutine的泄漏——某些 goroutine 在预期之外长期运行或阻塞，无法被回收，导致内存和 CPU 资源浪费，甚至程序崩溃（其实阻塞有时就是死锁）。\n无缓冲 channel 阻塞：我们之前一直强调 channel 要是成对出现的，如果永久阻塞就会泄漏 未关闭 channel 导致接收者阻塞 无限循环的：增加退出条件，例如 ctx.Done() 未处理的 select 分支:最终原因也是堵塞 为了解决堵塞，我们推荐使用有缓冲的 channel，确保关闭通道，使用 context 来控制生命周期，使用 select（case）+default分支结构。\n区分 I/O密集型和CPU密集型任务 I/O 密集型任务：网络请求，文件操作，数据库访问。 CPU 密集型任务：数学计算，图像处理等。\n由于 Go 的调度依赖运行时，之前我们提到他是在用户态的调度，所以上下文的切换成本比 thread 要低得多。于是可以创建数十万级别的 goroutine. 如果有10000个HTTP请求，他就可以开10000个 goroutine，而由于调度器的存在（GMP模型）实际上只需要少量的 OS thread。 其通信模型 channel 也十分适合这类型的任务。\n但是到了 CPU 密集型任务，就得靠 GOMAXPROCS 这个参数了，它是实打实的表征着正在运行的 thread数量，Gol 对于这类任务并没有体现出更优的性能，C/C++ 也许更为合适。\n总结 Go 依靠其独特的 goroutine、channel、context结构，展现出优异的并发能力（特别是在I/O密集型任务上） Go 的核心理念是：不要通过共享内存来通信，而是通过通信来共享内存\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/031golangep05_%E5%B9%B6%E5%8F%9103%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%A8%E8%AE%BA%E5%B9%B6%E5%8F%91/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e之前的两篇文章关于\u003ca href=\"https://www.bfsmlt.top/posts/005golangep01_%E5%B9%B6%E5%8F%9101_context/\"\u003econtext\u003c/a\u003e和\u003ca href=\"https://www.bfsmlt.top/posts/007golangep02_%E5%B9%B6%E5%8F%9102chan/\"\u003echannel\u003c/a\u003e，我们都从源码的角度去讨论了其各自的底层原理以及使用方法。\u003c/p\u003e\n\u003cp\u003e但是我并没有从宏观的角度上来阐述为什么我们需要并发？为什么要设计这两种结构来处理并发，本着做到\u0026quot;what-how-why\u0026quot;的态度，我们今天来深入讨论一下 Go 中的并发。\u003c/p\u003e","title":"GolangEP05_Go中的并发"},{"content":"引子 心血来潮。\n技术栈概览 Golang Postgresql Gin,Gorm 数据库 homebrew install postgresql@17 echo \u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/postgresql@17/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc vi /opt/homebrew/var/postgresql@17/postgresql.conf # 修改允许连接地址从localhost到* # 由于我们用homebrew安装的，所以使用brew控制 brew services start postgresql@17 # 默认安装后的 PostgreSQL 没有设置密码，也不允许远程连接。**你需要创建一个可以远程连接的用户** psql postgres CREATE USER myuser WITH PASSWORD \u0026#39;mypassword\u0026#39;; ALTER USER myuser WITH SUPERUSER; -- 可选，根据权限需求 \\q # 退出 # 后续连接数据库可以采用 psql -U user -d DatabaseName # 后续检测其运行连接状态可用 /opt/homebrew/opt/postgresql@17/bin/pg_isready 在Navicat上进行本地连接,然后自建数据库PayRecord,建表transactions。\nCREATE TABLE transactions ( id SERIAL PRIMARY KEY, date DATE NOT NULL, // 日期 amount DECIMAL(10, 2) NOT NULL, // 金额 category VARCHAR(100), // 类型 description TEXT, // 描述 type VARCHAR(10) CHECK (type IN (\u0026#39;income\u0026#39;, \u0026#39;expense\u0026#39;)), // 支出还是收入 mood TEXT, // 当时的心情 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP // 时间 ); ","permalink":"http://localhost:1313/posts/030%E8%AE%B0%E5%BD%95%E5%BC%80%E5%8F%91%E6%94%B6%E6%94%AF%E7%A8%8B%E5%BA%8F%E8%BF%87%E7%A8%8B/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e心血来潮。\u003c/p\u003e\n\u003ch2 id=\"技术栈概览\"\u003e技术栈概览\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGolang\u003c/li\u003e\n\u003cli\u003ePostgresql\u003c/li\u003e\n\u003cli\u003eGin,Gorm\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"数据库\"\u003e数据库\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehomebrew install postgresql@17\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/postgresql@17/bin:$PATH\u0026#34;\u0026#39;\u003c/span\u003e \u0026gt;\u0026gt; ~/.zshrc\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003evi  /opt/homebrew/var/postgresql@17/postgresql.conf\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 修改允许连接地址从localhost到*\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 由于我们用homebrew安装的，所以使用brew控制\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebrew services start postgresql@17\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 默认安装后的 PostgreSQL 没有设置密码，也不允许远程连接。**你需要创建一个可以远程连接的用户**\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epsql postgres\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eCREATE USER myuser WITH PASSWORD \u003cspan class=\"s1\"\u003e\u0026#39;mypassword\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eALTER USER myuser WITH SUPERUSER\u003cspan class=\"p\"\u003e;\u003c/span\u003e  -- 可选，根据权限需求\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\q\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 退出\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 后续连接数据库可以采用\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epsql -U user -d DatabaseName\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 后续检测其运行连接状态可用\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e/opt/homebrew/opt/postgresql@17/bin/pg_isready\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在Navicat上进行本地连接,然后自建数据库PayRecord,建表transactions。\u003c/p\u003e","title":"030记录开发收支程序过程"},{"content":"引子 做这个系列的目的也是心血来潮吧，同时也是适配我的奇思乱想系列的第二集消费观那里，每月以此作为警醒和反思，不断塑造自己的消费观。\nApril 支出 我的支出从三个渠道查询，微信支付，支付宝以及银行卡。\n微信 先从微信来看，直接点开账单，梳理一下自己的支出情况。\n餐饮（包括任何吃的东西）:12+33.61+15+23.5+3+26.3+16+100+16+12.5+61.75+200+6+11+19.6+16+12+16+200+4.9+12+5+16+8+15+6+12+12+100+12+14+10+19+25+5+31.5+12+12 = 1172.66 租赁支出（话费,applemusic等）:话费19.96+AM6+网费60+icloud21 = 106.96 演出:CBA280 = 280 电子产品:loffee键盘331+随身wifi49+音乐下载10 = 390 日用品:耳塞26+洗衣4+刷子5.8 = 35.8 未知15红包,6转账 = -21 衣物:两个短裤一个短袖300 = 300 总共从微信上的支出为 $1172.66+106.96+280+390+35.8+300-21 = 2264.42$\nps: 其中餐饮的支出比较难以具体划分，大概来源于校园卡充值以及麦叔的水果和宿舍聚餐。 希望下个月的时候可以每天进行细分统计，这样到月底可以直接查看并总结。\n支付宝与银行卡 由于平时支付宝使用较少，只有两笔订单：打车12+饮料4.9 = 16.9 银行卡也没有对应支出。\n支出总结 所以我在4月份的支出一共是2281.32元，宏观上看属于还能接受的范围；细看的话也还好，没有什么超出范围的消费。 如果要硬扣的话，衣物的消费在下个月应该是最多找两件跑步时穿的运动服，电子产品的支出也大概率没有；网费由于随身wifi的缘故，下个月会减少。\n总的来看中规中矩，继续保持。\n收入 由于还是在校学生脱产，收入来自于生活费2k+学校补助600+老师的补助2k，一共4600元。\n总结 能不能写个程序出来？来记录自己每日的支出情况？也许你会说为什么不用现成的程序，我觉得如果用上自己的程序那么记录的动力会大很多。Let\u0026rsquo;s try it.\nMay 于6.7日周六，对五月的个人消费做以总结。可喜的一点是，目前我做出了一个很小的玩具项目，React + Golang 的组合用于记录我的消费情况。\n不过这个程序还是太过简单，每天都得向其中自己填入数据，前端的页面也过于简陋（现在甚至没有删除记录的按钮），所以在六七月份我学完 js，ts 和 React 后，看看自己能不能改出一个有点模样的程序。\n当然后期学完这些前端的组件后，可能也会考虑做一个微信小程序或者安卓程序来记录，这样会更加方便也更符合一个真正的软件该干的事情。\n支出 根据数据库中的 category 字段来统计不同类别的支出。\n吃喝 food 在食物层面，本月一共花费了 809.81元，可以说整个月几乎都在吃食堂，少有两三次点了外卖。不过最近的外卖价格战打的激烈，导致外卖也没有贵多少。\nfruit 水果方面，本月支出 106.50元。由于食堂中有3~3.5元的小水果，所以我不再像之前那样去麦叔超市那里买更贵的果拼，所以本月的水果支出并不多。\nsnack 这个类别的意思是平时买的一些零食饮料之类的小玩意，本月支出 130.4元。全部来自于跑完步后的脉动和有时候早上的9.9瑞幸。\n将以上三者加起来可以发现，在日常的吃喝方面我一共消费了 1046.71元。平均到每天，也就35元。可以发现我对吃喝这方面并无太多的追求。\n吃的最多的就是食堂的自选餐和三楼的面了。\n起居 living:\n在生活起居方面花费了 116.50元,花在了理发，洗衣机，纸巾等用品上，没有特别高的消费，都是必需品。 购物 clothes:\n在衣物方面花了 233.23元，消费了三件迪卡侬短袖用来跑步，没错又把跑步捡起来了（可惜没有注重热身和乱热身给自己跑伤了，希望下周能好啊） 买了三双袜子以及续费了京东 Plus 会员。总的来说正常消费。 聚会 聚会本月消费 1051元。\n由于五月七号是自己的生日，我们宿舍向来有生日请客的传统，所以在很久以前烧烤店搓了一顿，虽然很贵，但是这家烧烤店的烧烤真的好吃。这笔消费也是没法逃掉的。\n题外话一句，在北京出去想吃点好的真贵啊，而且也确实有点美食荒漠，不过想吃饱倒是还好。\n影音 这是新加的内容，用来统计音乐和视频的支出，比如音乐会员，演唱会，某些付费视频。\n音乐方面本月共消费 1386元，抢到了王力宏北京演唱会1380的票，自己抢到的，感觉自己太幸运了！这下周王陶林新“四大天王”的演唱会我全集齐了哈哈，也算是了却心愿了。 虽然很贵，但是我觉得很值，这种机会今生难得吧。另外是音乐会员的 6元 视频方面花费了 10 元，看了 B 站食贫道的充电视频，一共看了4~5个纪录片吧，讲美国，日本等国家的。拍的是蛮不错的，可以了解其他国家的部分情况，这十块钱花的很值。 看完只能感慨，世界上的二八原则永远适用，底层的我们只是惨和更惨的区别罢了。 科技 关于科技方面我的定义是电子产品，包括现实中的例如手机手表，也包括虚拟中的比如关于 ai 的一些东西。\n本月科技方面消费 **1321.21元，**主要是佳明255手表为了督促自己重拾跑步（可惜现在小伤唉）\n佳明255在国补下只需要 1173元，我觉得这是非常香的一个价格了，再加上我不再一直使用 iphone，以及 applewatch 也用了两年了，电池扛不住了（我也扛不住每天充电了），遂作出决定入手佳明255. 戴着它目前已经跑了40公里了，体感良好，我觉得这笔消费也是值得的。 其次购买了小米路由器 AX3000T,之前一直用随身 wifi 作为集群的网络，后来才发觉可以连工位的网线然后自己用路由器分配局域网就行，网速比随身 wifi 不知道快到哪里去了。 这种问题早该问我师兄了，毕竟他也是搞集群，唉，怎么就没有想到呢。白白研究了好久随身 wifi 的问题。 剩下的就是租云服务器了，20多一个月。做实验不得不支出的。 支出总结 最后，本月一共消费 5187.22元！花超了很多，不过每笔消费我觉得都没有冲动消费，都是我认为我该支出的，特别是三笔大消费\n演唱会 手表 生日宴会 如果去掉这些消费，本月消费也就1500元左右，属于正常消费。所以这个月的消费我还是比较满意的，那我们就下个月再见，希望下个月的程序有新的面貌！\n收入 本月收入是 600 + 1200 ,老师终于把空天院的酬劳发了，听说要发三四个月，我估计也就发四个月吧。总归是发了，比什么都不发要强！\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/029%E6%AF%8F%E6%9C%88%E6%94%B6%E6%94%AF%E5%9B%9E%E9%A1%BEep01-25_04/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e做这个系列的目的也是心血来潮吧，同时也是适配我的奇思乱想系列的第二集消费观那里，每月以此作为警醒和反思，不断塑造自己的消费观。\u003c/p\u003e\n\u003ch2 id=\"april\"\u003eApril\u003c/h2\u003e\n\u003ch3 id=\"支出\"\u003e支出\u003c/h3\u003e\n\u003cp\u003e我的支出从三个渠道查询，微信支付，支付宝以及银行卡。\u003c/p\u003e","title":"每月收支回顾EP01_25年"},{"content":"引子 HTTPS在我们的日常生活中已经司空见惯,特别是有时我们会遇到这种页面:\n这是什么意思呢?这个来自浏览器的提示意味着我们访问的 HTTPS 页面并不安全. 那为什么会不安全呢?是因为其网站的证书出现问题了.\n那么什么是证书?为什么我们需要证书来保证安全?接下来我们将一一说明.\n与之前的 DNS 文章联系一下,二者都是在应用层工作的协议.DNS 的高级过程例如 DoH就采用了 HTTPS 来传输查询结果.\n通常访问某个域名的过程中,先用 DNS 解析 IP 地址, 再用 HTTP 协议向该地址发起请求,例如 GET,POST等,最后服务器响应网页内容. 这个过程我们先在脑子中有个印象,后面大多围绕这个过程展开.\n为什么需要HTTPS HTTPS 故名思义,HTTP over SSL(TSL),是在 HTTP 协议上加了一层 SSL(什么是SSL我们稍后解释)\n众所周知,HTTP 传输的报文是明文传输,在之前 DNS 那篇文章中我们就说 DNS 查询过程是明文传输, 为了保证安全后面采用了 DoT 与 DoH,其中 DoH 就是 DNS over https.\n所以为了保证安全,使我们传输的报文变得不那么透明,我们需要给他加上一层保护,这层保护就是 SSL. 可以阻止其他人嗅探我们的报文,获取我们的报文内容等等不当操作.\n如果没有 SSL,光用 HTTP 行不行呢?举一些例子来说明:\n中间人攻击:如果我们使用 HTTP 进行登陆某个网站,攻击者可以看到其中的内容,那么你的用户名和密码就泄漏了. 内容篡改:同样的,攻击者或者 ISP 可以偷偷修改网页内容例如插入广告 钓鱼网站:访问银行网站,但是攻击者将你的请求重定向(例如DNS劫持)到假的网站,内容是复制真的网站,从而误导你进行错误操作. 所以我们需要 SSL !\n什么是SSL(TLS) 我们上面提到单纯的 HTTP 会遇到嗅探等攻击,所以我们要为它创造一条隧道,一条受保护的隧道,这就是 SSL 的目的. Secure Socket Layer\n一个安全的,受到 SSL 保护的网站是这样的\nSSL 可以做到的是\n加密通信:防止中间人窃听 身份验证:防止钓鱼网站 完整性校验:数据不被篡改 在解释什么是 SSL 之前,我觉得我们需要先了解一些其他概念,辅助我们后续理解.\n证书与CA 密钥与加密 什么是证书与CA 正如上面钓鱼网站的例子,网站怎么能够证明他是真的网站,是我们要访问的网站呢?\n举例说明 仿照别人的例子来说:\n你是A公司的员工,带着一封信要访问B公司,到门口保安室的时候信上带有A公司的公章,由于B公司信任A公司的公章,所以接受这封信.(先不提伪造公章这种事) 这里的公章就是证书的感觉,钓鱼网站一般没有这种证书,所以我们可以知道这是假的网站.\n如果有D,E等等很多公司,B公司也都信任他们,可是这对保安来说是一个坏消息,这么多不同公司的公章,我该怎么认全呢? 所以这时有一个C公司出场了,他是一个中介公司,专门处理代理公章业务——B公司必须信任C公司\n之后A公司再访问B公司情况就变成了这样:A公司携带两封信访问B公司,第一封信上有C公司的公章和A公司的公章,保安看见后, 他只识别C公司的公章,正确,看第二封信.第二封信上有一个A公司的公章,和第一封信上的一对比,发现一致,保安接受第二封信.\n可以发现,对于其他公司也是一样,多带一封信来验证身份,B公司的保安只需要知道C公司的公章即可,然后对比一二封信的公司公章进行验证.\n这个例子最后的C公司就是 CA 证书颁发机构(Certificate Authority),正如例子中所言,他是一个双方都可以完全信任的机构. 常见的 CA 包括 DigiCert,Let\u0026rsquo;s Encrypt(免费),Secigo 等;常见的 CA 工具包括:OpenSSL,EasyRSA,CFSSL等.\n回到我们访问网站的例子,如果我们访问某个 https 网站,浏览器就会去验证这个网页的 CA证书, 如果这个证书没有问题,那么我们就知道这个网站就是我们要访问的网站,不是什么钓鱼网站.\n所以 CA 就作为了浏览器和网站之间的公证人,让双方知道他就是真实的他,不是伪装者.\n证书链 由于互联网上有那么多的网站,绝大多数网站的证书不是由根 CA 直接签发的,而是由中间 CA 签发的,但是浏览器和操作系统只默认信任根 CA 的证书.\n为什么要这么做呢?根 CA 证书不能频繁签发,通常离线保存,为了避免安全风险,所以通过中间 CA 作为代理向下签发.于是证书链应运而生.\n访问网站时浏览器会:\n拿到网站的证书链 根据证书链向上查询,如果可以查到根 CA ,就证明网站可信. 我们后面的过程直接简化掉了这个证书链的过程,大家注意.\n密钥与加密 可以发现,证书就是我们的身份凭证,那么实际工作中,证书的内容是什么呢? 有了身份凭证就可以做到安全通信了吗?\n很明显不够,在庞大的网络线路上,我们还需要对我们的请求进行加密,那么用什么加密呢? 自然是密钥.\n公钥与私钥 公钥:可公开,用于加密数据or验证签名,加密后的数据只能用对应的私钥解密 私钥:不可公开,用于解密数据or创建签名;经其签名后的数据,可用公钥验证真实性 对称交换与非对称交换 密钥的概念中有对称交换与非对称交换,这里我们略微介绍一下:\n对称交换:双方基于同一个密钥进行通信(加密解密数据) 非对称交换:双方各自持有一对公钥与私钥,发送方使用接收方的公钥加密数据，接收方用自己的私钥解密(正如上面所说) 可以发现,对称交换需要双方提前协商用哪个密钥(这称为密钥交换),而在不安全的通信中如何安全地将这个密钥给到双方,是最大的缺点;而非对称加密不需要事先共享密钥，可以在不安全网络中建立安全连接.\n但是非对称的速度太慢,所以我们要将二者结合使用,即混合加密:\n先用非对称把对称密钥交换给双方(密钥交换),然后双方基于这个对称密钥进行通信.做到了两全其美.\n但是为什么钥先用非对称加密来进行密钥交换呢?具体见下方案例.\n举例说明密钥交换过程 有了上面这些概念,我们来大致描述一下浏览器如何与网站进行通信,首先我们来说说上面刚提到的密钥交换.\n方案一:单纯非对称加密 Step1:网站随机生成密钥对 k1 \u0026amp; k2 (k1是公钥,k2是私钥) Step2:网站保留 k2,将 k1 通过明文方式发给浏览器;即使是明文,被偷窥者拿到也不会推算出k2(密码学原理) Step3:浏览器拿到 k1, 随机生成一个对称加密的密钥 k(视作私钥),用 k1 去加密 k,得到 k\u0026rsquo;;浏览器将 k\u0026rsquo;发送给网站 注意,此时只有 k2 才能解密经过 k1 加密的数据(见上方概念);所以偷窥者拿到 k\u0026rsquo;也不能解密出k Step4:网站拿到 k\u0026rsquo;,用 k2 解密,得到 k,至此双方就可以用 k 来进行数据交换的加密,建立起 SSL 隧道 这个过程看起来严密,但是攻击者(我)可以伪造密钥对——我在第二步时截获 k1,我自己生成一个伪造的密钥对 fk1 \u0026amp; fk2,将 fk1 抢先发给浏览器\n浏览器拿到 fk1,以为是网站的 k1, 就用其加密 k,将 k\u0026rsquo; 发出去 发出去后,再次被我截获,我拿 fk2 解密 k\u0026rsquo;,就会得到 k;此时,我再用之前的 k1去加密 k,得到 k\u0026rsquo;\u0026rsquo;,将其发给网站 最后网站利用 k2 解密 k\u0026rsquo;\u0026rsquo;,发现可以正常解密,以为自己与浏览器已经建立起了加密通道. 总结一下就是攻击者(我)让浏览器以为我是网站,让网站以为我是浏览器.这就是中间人攻击(Man-In-The-Middle attack)\n方案二:添加CA证书 核心问题在于浏览器和网站不知道发来的公钥是不是对方的,这就需要身份认证,也就是我们之前提到的证书与CA环节,CA扮演公证人. 加上 CA 后,整个环节变为了:\nStep1:网站从 CA 那里购买证书,生成证书,证书内容包括:网站公钥(私钥网站自己保存),域名,有效期,经过 CA 私钥签名后的数字签名——这是信任链的根源,浏览器内置了受信任的 CA 的公钥. Step2:浏览器访问,网站将此证书先发给浏览器,告诉浏览器\u0026quot;我是我\u0026quot;,浏览器会提取证书内容,用内置的 CA 公钥解密数字签名; 如果解密成功,就可以基本确定这是要访问的网站,是真的 如果解密失败,浏览器提示 CA 证书安全警告 同时再检查域名是否一致,这样就完成了身份验证, Step3:浏览器从证书中提取出网站公钥,随机生成对称加密的 k, 拿证书中的网站公钥加密 k,得到k\u0026rsquo;,将 k\u0026rsquo; 发给网站 Step4:网站用自己的私钥解密 k\u0026rsquo;,得到 k,至此,密钥交换完成. 这个过程中,就算攻击者拿到了证书,但是当他伪造密钥对发过来时, 由于攻击者没有 CA 的私钥,无法生存有效的数字签名,浏览器会解密失败,显示“此证书不受信任”\n这是 RSA 密钥交换,理论上讲,这个方案无懈可击,但是仍有服务器私钥泄漏的风险.\n握手过程 说了这么多背景知识,终于来到了 SSL(TLS) 的正题,如何建立这个加密的安全通道.\n握手环节发生在浏览器查询网站的源服务器阶段or任何其他使用 HTTPS 的通信过程.(例如 DoH);在通过 TCP 握手打开 TCP 连接(HTTP 使用 TCP 连接)后,发生握手.\nRSA 密钥交换下的握手 Step1:ClinetHello,该消息包含客户端支持的 TLS版本,支持的密码套件,以及一串称为“客户端随机数（client random）”的随机字节 Step2:ServerHello,作为对 client hello 消息的回复，服务器发送一条消息，内含服务器的 SSL 证书,服务器选择的密码套件,以及服务器随机数. Step3:客户端拿到 SSL 证书开始身份验证(上面讨论过) Step4:ClinetKeyExchange: 发送 premaster secret(就是上面的 k\u0026rsquo;)给服务器 Step5:服务器使用私钥解密 Step6:双方生成会话密钥(就是上面的 k) Step7:客户端发送 Change Cipher Spec通知,同时发送加密的 Finished 消息验证握手完整性 Step8:服务器发送加密的 Finished 消息 Step9:至此,已完成了握手,使用 k 进行后续的通信. 一共2-RTT\nTLS1.3 ECDHE密钥交换 在新版本 TLS1.3 中,不再支持 RSA密钥交换,缩短了握手过程\nClinetHello:由于已从 TLS 1.3 中删除了对不安全密码套件的支持，因此可能的密码套件数量大大减少。 客户端问候消息还包括将用于计算预主密钥的参数(例如ECDHE)。大体上来说，客户端假设它知道服务器的首选密钥交换方法（由于简化的密码套件列表，它有可能知道）。 这减少了握手的总长度——这是 TLS 1.3 握手与 TLS 1.0、1.1 和 1.2 握手之间的重要区别之一 服务器生成主密钥：此时，服务器已经接收到客户端随机数以及客户端的参数和密码套件。它已经拥有服务器随机数，因为它可以自己生成。因此，服务器可以创建主密钥 k 服务器问候和“完成”：服务器问候包括服务器的证书、数字签名、服务器随机数和选择的密码套件。因为它已经有了主密钥，所以它也发送了一个“完成”消息 最后步骤和客户端“完成”：客户端验证签名和证书，生成主密钥，并发送“完成”消息 缩短到了1-RTT,k是由客户端与服务器各自独立生成的,至于其中的原理,涉及到密码学,还不是鼠鼠我考虑涉及的领域,总之就是 ECDHE 这个算法很牛逼.\n如何应用 TLS 在这篇文章中,我们建立了 Docker 私有镜像仓库,使用到了 TLS 证书.\n在这个过程中, 仓库就相当于是网站服务器, Docker 客户端就相当于是浏览器.\n在实现时,我们将 Let\u0026rsquo;s Encrypt 这个 CA 生成的证书复制给了仓库 registry 容器使用, 每当我们使用 docker 命令例如 docker pull/push myregistry.example.com/image:tag时, Docker Daemon 就会向仓库发起 HTTPS 请求. 仓库会监听 HTTPS 端口(43),响应客户端的请求.\n我们拿当时生成证书后产生的结果说明:\nYour cert is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/jimlt.bfsmlt.top.cer [Fri May 9 09:53:29 AM CST 2025] Your cert key is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/jimlt.bfsmlt.top.key [Fri May 9 09:53:29 AM CST 2025] The intermediate CA cert is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/ca.cer [Fri May 9 09:53:29 AM CST 2025] And the full-chain cert is in: /root/.acme.sh/jimlt.bfsmlt.top_ecc/fullchain.cer top.cer 是我的站点的证书,即服务器证书,包括域名,公钥,有效期,签发者等信息 top.key 是服务器私钥 ca.cer 是中间 CA 机构的证书 fullchain.cer 是完整的证书链,可以让浏览器从我的网站一路向上追溯到根 CA 应用中遇到的问题 date 如何影响 TLS ssh 连接中的证书 session 和 cookie 是干嘛的 总结 本文介绍了 HTTPS 中的 SSL,包括非对称加密,密钥交换,证书,CA,握手等概念.\n本文的写作动机是在进行相关操作时通常对操作过程的原理不够清楚,导致操作过程中出错也不置可否,所以旨在弄清楚每一个过程中的原理,让自己能够安心操作.(不只是依赖于 AI)\n当然,本文对 SSL 的一些细节并没有过于深入,特别是密码学的部分,有兴趣的地方希望大家补充.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n引用 https://www.cloudflare.com/zh-cn/learning/ssl/what-happens-in-a-tls-handshake/ https://program-think.blogspot.com/2014/11/https-ssl-tls-0.html#index ","permalink":"http://localhost:1313/posts/028%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C02https/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003eHTTPS在我们的日常生活中已经司空见惯,特别是有时我们会遇到这种页面:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1\" loading=\"lazy\" src=\"/img/bg/https1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是什么意思呢?这个来自浏览器的提示意味着我们访问的 HTTPS 页面并不安全.\n那为什么会不安全呢?是因为其网站的证书出现问题了.\u003c/p\u003e","title":"兴趣八股之计算机网络EP02——HTTPS"},{"content":"引子 当我们不挂梯子的时候访问 www.google.com 会发生什么？ 没错，大概率是访问不到的,我的火狐浏览器会一直显示在连接但是连接不到，那么这是为什么呢？\n这是因为我们的 DNS 受到了污染，也有可能遭到了劫持，至于这两者是什么意思，就请慢慢往下看吧。\nDNS是什么 全称 domain name system\n还是拿 www.google.com 举例，这是一个 url，当我们在浏览器中输入它的时候，我们电脑中的软件会根据它将其翻译为一个具体的 IP 地址，然后访问这个 IP 从而访问到真实的网站。 这整个过程就是域名解析，没错，DNS 就是一个翻译器角色。至于它怎么翻译的，我们稍后会说。\n那我们顺便来解释一下这个 url 吧，很明显它通过 dot 分为了几个部分，因为互联网上的域名是树形结构组成的，从 .com开始是顶级域名，往左边依次递减；平时常见的 .cn .org等都是顶级域名。\n好的，知道了这些，我们就来解释一下从我们的电脑视角出发，域名解析的过程是怎么样的。\n从客户端的视角出发 发起对 url 的访问请求后\n浏览器会先检查自己有没有缓存过这个域名的 IP 地址（这叫浏览器缓存），如果没有进入下面步骤： 找本地 DNS 缓存（其他程序访问过留下的）； 找 Host 文件/etc/hosts，查看这个 url 是否有写好的对应 IP 地址； 如果有，那直接获取这个地址；如果没有，会去查我们是否有配置（在hosts中）指定的 DNS 服务器地址（比如我们常见的 8.8.8.8,或者路由器自动配置的） 如果有，电脑会向这个服务器发送 DNS 查询请求，等待其返回我们想要的 IP 地址 最终拿到 IP 地址并访问。 从服务器的角度出发 刚刚我们的电脑把解析任务都交给指定的 DNS 服务器了，那么它具体做了什么呢？\n我们平时配置的服务器都是递归（本地）服务（解析）器，而这个 DNS 服务器可能也有缓存，如果有就直接回复 如果没有，递归就会去找根域名服务器帮忙，询问：“请问 .com 顶级域名的服务器在哪里？” 根服务器会返回 .com 顶级域名服务器的地址，然后到了顶级域名服务器再问 “ google.com 的权威服务器在哪里？” 到了权威服务器后，\u0026ldquo;www.google.com 这个域名的 IP 地址是 xxxx\u0026rdquo; 最后，递归服务器问了一圈拿到了这个 IP 地址后给到电脑，电脑再给到浏览器，浏览器再拿去访问，同时缓存。 这里需要注意的是，缓存是个很重要的东东，因为这一串访问是十分耗费时间的；\n同时如果某个域名对应的 IP 地址变了或者这个域名不再使用了该怎么办？在我们的递归服务器上会对这些记录有周期记录，其生命周期一过就会重新去找各位大哥服务器寻找最新的 IP 。\n可能发生的问题 看了上面的两类流程后，你觉得可能会发生什么问题吗？我所说的问题不是解析不到 IP 的问题，有没有可能我们拿到假的 IP 地址？\n拿一个案例来说，很早以前，据坊间流言，有的网络中访问 www.google.com最终会呈现出百度的首页，这是为什么呢？\n这就引出了我们接下来要说的两个问题：DNS 污染 与 DNS 劫持。\nDNS劫持 回答我们上面案例中的问题，这是因为我们的 DNS 被污染了——在某些运营商的网络下，它会给我们自动设置 DNS 递归服务器地址.\nAnd by default, the OS will just use whatever resolver the network told it to. When the computer connects to the network and gets its IP address, the network recommends a resolver to use.\n在我们进行 DNS 解析的过程中，它会将递归服务器中有关 www.google.com的对应 IP 地址悄悄地“更改”为百度的 IP 地址（至于其为什么这么做，肯定是与百度有合作的啦）\n这就造成我们每次访问谷歌的时候在递归服务器的缓存中都是百度的 IP 地址，自然就跳转到了百度的首页。\n这整个过程就是 DNS 劫持。\n如何做到劫持 劫持通常都是 ISP（运营商）干的好事，如果我们用它自动设置的递归服务器地址，那么就得按照这个服务器上的规则来，它一旦修改了某个域名对应的 IP 地址，我们自然就被劫持了。\nThat’s because the resolver itself — the one that the network gives to you — could be untrustworthy.\n如何应对劫持 最简单的的方法就是修改我们 hosts 文件中的 DNS递归服务器的地址，不再是运营商为我们自动分配的，比如改成8.8.8.8,8.8.4.4这些常见的地址。\n这些是谷歌提供的地址，谷歌不至于劫持我们的 DNS 请求吧，这就可以避免 ISP 根据其递归服务器地址来劫持我们的请求。 Firefox的应对方法是使用 Cloudflare 作为他们的递归服务器，因为 Cloudflare 承诺保证用户的隐私性。\n但是这并不能解决所有问题，因为 DNS 查询的明文性，还是会有可能直接拦截 UDP/53 端口并进行重定向。\n那怎么办？诶，还有对策，但是这里的对策涉及到了其他知识点，我们先放放，待会儿再说。\nDNS污染 除了劫持这种直接的手段以外，还存在更高明的手段，就算我们修改了递归服务器的地址，DNS 污染也能让我们访问不到正确的网站。\n这是因为在 DNS 应答的过程中存在时间差，我们得等 DNS 递归服务器去外面兜一圈，但是等到它回来发现家都被偷了！\n其实 DNS 污染还有一个更加可怕的名字，DNS 投毒。至于为什么，请看下面的内容。\n如何做到污染 This means that every server that you ask to help with domain name resolution sees what site you’re looking for. But more than that, it also means that anyone on the path to those servers sees your requests, too.\n上面说过，DNS 查询的过程是明文的（UDP/53）。 所以这条查询路上的中间人可以监听你的 DNS 请求包，并快速伪造一个虚假的 DNS 响应包抢在真正的回应前返回，这样我们就会拿到错误的 IP 地址。\n那为什么又叫做投毒呢？假如我们不用国外的递归服务器，还是用国内 ISP 的，ISP 并没有发生劫持。 但是我们要访问一些国外的网站，那这些请求就一定要出海，在这个过程中有人对我们进行 DNS 污染，这会导致什么后果呢？还记得我们前面说到的重要的缓存吗？这就会让国内的服务器上记录这些域名与其错误的 IP 地址，那久而久之，广而广之，所有的国内服务器是不是已经全部被污染了！\n最终，我们从任何一个国内服务器访问这些网站都会定向到假的 IP 地址，这就达到了恐怖的投毒效果，火烧连营啊！\n如何应对污染 从原理看我们已经发现了DNS 过程的脆弱性了，DNS 查询过程是明文的，我们对其加密不就好了吗？\n这样，就引出了 DNSSEC, DoT, DoH 这三种更为高级的 DNS 技术。来看看怎么回事。\n高级的DNS DNSSEC 这是出现较早的一种方法，它的目的是保证数据的完整性，确保 DNS 的响应不被篡改。怎么做到的呢？\n增加了数字签名机制，会在每一级使用数字签名来证明下一级的可信性。 回到我们服务器的角度，DNS 递归解析器内置了根域的公钥，根服务器会返回 .com的DNSKEY,NS记录，用根的公钥验证.com的DNSKEY，用这个DNSKEY 验证 NS 记录，依次类推。\n整个过程就是：上一级用数字签名告诉你：“下一级公钥是真的”，从根开始，逐级验证，一环都不能断。 所以完整性由上级域保证，其他人就无法篡改我们的内容了，就算你返回一个假的响应包过来，没有签名验证，我们客户端的递归服务器是不要的。\n但是，诶，是不是还有些问题？我们只保证了数据的完整性，但是数据的加密没有得到保证\n别人依然能看到我们这个 DNS 请求是往哪里去的，隐私在别人面前暴露无疑。 劫持仍有可能发生，因为我们本机与递归服务器之间还有一段传输过程，这一段传输过程没人管啊。 因为上述缺点，引出下面两个更加强大的方式。\nDoT 全称 DNS over TLS,通过TLS协议加密DNS查询和响应，防止窃听和篡改.（关于TLS如何做到这一点，见这篇文章） 专用端口853运行DNS over TLS，与传统明文DNS（UDP 53端口）隔离；TLS握手：客户端与递归服务器建立加密连接。\n协议栈如下：\n-------- DoT -------- TLS -------- TCP -------- IP -------- 缺点在于853端口可能被 ISP 禁用，客户端也就是我们的电脑或者浏览器需要支持 DoT,目前很多的浏览器更多地在用 DoH，至于为什么请往下看。\nDoH 全称 DNS over HTTPS,诶，相比于 DoT多了什么？多了HTTP！（HTTPS=HTTP+TLS）\n协议栈如下：\n-------- DoH -------- HTTP -------- TLS -------- TCP -------- IP -------- 同时还可以做到将 DNS 查询伪装成普通 HTTPS 流量，绕过审查和干扰（DNS查询通过HTTP/2或HTTP/3传输，端口443，与网页流量混合）\n由于集成了 HTTP 的缘故，像 Firefox, Chrome 等浏览器都集成这个功能，但是比 DoT来说更大了。\n并且有的浏览器和服务商也根据 DoH 对 DNS 的过程进行了优化，例如循环访问的过程只携带部分信息。QNAME minization（我们上面举例的过程其实就做到了QNAME minization）\n潜在问题 在 HTTPS的过程中，有这样一个问题：你访问网站时最开始的 SNI 还是明文，可能被 ISP 知道你访问了什么站；但一旦 TLS 建立成功，你访问服务器上其他站点的过程是加密的，也更私密了。\n这一点我们在后续有关HTTPS的文章中详细讨论。\n总结 DNS 作为域名与 IP 地址之间的翻译器，会通过一系列的 recursive resolve 来进行翻译。 在这个过程中可能会遇到 DNS 劫持和污染，为了防止这些问题，其采用了 DNSSEC, DoT, DoH 等策略来对传输的数据进行加密和防止篡改。\n最后关于文章一开始说的挂梯子的操作，其为什么可以让我们访问到原本被污染或者劫持的网站，简单来讲是因为 VPN 会加密所有流量（包括 DNS），这样攻击者就无法识别我们这些流量的作用，自然就不能轻易地攻击我们。（关于 VPN 如何加密后面考虑出一篇文章）\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n引用 https://hacks.mozilla.org/2018/05/a-cartoon-intro-to-dns-over-https/ ChatGPT \u0026amp; DeepSeek ","permalink":"http://localhost:1313/posts/027%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C01dns/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e当我们不挂梯子的时候访问 \u003ccode\u003ewww.google.com\u003c/code\u003e 会发生什么？ 没错，大概率是访问不到的,我的火狐浏览器会一直显示在连接但是连接不到，那么这是为什么呢？\u003c/p\u003e\n\u003cp\u003e这是因为我们的 DNS 受到了污染，也有可能遭到了劫持，至于这两者是什么意思，就请慢慢往下看吧。\u003c/p\u003e","title":"兴趣八股之计算机网络EP01——DNS"},{"content":"引子 看到这个标题，可能你第一个想到的是抖音这个常常被人们说为精神鸦片的软件，刷着刷着时间就走完了。但今天我想聊的是另外两款红色封面的软件，二者的内容呈现形式几乎相同，但是内容格式完全不同，我对于它们的使用也迥然不同。\n没错，正是小红书与B站。\n小红书 先来说说小红书吧，它是如何令人上瘾的呢？现在的小红书，已经被人打上了“年轻人的搜索引擎”这样的标签，遇到什么问题去小红书搜一下，大部分都能解决。因为那里面有着遇到过相同问题的姐妹兄弟的笔记攻略，而且内容十分详细。\n但是在我日常使用中会遇到什么样的情况呢？我搜了一下这叫做注意力劫持。比如某天我带着想要解决某个问题的目的打开小红书，我第一步该做什么？一定是搜索我的问题对吧，但是映入眼帘的通常是四条内容，比如我现在刚刚打开的小红书：\n“这两头猪谁更蠢一点？” 封面是两位NBA教练 “如果完成四周跳，母亲承诺与他发生性关系” 封面是一男两女的照片 “和老公的产房聊天记录” 封面是微信聊天记录 “幕后的每天都在上班” 封面是一位明星 哇，“两头猪”、“四周跳”、“性关系”、“产房”、“聊天记录”、“明星幕后”，几乎每一条内容的标题都很吸睛，可能除了第一条以外我都想点进去看看究竟。\n然后通常情况下我就真的点进去看了，看完笔记的内容后还去翻评论区的评论，一个帖子几乎可以耗掉我3分钟的时间。退出的时候我好像已经忘了自己是来干嘛的，我是来搜索问题的啊。这个时候潜意识就会告诉自己，没事，再看一些吧，把其他你想看的都看了再查也不迟。\n在越来越往下看时，会出现更多地针对我的，符合我胃口的笔记，例如暑期实习，秋招，Mac等等内容。就这样我就陷入了小红书笔记的汪洋大海中，越来越无法回头。\n就这样小红书以其夸张精湛的标题和吸引人的封面图锁住了一个又一个的用户群体。\n对了，xhs没有“稍后再看”这个功能。\nB站 再来说说B站吧，同样的内容网站只是从文字换成了视频，内容的结构也差不多，四个标题+封面的组合（除了一开始中间会有一个滚动的长屏），但是B站就是吸引不了我点开所有内容，来看看当前我的B站主页上的内容吧\n“小潮生活vlog” 封面是小潮院长这位up主，并且是我的关注。 “力扣周赛精讲” 封面是直播内容，因为这是一场直播，还是我的关注。 “春夏刚需！49元完美白T，外穿内搭无敌” 封面是一位up拿着一件白T，但不是我的关注。 “C罗，马内，杜兰破门！打败日本横滨水手只用半场” 封面是C罗。 与小红书的内容比较就会发现很大的不同，其中两个内容来源于我的关注，另外两个的标题好像也没有那么吸引我的注意力，这四个内容我的打开欲望根本没有小红书那么大。\n对比下来很明显二者的主页逻辑不同，小红书主页很少出现我关注的博主的内容，或者说出现的频率较低，通常我都要去点关注那一栏看；而B站则会优先推送我关注的博主；这或许与平台的推荐算法有关。\n二者的内容的标题也大不相同，小红书每一条内容的标题几乎都十分吸引眼球，即使点进去会存在偏差，但B站的视频标题没有那么劲爆或者引人瞩目。\n结果就是，打开小红书我会很容易地被吸引进去从而消耗掉原本没想要消耗的时间；而B站并没有小红书那样大的吸引力来劫持掉我的注意力，我在主页停留的时间反而较短，转头进入了自己的关注列表即动态中去寻找想要看的视频。\n背后发生了什么 《注意力商人》与《不可打扰》这两本书我希望我今年可以读完。\n","permalink":"http://localhost:1313/posts/026%E5%A5%87%E6%80%9D%E4%B9%B1%E6%83%B3ep03%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84app/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e看到这个标题，可能你第一个想到的是抖音这个常常被人们说为精神鸦片的软件，刷着刷着时间就走完了。但今天我想聊的是另外两款红色封面的软件，二者的内容呈现形式几乎相同，但是内容格式完全不同，我对于它们的使用也迥然不同。\u003c/p\u003e","title":"LT的奇思乱想EP03——让人上瘾的APP们"},{"content":"引子 字符串操作在任何语言中的地位都十分重要，在上篇关于 Golang 中特殊的切片讲完之后，这一次我准备进入 Golang 中字符串的底层世界，包括引用总结自\u0026lt;100 Go Mistakes and How to Avoid Them (Teiva Harsanyi)\u0026gt; 书籍的注意事项。\n先拿一段力扣上的代码来说吧，本题是125验证回文串,大概要求是这样：如果在将所有大写字符转换为小写字符、并移除所有非字母数字字符之后，短语正着读和反着读都一样。则可以认为该短语是一个 回文串 。字母和数字都属于字母数字字符，s 仅由可打印的 ASCII 字符组成。\nfunc isPalindrome(s string) bool { var filtered []rune s = strings.ToLower(s) for _, c := range s { if unicode.IsLetter(c) || unicode.IsDigit(c) { filtered = append(filtered, c) } } // 处理完后判断回文 left, right := 0, len(filtered)-1 for left \u0026lt; right { if filtered[left] != filtered[right] { return false } left++ right-- } return true } 这段代码中有一个在其他语言中没见过的东西 []rune，可以看到，我们先将 s 全部小写化，再遍历判断是字母还是数字最后全部添加回了这个 rune 切片当中（由于不知道长度，所以没有提前声明切片长度），然后使用双指针进行判断回文。\n那么我们就来研究一下为什么要多次一举将字符串转换，并且还新开辟了一片空间专门保存，这看起来是有损性能的不是吗？如果换成其他语言例如Java，Python，会怎么处理呢？\n本篇文章将梳理 Golang 关于字符串的各种常见知识点，并且在每个知识点后都会与 Java 进行对比学习（以 JDK8 为准）。\nstring底层 Go:\ntype stringStruct struct { str *byte // 指向底层字节数组的指针 len int // 字符串的长度 } 首先要注意的是 string 的不可变性，只读。在 Golang 的底层，字符串是由一个字节数组构成的，就像切片指向底层数组那样。\nJava:\npublic final class String implements java.io.Serializable, Comparable\u0026lt;String\u0026gt;, CharSequence { private final char value[]; // 存储字符 private final int offset; // 起始位置（早期为了 String.substring() 设计的） private final int count; // 字符串长度（同样是为了 substring） private int hash; // 缓存hashCode，提高效率 } 底层是一个 char[] 字符数组，这一点与 Golang 不同\n说到字节数组，我们就不得不考虑，什么是字节？\nUnicode/UTF-8 这当然是一个简单的不能再简单的概念，但是为了引出rune的概念，我们得看看 Golang 是如何利用字节的。在此之前我们需要引出两个基础概念:\ncharset字符集 encoding编码方式 看本节标题就知道Unicode是字符集，UTF-8是编码方式\n在讨论字符集时，我们常听到 ASCII，它表示每个字符的 Code Point 的值。 而 Code Point 标志着每个字符在字符集中的编号,即字符的逻辑（抽象）编号。 例如，字符 \u0026lsquo;A\u0026rsquo; 的 Code Point 是 U+0041, 对应的 ASCII 是 65。\nGo 语言使用的是更通用的 Unicode 字符集，它不仅兼容 ASCII，还能表示更广泛的字符，如中文、表情符号（如 😊）等，因此更加包容和国际化。\n关于编码方式，其关心的就是如何将字符集中的 Code Point 转换为二进制的形式供硬件理解使用，UTF-8是Golang在标准库中采取的编码方式，其一般使用1~4字节来编码；例如如果是一个汉字，他会用3字节编码。\n用一个例子来总结上面两个概念，\u0026lsquo;汉\u0026rsquo;这个字符的字符集是 Unicode ,其 Code Point 是 U+6C49 ,要将其转换为二进制形式我们需要使用UTF-8编码方式，并需要3个字节进行编码：0xE6, 0xB1, 0x89，最终形成一个二进制。\n那么Java呢？\n在早期， Unicode 还没有那么多的字符的时候，16位两字节恰好可以表示所有的字符，所以 Java 采用 16bit 来表示 char 类型，一个 Java 字符 = 一个 Unicode 字符。\n并且对这些字符的编码采用 UTF-16 的方式，即2字节编码，而不是1～4字节编码，（但是后续例如 emojy 表情的出现，使得 Java 对这些特殊字符采取了4字节编码的方式）\n那么 UTF-8 与 UTF-16 的区别就在于8的灵活性1～4字节，和16的死板性全部2字节，当英文字母多的时候8所占的字节就少，但当汉字多的时候16所占变少。所以相对而言， Golang 的编码方式更加适合网络传输，节省空间。\nrune类型 rune is an alias for int32 and is equivalent to int32 in all ways. It is used, by convention, to distinguish character values from integer values\n在源码中官方注释称 rune 是 int32 类型的别名，相当于 int32 类型，按照惯例用来区分字符值和整数值。\n上面提到过，Go 使用至多4字节进行编码，也就是 32bit；而 rune 就是用于表示 Unicode 字符的内建类型，用来存储 Code point 这个编号的，故本质上是一个 int32 类型\n用一段代码来演示：\nfunc main() { s := \u0026#34;hello\u0026#34; r := []rune(s) for i, v := range r { fmt.Printf(\u0026#34;r[%d] = %d (char: %c, code point: %U)\\n\u0026#34;, i, v, v, v) } } // 输出结果如下 r[0] = 104 (char: h, code point: U+0068) r[1] = 101 (char: e, code point: U+0065) r[2] = 108 (char: l, code point: U+006C) r[3] = 108 (char: l, code point: U+006C) r[4] = 111 (char: o, code point: U+006F) 可以发现 rune 用来表示（or存储）一个 Unicode 字符的 Code Point。 当我们执行 fmt.Println(rune('A')) 的时候，它会输出 65，代表着 \u0026lsquo;A\u0026rsquo; 的 Code Point 的值。\n当然我们也可以粗略地将 rune 看作字符串中的每一个字符，即 Go 中字符字面量是 rune.\n那么Java呢？\nJava 中的 code point 是通过 1个char 或2个char组合而成的一个 int 型整数（之所为会有2个上面说过了，为了应付新出现的字符）\nString s = \u0026#34;𝄞a\u0026#34;; // U+1D11E System.out.println(s.length()); // 3，因为 \u0026#34;𝄞\u0026#34; 占 2个char + \u0026#34;a\u0026#34; 1个char System.out.println(s.codePointCount(0, s.length())); // 2个code point rune与byte 最早我们提到过，Go 中字符串指向字节数组，那么这里就引出来一个致命问题，为什么 Go 的设计者要引入 rune 这个概念，到底有什么作用呢？\nbyte 是 uint8 的别名，仅能表示 0-255 的值，所以我们说的\u0026rsquo;汉\u0026rsquo;这个字符不能单个表示，得多字节表示。 所以如果只用字节操作多字节字符，就需要手动处理，容易出错。 rune 是 int32 的别名，其可以直接单独表示某多字节字符。从程序员的角度来看，不需要考虑编码问题，不关心这个多字节字符由几个字节编码。 rune 有效区分了字符和编码，并为我们屏蔽了下面的细节。 那么 Java 呢？\nJDK9也意识到了2字节的编码方式太浪费内存了，毕竟大部分还是以英文字母形式出现的，所以将底层的字符数组改为了字节数组,并且设置不同的编码器按情况编码，不再拘泥于 UTF-16。（但只要 coder 为1，整体都要用 UTF16编码）\npublic final class String implements java.io.Serializable, Comparable\u0026lt;String\u0026gt;, CharSequence { private final byte[] value; // 存储字节数组 private final byte coder; // 编码器 0=LATIN1(单字节)，1=UTF16(双字节) private int hash; // 缓存hashCode } 但是 Java 的向后兼容性很强，平常用的方法还是继续使用就好，作为开发者就好像无事发生，透明的。s.charAt(i)\n概念理清楚后，接下来让我们看看可能遇到的坑。\n常见问题 这里引用100 Mistakes 中列举出来的问题加以说明巩固。\nlen(s) s := \u0026#34;汉\u0026#34; fmt.Println(len(s)) 猜猜这里会输出1还是3？显然，字符串的底层还是字节数组，这是逃不掉的命运，所以len函数输出的是一个字符串的字节数组长度，而不是 rune 的长度。\n同样的，在 Java 中 s.length() 返回的不是字符数，而是底层的 char 单元的数量，比如我们上面提到过的音符字符，它会占两个 char 单元，所以整体长度为3而不是2。\nrange遍历字符串 s := \u0026#34;hêllo\u0026#34; for i := range s { fmt.Printf(\u0026#34;position %d: %c\\n\u0026#34;, i, s[i]) } fmt.Printf(\u0026#34;len=%d\\n\u0026#34;, len(s)) // 输出结果如下 position 0: h position 1: Ã position 3: l position 4: l position 5: o len=6 两个问题：\n下标2怎么不见了？ 为什么第二个输出了Ã？ 很明显，ê 并不是一个ASCII字符，其需要不止一个字节表示； 但这并不能回答我们的问题，核心在于：range s 中的 i 指向每个字符底层字节数组中的起始位置，s[i]自然也就输出的是起始位置处的字符。\n那该如何规避这个问题呢？在range的时候不要用下标，直接使用其值。发现下标还是指向的是底层字节切片的起始下标。\nfor i, r := range s { fmt.Printf(\u0026#34;position %d: %c\\n\u0026#34;, i, r) } // position 0: h position 1: ê position 3: l position 4: l position 5: o 或者更常见的方法，转换为[]rune切片。好处在于下标是对的上的，就表示每个字符在字符串中的位置。\ns := \u0026#34;hêllo\u0026#34; runes := []rune(s) for i, r := range runes { fmt.Printf(\u0026#34;position %d: %c\\n\u0026#34;, i, r) } // position 0: h position 1: ê position 2: l position 3: l position 4: o 这里再补充一下，s[i] 这样的操作会输出第i个字节。\n所以在对于普通的英文字符串进行遍历访问时，s[i] 基本等同于字符访问；而对于多字节字符（中文等）这样访问就很容易出 bug。\n用 range s 遍历字符 转换为 rune 切片 []rune(s)，按照下标进行访问 这也就回答了我们最开始的问题，那段回文代码中为什么要使用 rune,就是为了防止多字节字符的影响。\n那么 Java 呢？\nString s = \u0026#34;Go语言\u0026#34;; for (int i = 0; i \u0026lt; s.length(); i++) { char c = s.charAt(i); System.out.println(c); } 上面我们说到的 char 单元的概念你应该还记得，没错就是那个音符，charAt(i)会按照 char 序号，第几个就是第几个 char。\n但是遇到音符我们的 charAt 就不能解决了，因为音符一定是由多个 char 组成的，charAt(i) 时我们会将这个音符“撕裂”,推荐使用提供的 API codePointAt 会识别 code Point 是一个 char 还是两个 char。\nString s = \u0026#34;A😊B\u0026#34;; for (int i = 0; i \u0026lt; s.length(); ) { int cp = s.codePointAt(i); System.out.println(Integer.toHexString(cp)); i += Character.charCount(cp); // 注意步长是1或2 } Trim方法 Go 的 strings 包下包含了多种操作字符串的方法，其中容易出问题的是 trim 类的方法。\n‘TrimRight removes all the trailing runes contained in a given set’ ,而 TrimSuffix 是真正移除给定 set TrimLeft 和 TrimPrefix 同理 Trim 是 TrimRight 和 TrimLeft 结合体 需要补充的是，strings包下有很多好用的方法，如果我们想操作字节数组该怎么办呢？ 要把 []byte 转为 string 然后利用 strings 最后再转回去吗？ 其实 Go 的 bytes 包也为我们准备许多类似于 stirngs 包中的方法，可以直接操作字节数组。\n连接字符串 由于 string 是不可变的，所以我们如果简单地用 \u0026lsquo;+\u0026rsquo; 来连接那岂不是不断开辟新的底层字节数组然后将内容复制进去。\n他不会像切片那样共享底层数组，因为其不可变性。\n对此，Go 标准库提供了一个好方法 strings.Builder\nfunc concat(values []string) string { sb := strings.Builder{} for _, value := range values { _, _ = sb.WriteString(value) } return sb.String() } 这会一次性分配大内存，避免多次复制。其底层实际上还是一个字节切片，每次 WriteString 时就是给这个切片中 append\n到这里如果你看过之前关于切片底层的文章，你会敏感地发现向这个字节切片进行 append 不是也从0开始的吗？然后不是还会发生扩容吗？感觉效率仍然不是很高。\n所以 Go 还为我们提供了一个附加方法，如果我们可以知道这个 Builder 的总长度，可以预先声明，利用 sb.Grow(x)方法，带来极大的性能提升。\n最后，平时开发中（也是我在百度代码中见的最多的）还是使用 fmt.Sprintf(),fmt.Sprintf() 来格式化连接字符串较为常见。\nmessage := fmt.Sprint(500, \u0026#34;internal server error\u0026#34;) 那么 Java 呢？\n我们平时好像也用+号，例如String s = \u0026quot;Hello\u0026quot; + \u0026quot;World\u0026quot;;但实际上 javac会在编译时直接优化成 String s = \u0026quot;HelloWorld\u0026quot;;；如果是 a+b,底层会优化为 StringBuilder\n所以 Java 官方还是推荐我们使用 StringBuilder（当然 StringBuffer也可以，线程安全）\nString concatenation is implemented through the StringBuilder(or StringBuffer) class and its append method\nStringBuilder sb = new StringBuilder(); for (int i = 0; i \u0026lt; 100; i++) { sb.append(\u0026#34;data\u0026#34; + i); } String result = sb.toString(); 和 Golang 有点类似对吧。也可以使用 join String result = String.join(\u0026quot;,\u0026quot;, listOfStrings);\n其实这么看来 JVM 已经为我们自动优化了，而 Golang 则需要我们自己注意一下。\n截取字符串 s := \u0026#34;hello world\u0026#34; sub := s[0:5] // sub == \u0026#34;hello\u0026#34; 如果我们直接用 [:]来截取的话, sub 的底层字节数组并没有改变还是持有着 s 的底层数组，看似没有什么太大的问题，如果这个 s 是一整个很大的 log 呢？\n这就会让内存压力很大，很大的 log 并不能及时地得到 GC 处理，因为我们 sub 还在用它。\n建议的解决方案有两个\nsub = s[0:5] subCopy := strings.Clone(sub) 显式复制，会将这部分字节数组复制到一片新的空间中从而避免共享。 sub := string([]byte(s[0:5])) 先转换为[]byte,再转换回string——Go 会 重新分配新的只读内存，拷贝 byte 数组内容，得到另外一个独立的字符串，不再共享。 Java 中使用 substring String sub = s.substring(0, 5);并且在 JDK8 以后它不会带来上面的问题，他会直接创建一个新的字符串和它的 char 数组（JDK7 以前会）\n比较字符串 在 Golang 中，两个字符串可以直接用运算符号进行比较，其底层执行的是字典序的比较。\n相等性比较 逐个比较字节，只有长度相同，对应位置字节全部相同时，才相等。 其他比较(\u0026lt;,\u0026gt;,\u0026lt;=) 直接字典序比较，不论长度；对应位置谁的字典序小谁就小。 替换字符串中的字符 在力扣 1410HTML 实体解析器题目中，我们可以直接使用 Golang 的 NewReplacer() + Replace() 方法进行多个位置上的一次性替换。\nfunc entityParser(s string) (ans string) { return strings.NewReplacer(`\u0026amp;quot;`, `\u0026#34;`, `\u0026amp;apos;`, `\u0026#39;`, `\u0026amp;gt;`, `\u0026gt;`, `\u0026amp;lt;`, `\u0026lt;`, `\u0026amp;frasl;`, `/`, `\u0026amp;amp;`, `\u0026amp;`).Replace(s)} 原理大概如下：\nNewReplacer() 形成了一棵决策树，这会建立起一个高效的搜索引擎。 Replace() 会执行一次性扫描，将符合决策树中的内容进行替换。 这样的时间复杂度只有一趟，是 $O(n)$的,而如果我们对每个字符都使用 ReplaceAll() 的话，时间复杂度就是 $O(n*m)$了。 总结 Go 中的 string 我们要注意 rune 和 byte 的区别；注意 len 返回的是什么；注意如何遍历，连接，截取字符串。\nJava中就三类 String, StringBuilder, StringBuffer；由 char[] 转向 byte[]，但还是 char[] 更常用， JDK8 的功劳。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n引用 《100 Go Mistakes and How to Avoid Them (Teiva Harsanyi)》 ChatGPT Leetcode ","permalink":"http://localhost:1313/posts/025golangep04_string%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e字符串操作在任何语言中的地位都十分重要，在上篇关于 Golang 中特殊的切片讲完之后，这一次我准备进入 Golang 中字符串的底层世界，包括引用总结自\u0026lt;100 Go Mistakes and How to Avoid Them (Teiva Harsanyi)\u0026gt; 书籍的注意事项。\u003c/p\u003e","title":"GolangEP04_string底层原理及注意事项"},{"content":"引子 手动更新Goland中较旧的delve 使用delve进行debug,但是在console页面显示警告信息 \u0026ldquo;Version of Delve is too old for Go version 1.23.0 (maximum supported version 1.24)\u0026rdquo;\n解决方法:下载最新的delve，替换掉Goland之前携带的delve\ngo install github.com/go-delve/delve/cmd/dlv@latest which dlv cp \u0026lt;which dlv results\u0026gt; /Applications/GoLand.app/Contents/plugins/go-plugin/lib/dlv/macarm/dlv 重启Goland，再次debug,console页面警告信息消失。方法来源于StackOverflow\ndebug闭包函数在走到闭包函数时会显示internal debugger error: runtime error: invalid memory address or nil pointer dereference\nVscode,Goland等IDE连接本机的终端是怎么连接的，为什么Goland第一次会用比较久的时间，而Vscode在Goland连接之后显示出的历史命令并不是之前的了？\n","permalink":"http://localhost:1313/posts/024goland%E6%93%8D%E4%BD%9C01_dlv/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003ch2 id=\"手动更新goland中较旧的delve\"\u003e手动更新Goland中较旧的delve\u003c/h2\u003e\n\u003cp\u003e使用delve进行debug,但是在console页面显示警告信息 \u0026ldquo;Version of Delve is too old for Go version 1.23.0 (maximum supported version 1.24)\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e解决方法:下载最新的delve，替换掉Goland之前携带的delve\u003c/p\u003e","title":"024Goland操作01_dlv"},{"content":"引子 在K3sEP04中最后的总结中我们提到，反思了一下我们的移植过程，开始从k3s的结构看起，再到k8s的书籍，再到重新看二者的架构设计不同点，再到对应的命令，再追踪到k3s的源码，最后再开发板上适配的时候解决出现的一系列问题，我们处理移植问题的思路首先是有问题的。\n还是引用一句话，大致的意思是，“你所遇到的问题100%都在别人的身上发生过，这意味着只要google，就一定可以找到答案。”但是我在这其中难道没有google过吗？我也不清楚为什么我没有发现对应的issue，直到最近，我才在github中找到了k3s关于支持riscv的相关issue，这证明自己处理此类问题的方法很有问题。\n虽然我也可以甩锅到带我的老师没有指引我，但实际上这是一种解决问题的能力欠缺的表现，就像我今天读到的一篇文章，“编程思维不是说你能写出多么漂亮的代码，而是考验你如何快速高效解决问题”，是的，从这件事情上我意识到其实自己这方面的能力十分欠缺。\n说回来，在这个issue中我发现了一个最有意思的问题，在2024-08-30，一位开发者说他只通过三条命令就在riscv64的机器上安装好了k3s并能够顺利运行。\n可以看到这位开发者和我的思路一致，都是自行构建解决了pause镜像的问题，所以我们根据他们的说法来试试吧，从k3s的源码构建，而不是简单的dnf install k3s\n从源码构建可运行的k3s二进制文件 cd k3s ./script/download ./script/build ./script/package-cli 一共就是这三条命令，就可以构建出可执行的k3s文件，注意执行顺序不可更改。\n接下来我来排一下可能会遇到的坑。\nyq未安装 yq是一个处理YAML文件的命令行工具,K3s构建脚本中使用它来解析yaml文件，所以需要提前安装yq。如果dnf install yq显示包管理器中没有yq的话，我们就需要从源码进行构建。\n# 需要安装 go 编译环境 git clone https://github.com/mikefarah/yq.git cd yq/v4 go build -o yq . sudo mv yq /usr/local/bin/ 或者直接去官方的release中查看有没有编译好的，发现其实是有的，可以直接进行下载使用\nwget https://github.com/mikefarah/yq/releases/download/v4.44.6/yq_linux_riscv64.tar.gz tar -zvxf yq_linux_riscv64.tar.gz mv yq_linux_riscv64.tar.gz yq chmod +x yq mv yq /usr/local/bin yq --version 有可能出现的源码错误 关于这个错误我也很疑惑，我并没有在开发板上修改过源码的内容，但是显示\n-w -s \u0026#39; -o bin/k3s ./cmd/server # github.com/k3s-io/k3s/pkg/agent pkg/agent/run.go:110:53: syntax error: unexpected name fmeEndpoint, expected { 证明源码中有地方出错了，我对比之下发现确实多了个空格，如果有遇到相同问题的朋友欢迎交流。\nlibseccomp未安装 与yq不一样的是，这是linux系统级别的底层库，如果未安装的话其大概率是在包管理器中的，我们可以先检查一下是否存在。\ndnf list libseccomp\\* Last metadata expiration check: 1:23:59 ago on 2025年04月19日 星期六 18时37分42秒. Installed Packages libseccomp.riscv64 2.5.4-1.oe2309 @OS libseccomp-devel.riscv64 2.5.4-1.oe2309 @OS Available Packages libseccomp-debuginfo.riscv64 2.5.4-1.oe2309 OS libseccomp-debugsource.riscv64 2.5.4-1.oe2309 OS libseccomp-help.noarch 2.5.4-1.oe2309 OS libseccomp-help.noarch 2.5.4-1.oe2309 EPOL 这里显示我已经安装好了，如果你没有安装的话，使用dnf install libseccomp-devel.riscv64进行安装。\n其他缺失的类似库也一样，我们都可以先list检查其是否存在然后再选择安装还是自行编译。（毕竟这是一个比较新的系统）\n结果 最终执行完毕之后结果如下:\n+ go build -tags urfave_cli_no_docs -buildvcs=false -ldflags \u0026#39; -X github.com/k3s-io/k3s/pkg/version.Version=v1.32.3+k3s-76c5c770 -X github.com/k3s-io/k3s/pkg/version.GitCommit=76c5c770 -w -s -extldflags \u0026#39;\\\u0026#39;\u0026#39;-static\u0026#39;\\\u0026#39;\u0026#39;\u0026#39; -o dist/artifacts/k3s-riscv64 ./cmd/k3s + stat dist/artifacts/k3s-riscv64 文件：dist/artifacts/k3s-riscv64 大小：80478360 块：157192 IO 块大小：4096 普通文件 设备：179,3 Inode: 661710 硬链接：1 权限：(0755/-rwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root) 访问时间：2025-04-19 19:04:50.007082376 +0800 修改时间：2025-04-19 19:04:50.375090740 +0800 变更时间：2025-04-19 19:04:50.375090740 +0800 创建时间：2025-04-19 19:04:50.007082376 +0800 + ./scripts/build-upload dist/artifacts/k3s-riscv64 76c5c770b21e101f00a32445d3e6fdc325d563b5 AWS_SECRET_ACCESS_KEY is not set 最终的二进制文件是./dist/artifacts/k3s-riscv64，我们可以在后面加上server和agent来分别执行这个二进制文件。\n执行过程中存在的问题 当然执行并不是一蹴而就的，会存在一系列的问题（谁能想到我第一次执行成功之后的就一直在失败）\nGolang的版本不匹配问题 如果我们真正执行起来会发现出现 Golang 的版本不匹配问题\n./dist/artifacts/k3s-riscv64 server INFO[0000] Acquiring lock file /var/lib/rancher/k3s/data/.lock INFO[0000] Preparing data dir /var/lib/rancher/k3s/data/ce5fac18afd90ce4ba321d56c827d30b3b3de0cbe5c654dbb397bf62a178878e FATA[0000] Failed to validate golang version: incorrect golang build version - kubernetes v1.32.3 should be built with go, runtime version is go1.23.3 意味着当前 k8s 需要的 Golang 版本不是我们目前的 go1.23.3，所以在哪里查找其需要的特定 go 版本呢？\n我 google 并没有得到准确的答案，gpt 告诉我在这个 commit中，其修改了 v1.32.3版本的 k8s 所需版本为 go1.23.6\n其实还可以从构建脚本中发现其所需要的版本，在build脚本中的这两句\nDEPENDENCIES_URL=\u0026#34;https://raw.githubusercontent.com/kubernetes/kubernetes/${VERSION_K8S}/build/dependencies.yaml\u0026#34; VERSION_GOLANG=\u0026#34;go\u0026#34;$(curl -sL \u0026#34;${DEPENDENCIES_URL}\u0026#34; | yq e \u0026#39;.dependencies[] | select(.name == \u0026#34;golang: upstream version\u0026#34;).version\u0026#39; -) 就是在这个 dependencies.yaml 文件中寻找 Golang 的版本，我们打开这个文件发现 v1.32.3的 K8s 所需的 Golang 版本为 1.23.6，与上面 commit 中的版本相同，验证了我们的想法。\n# Golang - name: \u0026#34;golang: upstream version\u0026#34; version: 1.23.6 所以我们构建就要使用 go1.23.6，想想也挺奇怪的，明明 k3s 源码的 go.mod 中指明了 go1.23.3,但是其依赖的 k8s 中的内容却要 go1.23.6???\n奇怪的downloading 一旦进入源码目录执行一切 go 开头的命令就会出现 go: downloading go1.23.3 (linux/riscv64)，我的 go 本地本来就有啊，为什么他还是会去 download 呢？并且去非 k3s 源码的其他目录执行 go version类似命令，正常输出。\n原因如下：在 Go 里面，如果当前目录下有 go.mod 文件，不管你执行什么 go 命令，它都会默认开启 module模式。\n检查 go.mod 和 go.sum 有需要时自动联网去拉缺失的模块 解决可能出现的问题 尝试切换版本 我们推倒重来，默认使用Golang版本为1.23.6,即构建版本为1.23.6；使用时切换到1.22or1.21。 安装时提前下载好对应版本的Golang安装包。\nrm -rf /usr/local/go tar -C /usr/local -xzf go1.23.6.linux-riscv64.tar.gz vi ~/.bashrc # Go environment export GOROOT=/usr/local/go export PATH=$GOROOT/bin:$PATH export GOPATH=$HOME/go export PATH=$GOPATH/bin:$PATH export GOPROXY=https://goproxy.cn,direct # source ~/.bashrc go version # 将另一个版本放到另一个目录下 mkdir -p /opt tar -C /opt -xzf go1.22.9.linux-riscv64.tar.gz mv /opt/go /opt/go1.22.9 vi ~/.bashrc # 在最后添加转换脚本 usego() { if [ \u0026#34;$1\u0026#34; = \u0026#34;1.23.6\u0026#34; ]; then export GOROOT=/usr/local/go elif [ \u0026#34;$1\u0026#34; = \u0026#34;1.22\u0026#34; ]; then export GOROOT=/opt/go1.22.9 else echo \u0026#34;Usage: usego [1.23.6|1.22]\u0026#34; return 1 fi export PATH=$GOROOT/bin:$PATH echo \u0026#34;Switched to Go version: $(go version)\u0026#34; } # source ~/.bashrc 之后我们可以执行usego 1.23.6和usego 1.22来回切换开发板上的Golang版本了。\n执行构建./script/download\nDownload遇到网络问题 如果开发板上遇到网络问题（即使我添加了代理也无法解决），我从 gpt 上得到的建议是可以在主机上进行 download 操作，因为这一步只是下载所需的各种依赖，并不是编译过程，所以不太涉及到架构问题。\n只需要在 download 前引入 riscv64 的临时环境变量，让其安装的依赖适合 riscv64。\nexport GOARCH=riscv64 export ARCH=riscv64 export OS=linux ./scripts/download # 然后拷贝build目录到开发板的k3s源码上 scp build/ root@ip:/root/k3s 这样就可以免于在开发板上进行直接 download，从而避免长时间的等待。\n探究为什么可以成功 download 这里我们列出其脚本内容，不算太多可以分析一下。分析我都写在每行命令的后面方便查看。\n#!/bin/bash # 指定以bash启动脚本 set -ex # 设置如果任何命令返回非零退出状态，立即退出；打印执行的每个命令及其参数，便于调试 cd $(dirname $0)/.. # 切换到脚本所在目录的父目录 . ./scripts/version.sh # 执行version.sh脚本，里面定义了版本信息 CHARTS_URL=https://k3s.io/k3s-charts/assets # 定义各种URL和目录路径变量 CHARTS_DIR=build/static/charts RUNC_DIR=build/src/github.com/opencontainers/runc CONTAINERD_DIR=build/src/github.com/containerd/containerd HCSSHIM_DIR=build/src/github.com/microsoft/hcsshim DATA_DIR=build/data export TZ=UTC umask 022 # 设置新创建文件的默认权限（755目录/644文件） rm -rf ${CHARTS_DIR} # 删除之前的目录并创建空的新目录 rm -rf ${RUNC_DIR} rm -rf ${CONTAINERD_DIR} rm -rf ${HCSSHIM_DIR} mkdir -p ${CHARTS_DIR} mkdir -p ${DATA_DIR} case ${OS} in # 如果系统是linux就会下载指定版本，指定架构的runc(底层容器运行时，负责运行 Pod 中的容器)以及k3s-root并解压 linux) git clone --single-branch --branch=${VERSION_RUNC} --depth=1 https://github.com/k3s-io/runc ${RUNC_DIR} curl --compressed -sfL https://github.com/k3s-io/k3s-root/releases/download/${VERSION_ROOT}/k3s-root-${ARCH}.tar | tar xf - ;; windows) # 如果是windows git clone --single-branch --branch=${VERSION_HCSSHIM} --depth=1 https://github.com/microsoft/hcsshim ${HCSSHIM_DIR} ;; *) # 如果是其他系统就会直接输出错误信息并退出 echo \u0026#34;[ERROR] unrecognized operating system: ${OS}\u0026#34; exit 1 ;; esac # 注意这里case in的用法 git clone --single-branch --branch=${VERSION_CONTAINERD} --depth=1 https://${PKG_CONTAINERD_K3S} ${CONTAINERD_DIR} # 克隆指定版本的containerd（容器运行时）仓库，使用depth=1的浅克隆只获取最新提交 # 从K3s charts仓库下载每个chart文件到构建目录 for CHART_FILE in $(grep -rlF HelmChart manifests/ | xargs yq eval --no-doc .spec.chart | xargs -n1 basename); do CHART_NAME=$(echo $CHART_FILE | grep -oE \u0026#39;^(-*[a-z])+\u0026#39;) curl -sfL ${CHARTS_URL}/${CHART_NAME}/${CHART_FILE} -o ${CHARTS_DIR}/${CHART_FILE} done 从这个 build 脚本可以看出其下载了以下东西:\nrunc+k3s-root(linux) / hcsshim(windows) containerd(底层是runc) HelmChart(k8s中类似于dnf的包管理器) 并最终将这些内容放在 build 目录下，其中会产生 src, static, data目录。\n可以发现，这里的 k3s-root-riscv64是关键，我在 k3s 相关的 issue 中找到了有一位开发者提供了对其的 commit 并得到了 k3s-root 的接受。\nbuild 接下来我们分层次介绍一下build脚本。\n#!/bin/bash set -e -x cd $(dirname $0)/.. . ./scripts/version.sh GO=${GO-go} # 默认使用 `go` 命令，但允许通过环境变量覆盖 PKG=\u0026#34;github.com/k3s-io/k3s\u0026#34; PKG_CONTAINERD=\u0026#34;github.com/containerd/containerd\u0026#34; PKG_CRICTL=\u0026#34;github.com/kubernetes-sigs/cri-tools/pkg\u0026#34; PKG_K8S_BASE=\u0026#34;k8s.io/component-base\u0026#34; PKG_K8S_CLIENT=\u0026#34;k8s.io/client-go/pkg\u0026#34; PKG_CNI_PLUGINS=\u0026#34;github.com/containernetworking/plugins\u0026#34; PKG_KUBE_ROUTER=\u0026#34;github.com/cloudnativelabs/kube-router/v2\u0026#34; PKG_CRI_DOCKERD=\u0026#34;github.com/Mirantis/cri-dockerd\u0026#34; PKG_ETCD=\u0026#34;go.etcd.io/etcd\u0026#34; buildDate=$(date -u \u0026#39;+%Y-%m-%dT%H:%M:%SZ\u0026#39;) 前面相同，后面定义了各个依赖组件的 Go 模块路径，用于后续的版本信息注入。\nVERSIONFLAGS=\u0026#34; -X ${PKG}/pkg/version.Version=${VERSION} -X ${PKG}/pkg/version.GitCommit=${COMMIT:0:8} -X ${PKG}/pkg/version.UpstreamGolang=${VERSION_GOLANG} -X ${PKG_K8S_CLIENT}/version.gitVersion=${VERSION} -X ${PKG_K8S_CLIENT}/version.gitCommit=${COMMIT} -X ${PKG_K8S_CLIENT}/version.gitTreeState=${TREE_STATE} -X ${PKG_K8S_CLIENT}/version.buildDate=${buildDate} -X ${PKG_K8S_BASE}/version.gitVersion=${VERSION} -X ${PKG_K8S_BASE}/version.gitCommit=${COMMIT} -X ${PKG_K8S_BASE}/version.gitTreeState=${TREE_STATE} -X ${PKG_K8S_BASE}/version.buildDate=${buildDate} -X ${PKG_CRICTL}/version.Version=${VERSION_CRICTL} -X ${PKG_CONTAINERD}/version.Version=${VERSION_CONTAINERD} -X ${PKG_CONTAINERD}/version.Package=${PKG_CONTAINERD_K3S} -X ${PKG_CNI_PLUGINS}/pkg/utils/buildversion.BuildVersion=${VERSION_CNIPLUGINS} -X ${PKG_CNI_PLUGINS}/plugins/meta/flannel.Program=flannel -X ${PKG_CNI_PLUGINS}/plugins/meta/flannel.Version=${VERSION_FLANNEL_PLUGIN}+${VERSION_FLANNEL} -X ${PKG_CNI_PLUGINS}/plugins/meta/flannel.Commit=HEAD -X ${PKG_CNI_PLUGINS}/plugins/meta/flannel.buildDate=${buildDate} -X ${PKG_KUBE_ROUTER}/pkg/version.Version=${VERSION_KUBE_ROUTER} -X ${PKG_KUBE_ROUTER}/pkg/version.BuildDate=${buildDate} -X ${PKG_CRI_DOCKERD}/cmd/version.Version=${VERSION_CRI_DOCKERD} -X ${PKG_CRI_DOCKERD}/cmd/version.GitCommit=HEAD -X ${PKG_CRI_DOCKERD}/cmd/version.BuildTime=${buildDate} -X ${PKG_ETCD}/api/v3/version.GitSHA=HEAD \u0026#34; 版本信息注入——使用 -X 标志向 Go 二进制文件注入版本信息，包括k3s版本，Git提交哈希，组件版本等。\nif [ -n \u0026#34;${DEBUG}\u0026#34; ]; then GCFLAGS=\u0026#34;-N -l\u0026#34; else LDFLAGS=\u0026#34;-w -s\u0026#34; fi 在调试模式下禁用优化，便于调试\ncase ${OS} in linux) TAGS=\u0026#34;$TAGS apparmor seccomp\u0026#34; RUNC_TAGS=\u0026#34;$RUNC_TAGS apparmor seccomp\u0026#34; if [ \u0026#34;$STATIC_BUILD\u0026#34; == \u0026#34;true\u0026#34; ]; then STATIC=$\u0026#39;\\n-extldflags \\\u0026#39;-static -lm -ldl -lz -lpthread\\\u0026#39;\\n\u0026#39; TAGS=\u0026#34;static_build libsqlite3 $TAGS\u0026#34; RUNC_STATIC=\u0026#34;static\u0026#34; fi if [ \u0026#34;$SELINUX\u0026#34; = \u0026#34;true\u0026#34; ]; then TAGS=\u0026#34;$TAGS selinux\u0026#34; RUNC_TAGS=\u0026#34;$RUNC_TAGS selinux\u0026#34; fi ;; windows) TAGS=\u0026#34;$TAGS no_cri_dockerd\u0026#34; if [ \u0026#34;$STATIC_BUILD\u0026#34; == \u0026#34;true\u0026#34; ]; then STATIC=$\u0026#39;\\n-extldflags \\\u0026#39;-static -lpthread\\\u0026#39;\\n\u0026#39; TAGS=\u0026#34;static_build $TAGS\u0026#34; fi export CXX=\u0026#34;x86_64-w64-mingw32-g++\u0026#34; export CC=\u0026#34;x86_64-w64-mingw32-gcc\u0026#34; ;; *) echo \u0026#34;[ERROR] unrecognized opertaing system: ${OS}\u0026#34; exit 1 ;; esac 关于静态构建选项？\nif [ ${ARCH} = armv7l ] || [ ${ARCH} = arm ]; then export GOARCH=\u0026#34;arm\u0026#34; export GOARM=\u0026#34;7\u0026#34; fi if [ ${ARCH} = s390x ]; then export GOARCH=\u0026#34;s390x\u0026#34; fi 根据架构设置交叉编译的 GOARCH 变量\nk3s_binaries=( \u0026#34;bin/k3s-agent\u0026#34; \u0026#34;bin/k3s-server\u0026#34; \u0026#34;bin/k3s-token\u0026#34; ... ) containerd_binaries=( \u0026#34;bin/containerd-shim\u0026#34; \u0026#34;bin/containerd-shim-runc-v2\u0026#34; ... ) for i in \u0026#34;${k3s_binaries[@]}\u0026#34;; do if [ -f \u0026#34;$i${BINARY_POSTFIX}\u0026#34; ]; then rm -f \u0026#34;$i${BINARY_POSTFIX}\u0026#34; fi done 清理旧的 k3s 和 containerd 的二进制文件\nif [ ! -x ${INSTALLBIN}/cni${BINARY_POSTFIX} ]; then ( TMPDIR=$(mktemp -d) git clone --single-branch --depth=1 --branch=$VERSION_CNIPLUGINS https://github.com/rancher/plugins.git $WORKDIR cd $WORKDIR rm -rf plugins/meta/flannel git clone --single-branch --depth=1 --branch=$VERSION_FLANNEL_PLUGIN https://github.com/flannel-io/cni-plugin.git plugins/meta/flannel GO111MODULE=off GOPATH=$TMPDIR CGO_ENABLED=0 \u0026#34;${GO}\u0026#34; build -tags \u0026#34;$TAGS\u0026#34; -gcflags=\u0026#34;all=${GCFLAGS}\u0026#34; -ldflags \u0026#34;$VERSIONFLAGS $LDFLAGS $STATIC\u0026#34; -o $INSTALLBIN/cni${BINARY_POSTFIX} ) fi 克隆并构建 CNI 网络插件（有关于 flannel，后续会写文介绍）\necho Building k3s CGO_ENABLED=1 \u0026#34;${GO}\u0026#34; build $BLDFLAGS -tags \u0026#34;$TAGS\u0026#34; -buildvcs=false -gcflags=\u0026#34;all=${GCFLAGS}\u0026#34; -ldflags \u0026#34;$VERSIONFLAGS $LDFLAGS $STATIC\u0026#34; -o bin/k3s${BINARY_POSTFIX} ./cmd/server for i in \u0026#34;${k3s_binaries[@]}\u0026#34;; do ln -s \u0026#34;k3s${BINARY_POSTFIX}\u0026#34; \u0026#34;$i${BINARY_POSTFIX}\u0026#34; done 构建k3s主程序，Go 编译器 (go build) 编译 ./cmd/server(主入口程序) 目录下的代码，生成 bin/k3s 二进制文件。\n最后构建 containerd 和 runc;\n整个流程确保了 K3s 及其所有依赖组件能正确编译并打包成可执行文件。\npackage-cli 此脚本用来构建有关于k3s的相关命令。\n汇总 最终我们将得到的 k3s-riscv64 二进制文件放入 /usr/local/bin中并使用 systemd 来控制其状态。具体步骤如下:\nmv k3s-riscv64 k3s mv /path/to/k3s /usr/local/bin chmod +x /usr/local/bin/k3s # 编写k3s.service vi /etc/systemd/system/k3s.service # 内容如下，来自于k3s源码中的同样内容 [Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io After=network-online.target Wants=network-online.target [Service] Type=notify EnvironmentFile=-/etc/default/%N EnvironmentFile=-/etc/sysconfig/%N EnvironmentFile=-/etc/systemd/system/k3s.service.env ExecStartPre=/bin/sh -xc \u0026#39;! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service 2\u0026gt;/dev/null\u0026#39; ExecStart=/usr/local/bin/k3s server KillMode=process Delegate=yes # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s [Install] WantedBy=multi-user.target 之后重新加载配置并使用 systemctl 管理 k3s\nsystemctl daemon-reload systemctl enable k3s systemctl start k3s systemctl status k3s 启动成功，状态如下：\nk3s.service - Lightweight Kubernetes Loaded: loaded (/etc/systemd/system/k3s.service; disabled; preset: disabled) Drop-In: /etc/systemd/system/k3s.service.d └─override.conf Active: active (running) since Mon 2025-04-28 14:21:55 CST; 50s ago Docs: https://k3s.io 执行相关命令\nk3s kubectl get node NAME STATUS ROLES AGE VERSION openeuler-riscv64 Ready control-plane,master 40m v1.32.3+k3s-76c5c770-dirty 设置软链接，可以省去前面的 k3s 标识。\nln -sf /usr/local/bin/k3s /usr/local/bin/kubectl 最终将上述操作汇总为k3s-init.sh,与 k3s-riscv64 和 golang 的tar文件在同一目录下,执行\nchmod +x k3s-init.sh sudo ./k3s-init.sh ","permalink":"http://localhost:1313/posts/023k3sep06%E4%BB%8Eissues%E4%B8%8A%E5%BE%97%E5%88%B0%E7%9A%84%E5%8F%AF%E8%83%BD%E5%B0%9D%E8%AF%95/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在\u003ca href=\"https://www.bfsmlt.top/posts/020k3sep04%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/\"\u003eK3sEP04\u003c/a\u003e中最后的总结中我们提到，反思了一下我们的移植过程，开始从k3s的结构看起，再到k8s的书籍，再到重新看二者的架构设计不同点，再到对应的命令，再追踪到k3s的源码，最后再开发板上适配的时候解决出现的一系列问题，我们处理移植问题的思路首先是有问题的。\u003c/p\u003e\n\u003cp\u003e还是引用一句话，大致的意思是，“你所遇到的问题100%都在别人的身上发生过，这意味着只要google，就一定可以找到答案。”但是我在这其中难道没有google过吗？我也不清楚为什么我没有发现对应的issue，直到最近，我才在github中找到了k3s关于支持riscv的相关issue，这证明自己处理此类问题的方法很有问题。\u003c/p\u003e","title":"K3sEP06——从issues上得到的可能尝试"},{"content":"strconv的ParseInt,Atoi,map下的delete 开发板go配置\n","permalink":"http://localhost:1313/posts/022golangstrconv%E5%8C%85/","summary":"\u003cp\u003estrconv的ParseInt,Atoi,map下的delete\n开发板go配置\u003c/p\u003e","title":"022GolangStrconv包"},{"content":"引子 又是奇思乱想环节，今天来聊聊自己的消费观和阅读吧。起因在于来自我大学舍友几个月前的一句话“很符合你的消费习惯”，这句话我算是一直记在心里，直到最近关于显示器和键盘的购买上，我发现自己的消费观确实有些困扰自己，所以聊聊；\n关于阅读这个话题，其实更多的应该是休息时要做的事情，当我想要阅读的时候我发现其无法带来显著的刺激感，无法带来即时的快感，所以阅读的优先级在我这里逐渐的降低，我会先选择玩电子游戏，再选择看电视剧，最后可能才会落在阅读上。\n好了，那我来聊聊吧。\n消费观 我的一个消费习惯在大学时期属于既奔放又收敛，对于我觉得不重要的领域我几乎不会怎么消费，比如我不喜欢吃零食，我几乎没有购买过零食有关的东西，比如我不喜欢喝奶茶，我一学期可能才主动买少于5次的奶茶。但是如果遇到我非常感兴趣的领域，只要我有钱，甚至只要我的京东白条里面足够，我就会去花费，比如科技领域。\n当时大一的时候我买了大疆的OSMO POKECT1,DJI MINI1(无人机),还有一个控制手机的云台，都是一代。我当时对这些东西极度入迷，以为自己拿上他们就可以成为想象中的那个VLOGGER,开始记录起自己的生活，看了宣传视频或者一些博主的评测视频就立马入手。虽然我也拿他们记录了一些东西，可见我的b站，但是当热情褪去之后他们又都被我闲置了，所以他们真的值得我购买吗？现在看来是不值得的，至少不值得全部都买，买其中一个就足够消磨自己的热情了。\n由于这些东西都不算太贵，当时我的生活费是2k一个月，这在云南来说已经很高了，因为食堂不是很贵，一天的吃饭开销在30元左右，再去掉一些出去玩的钱，所以一个月还可以剩下800多。对于无人机我一看，好家伙，第一款3k价位的无人机，那些5，6k的我买不起，3k买来玩玩还是可以的嘛，一看京东白条还有12期免息，买！\n现在回想起来，妥妥的冲动消费，就是因为新奇，但没有仔细考虑过自己是否真的需要。倘若真的需要，还可以通过租的形式来满足自己的新奇感，但我这个人对自己一直比较抠搜，总会想，那租一天也要多少钱，我租个一周的话还不如我买一个呢。以这种心态去对待租器械这个事情，现在看来有种洗脑自己购买的感觉。\n于是我很早就背上了京东的白条，不过还好的一点是，我对于超过我能力范围太大的东西从来都没有进行购买，比如4k往上的东西我从来没有超额透支过。还有一点是衣服与鞋子的购买，我在高中时期就很喜欢买鞋，不过都没有超过800我记得，由于家庭环境单亲的特殊性，我都是给我父亲说我想要这个鞋，过几天他就会给我买到家里，导致我被母亲经常称为蜈蚣。\n到了大学，自己第一次可以支配穿什么的时候，关注了一些穿搭博主，但我印象比较深刻的是当时双11还是618，我看阿迪的折扣比较大，一口气买了将近5，600的东西，我记得里面还有一双Ultra Boost，又满足了一把自己的虚荣心，那可是我高中一直想买却买不了的，现在居然这么便宜就到手了,没想到过了两周由于穿的不适合就闲鱼上出掉了。现在想想，当时的自己根本不是从需求的角度出发去购买的，而是从折扣优惠力度大的角度出发，觉得自己买到就是赚到，但真的赚到吗？最后消费的还是自己的金钱，但又不是自己的刚需。\n还有一件我印象比较深的事情，当时大三的时候我的电脑坏了，是一台大一的时候父亲主张买的华为轻薄本4000元左右，于是我向父亲要钱买电脑，当时没想到父亲如此的大方，一下子给我打了1w块钱让我买电脑，说买个好的能用的久的不容易坏的。当时给我激动坏了，我立马叫上哥们去了苹果的官方店（昆明有线下的官方店），直接激情下单了一台MacBookAir M1 8+256最小杯，当时是7999。\n同时发生了一个让我现在觉得自己都很蠢的事情，当时学生优惠是可以减800元的，但是我没有带身份证（我以为学生证就可以了）结果由于外面下雨加上自己的虚荣心作祟，直接说不用了，直接买吧。是不是非常可笑？其实蛮简单的解决方法，一个闪送过来就好了，那可是800块啊！现在我写下这些话的时候我都觉得自己十分可笑。\n还没结束，为什么当时要买mac呢？因为我当时出于虚荣心买了二手的iphone，二手的aw,再加上之前买的ipad和送的耳机，就差mac了，就可以集齐我的苹果全家桶了！不过我丝毫没有考虑过，苹果全家桶对我来说意味着什么。什么作用也没有啊，除了满足自己的虚荣心还有什么作用呢？买之前根本没有仔细了解过mac到底用来干什么，只看了一些评测视频说M1芯片非常好，无敌，殊不知仅仅过了四个月M2芯片就出了。🤣\n更离谱的是，由于当时本科读的材料专业，有一些专业性的软件mac用不了，以及自己想玩游戏的冲动即使当时在考研时期，我跟着同班同学买了一台二手的和他一样的天选3，又花了4k。现在想起来我真的无语住了，整整1.2w的购买资金，买一台联想拯救者是不是都绰绰有余了？或者你实在想买mac买一台满血版至少16G内存的Air是不是也是绰绰有余？再或者你想模仿别人，你学舍友配上一台主机，我都不敢想1.2w能配多么好的主机。\n哦，忘了说，之后我的那台二手天选3出了问题直接被我出掉了，2k。🤣\n回顾完我本科时期的购物经历，是不是觉得我非常的逆天，对，我现在也觉得当时我很逆天。\n回到现在，其实现在的我不逆天吗？有点，但是没有本科那么逆天。前几天我买了一个矮轴键盘，专门为我的mac配的键盘（之前的mac我送给我姐了，8G的内存实在用不下去，换了一台16G的m2,然后扩容了2T，准备用到老了），买这个键盘的时候我进行了不断调查，对比了4款类似的产品，最后买了loffee的小顺青春版，国补的加持下300元。到这里事情并没有结束，我研一的时候买过一款苹果官方的妙控键盘，当时花了我600元。（因为我的手是汗手，非常容易出汗，我的原生键盘已经被我打的很油了，所以键盘确实是我的刚需）所以当我看着我当时买的妙控键盘的时候好奇当初为什么没有选择其他更便宜，手感更好的键盘，因为妙控键盘用久了发现手感真的不适合自己，我是习惯按压力度比较大的。不会到头来这个妙控键盘又要在闲鱼出掉吧🤣\n好在现在我的京东白条已经全部还完了，以后再也不会使用白条来进行超前消费了。\n好啦，说了这么多，给自己立一个flag吧，虽然我专门立的flag从来没有成功过，所以也算是对自己的一个期许。\n以后的消费除了日常的吃行之外，都需要进行多方面的衡量，不要给自己凭空创造需求，买之前还是得多问问自己，真的需要吗？比如一个32G内存的mac自己真的需要吗？你能用到32G吗？16G都戳戳有余了吧——一个理科生怎么能不看参数不看一堆评测视频就进行购物呢哈哈哈，真是令我自己都匪夷所思； 要学会退货，不要说买了不合适就不退了，就将就着用，没有将就，自己没有这么有钱； 对于已经买了的无法退的东西可以将就，要做到长期主义，比如我现在手上的这款macbook 16+2T的，从2024-1-17日至今已经一年有余，我准备用至少五年，让我自己拭目以待吧。 哦对了我的aw也已经2年多了，现在电池健康度都7开头了，但是撑一天完全够了，我看我什么时候把它用到撑不到一天。 休息与阅读 来到这周想聊的第二个话题，休息与阅读。我发现自从我实习结束之后我就特别在意周末的休息，或者说有两天的休息。之前我都是大干百天，一下子就是一周都在工位学习，但不知道为什么这次实习后我觉得我还是得慢下来留两天休息的时间，打游戏，洗衣服，收拾收拾宿舍，以及写写文字来与自己对话。\n但是我发现自己缺少了很重要的一环就是阅读，在去年的这个时候我已经在Ipad上读完了秦腔这本书，而现在的我有了二手的Kindle却没有读完一本书？一本繁花我愣是没有看进去，看了半小时觉得没有多巴胺的刺激于是放弃了。但我却能看完遮天这样的网络小说，正是每一章能带来直接的刺激吸引着我读完。如果做个比喻，网络小说就像是抖音，不断地给予多巴胺的刺激，而文学书籍就像是B站，诸多的长视频，并不能一下子给我带来刺激。\n面对电视剧，游戏，长篇文学小说好像确实无法比拟二者，对我来说直觉上更倾向于前两者。但是，我觉得这样的自己是浮躁的，是急功近利的，是想要利用每一分每一秒的，是没有停下来思考的。\n就像我自己玩星露谷的时候，常常玩着玩着就变成了榨干每一分钟，晚上直到十二点甚至一点才回家睡觉，第二天又急匆匆地出去。当我意识到这一点时，我告诉自己，这只是一个单机的种田游戏，你没有必要这样真分夺秒，现实世界中争分夺秒还不够吗？来到虚拟世界就该随心所欲，让自己慢下来，享受游戏，该十点多结束一天就结束，没有必要弄的那么晚。\n所以我觉得自己虽然嘴上说的自己不焦虑，躺平了，但是从行动来看，根本没有，甚至一点减少的痕迹都没有。所以我刚刚卸载了抖音，想了想自己平时看抖音看什么，不就看一些擦边视频来取悦自己吗？不就看一些搞笑视频来获得那几秒钟的快乐吗？那之后呢？想了想自己平时看电视剧，很难做到一集50min的韩剧的中间不拿起自己的手机，为什么不能专注于眼前的这一件事呢？刷手机到底能带来什么呢？什么都没有。\n写到这里我又不得不引出今天的第三个话题了，关于刷手机。我反思了一下，由于自己是学生，不需要那些办公的软件比如飞书钉钉这些，我每天拿手机看什么？小红书，抖音，b站，虎扑，小黑盒，微信（好像平时也没人给我怎么发消息）没有了，真的就没有了。我也不玩手游，我觉得手游根本没意思，不如电脑玩，即使崩铁原神甚至金铲铲我都倾向于用电脑玩，手机屏幕太小了。\n诶，这么一看，我对这手机的要求也不高哈，拍照？我本身就是一个不注重于拍照效果的人。这么想来我可能唯一在乎的是高刷吧，毕竟已经用习惯了（但是我的mac还是60HZ啊）那我是不是用一个2k的手机就可以了？根本没有必要使用现在的旗舰机啊，从我的需求来看，它们其实已经对我来说过饱和了。所以我再立一个flag，当我手上的手机用不动的时候（我的期限还是五年）下一部手机我会选择品牌的2k-3k间的手机。\n跑题了，拉回来。急功近利不可取，慢下来，需要让自己慢下来。干一件事情的时候就专心干一件事，把每一件事情做好，独立的一件又一件事。不要多线程，这样只会什么都干不好。\n总结 好啦，这就是这周的奇思乱想了，又是与自己对话，写到这里我畅快了许多，觉得找回了自己的一些精气神，借着梁博的新专辑名称，这周的主题我也叫做精气神吧！\n我始终认为，精神上的强大与自足才是真正的强大。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/021%E5%A5%87%E6%80%9D%E4%B9%B1%E6%83%B3ep02%E6%B6%88%E8%B4%B9%E8%A7%82%E4%B8%8E%E9%98%85%E8%AF%BB/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e又是奇思乱想环节，今天来聊聊自己的消费观和阅读吧。起因在于来自我大学舍友几个月前的一句话“很符合你的消费习惯”，这句话我算是一直记在心里，直到最近关于显示器和键盘的购买上，我发现自己的消费观确实有些困扰自己，所以聊聊；\u003c/p\u003e","title":"LT的奇思乱想EP02精气神——消费观与阅读"},{"content":"引子 接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。\nkubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default redis-696579c6c8-v2wns 1/1 Running 4 (4m10s ago) 7d6h 10.42.0.17 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system helm-install-traefik-pv4hv 0/1 ImagePullBackOff 0 14d 10.42.0.20 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system local-path-provisioner-7b7dc8d6f5-48jjf 0/1 ImagePullBackOff 0 14d 10.42.0.19 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system helm-install-traefik-crd-ktfth 0/1 ImagePullBackOff 0 14d 10.42.0.21 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-668d979685-jthzj 0/1 ImagePullBackOff 0 14d 10.42.0.22 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-5f8bb7cf9f-5h5kj 0/1 Running 0 11s 10.42.0.36 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running， 只是还存在网络问题没有解决,所以并没有 Ready.\n所以这次我们来解决其他 local-path-provisioner 的问题。\n什么是local-path-provisioner 官方的介绍是:Dynamically provisioning persistent local storage with Kubernetes\n众所周知，如果我们起一个 Pod，里面的容器一定是需要挂载外部的存储目录的，否则容器一旦销毁那么相关的数据也不在了，无法做到持久化存储，而挂载外部存储目录本质上要求 PVC——持久化卷声明，举例说明：\nvolumeMounts: - mountPath: /data name: redis-storage volumes: - name: redis-storage persistentVolumeClaim: claimName: redis-data 这里我们声明了具体的 pvc persistentVolumeClaim:claimName: redis-data,这是一个真正的磁盘路径，见下方 PVC 的声明写法。\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: redis-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 一旦写了 PVC，local-path-provisioner 就会起作用：它自动为你的 PVC 创建一个目录, 如 /opt/local-path-provisioner/pvc-xxx/,将此目录映射给容器作为持久化的目录。 所以，这个镜像的修复是至关重要的一步，没有它我们就无法进行数据的持久化。\n修复问题 与CoreDNS不同的是，这次在官方的 release 界面并没有发现其针对于riscv架构的二进制文件，所以我们得尝试自己构建了。\n(后续更新,发现其已经支持了 RiscV 架构,只是没有跑通流水线部署,但是代码中已经 Commit 了,所以可以直接使用了)\n拉取到其源代码之后在我的 mac 本机进行构建\n1.构建自定义镜像 git clone git@github.com:rancher/local-path-provisioner.git cd local-path-provisioner GOOS=linux GOARCH=riscv64 CGO_ENABLED=0 go build -o local-path-provisioner 构建成功，这证明其中并没有不适配riscv的地方（例如coredns中构建时出现过的syscall不支持问题）得到一个local-path-provisioner的二进制文件，我们可以将这个二进制文件封装在一个Dockerfile中构建其自定义的镜像来代替默认的镜像。\ndockerfile如下:注意拷贝的路径选择为/usr/bin\nFROM alpine:latest # 使用阿里源（可选） RUN sed -i \u0026#39;s#https\\?://dl-cdn.alpinelinux.org/alpine#http://mirrors.aliyun.com/alpine#g\u0026#39; /etc/apk/repositories # 拷贝编译好的可执行文件 COPY local-path-provisioner-riscv64 /usr/bin/local-path-provisioner # 启动命令 ENTRYPOINT [\u0026#34;local-path-provisioner\u0026#34;] 如果不使用/usr/bin的话会遇到问题:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 79s default-scheduler Successfully assigned kube-system/local-path-provisioner-5cdfcc7d9c-hpqnm to openeuler-riscv64 Normal Pulled 35s (x4 over 79s) kubelet Container image \u0026#34;ltx/local-path-provisioner:v0.0.21\u0026#34; already present on machine Normal Created 35s (x4 over 79s) kubelet Created container local-path-provisioner Warning Failed 35s (x4 over 79s) kubelet Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \u0026#34;local-path-provisioner\u0026#34;: executable file not found in $PATH: unknown Warning BackOff 9s (x7 over 78s) kubelet Back-off restarting failed container 构建镜像并推送至本地的私有镜像仓库。\ndocker build --network host --platform=linux/riscv64 -f Dockerfile.local-path-provisioner-riscv64 -t 192.168.173.76:6000/riscv64/local-path-provisioner:1.1 . docker push 192.168.173.76:6000/riscv64/local-path-provisioner:1.1 2.拉取镜像修改配置文件 在 riscv 开发板进行拉取镜像并打标签至默认 local-storage 中指明的镜像名称。\ncrictl pull riscv64/local-path-provisioner:1.1 ctr -n k8s.io images tag docker.io/riscv64/local-path-provisioner:1.1 docker.io/rancher/local-path-provisioner:v.0.0.21 可以使用 kubectl -n kube-system edit deployment local-path-provisioner 检查其Deployment文件;\n或者与coreDNS相同,进入 /var/lib/rancher/k3s/server/manifests 目录下查看其 local-storage.yaml 文件. 这是local-path-provisioner的部署文件。\n如果未能识别到本地镜像，根据这个回答，我们只要修改前缀不是rancher的就可以识别到本地的镜像\n接下来与 coreDNS 相同，这是系统级别的镜像，为了防止每次重启都被覆盖，需要在k3s启动命令中添加--disable标签. 因为我们使用systemctl来管理k3s，所以需要修改/etc/systemd/system/k3s.service\n在此之前我们要先拷贝一份manifests目录下的yaml文件，修改为我们自定义的custom-local-storage.yaml文件，让之后的k3s识别到这个文件进行容器的构建。\ncp local-storage.yaml custom-local-storage.yaml 由于我们之前已经把标签打成了与默认 yaml 文件中相同名称的镜像，所以这个 custom 文件中不需要修改什么东西。\n回到刚才，继续修改启动命令。\nvi /etc/systemd/system/k3s.service # 在ExecStart中添加 --disable ExecStart=/usr/local/bin/k3s \\ server \\ --disable=local-storage,coredns \\ 这一方法来源于官方文档，或者这篇文章，加上这个标签会使k3s在启动时选择我们设定的自定义yaml文件并且删除默认的yaml文件与构建的容器。\n你还可以选择文章中的方法二，在manifests目录下添加 local-storage.yaml.skip 文件，这样会让k3s跳过某个容器的构建，但我没有测试其是否会去选择我们自定义的配置文件。\n接下来进行删除，检测。最好重启一下k3s,systemctl restart k3s\nkubectl delete pod -n kube-system -l app=local-path-provisioner kubectl get pods -n kube-system -l app=local-path-provisioner kubectl describe pod \u0026lt;PodName\u0026gt; -n kube-system 总结 经历了 coreDNS 和 local-path-provisioner 这两个系统级别的镜像之后，我开始反思整个适配过程，我觉得应该先去看看如何从头适配，即使 OREV 的兴趣小组先进行了一个基础适配，使得我们可以先下载到 k3s。\ngoogle 后发现在官方的ISSUE中有这个适配问题,其中提到一个巨大挑战是rancher-mirrored 镜像中缺乏对 riscv 平台的支持。\n这也正是我们之前处理的那些镜像，默认是拉取的 rancher 官方维护的镜像副本 rancher-mirrored,我们也对这些镜像做了适配。(后来知道可以修改默认镜像仓库,在启动命令时加上私有的仓库即可)\nWhile we could simply add this to the arch list, this would change the multi-arch digest for any tags that we currently mirror that have this platform available.\n下面也提到虽然可以简单地将 riscv 添加到镜像的架构列表中，但会导致当前镜像的多架构 digest 发生变化。\n镜像的 digest 是镜像的唯一标识，一个哈希值 sha256:xxx 我们平时用的镜像名称，其实就是 tag，指向的是一个多架构的manifest,这个 manifest 会列出所有对应架构的镜像(见下方图像) manifest list 本身是一个 JSON 文件,他也有 digest 标识;如果添加一个 riscv 字段进去，整个 JSON 内容变化导致 digest 变化 很多生产系统,CI/CD 都依赖这个稳定可验证的 digest,这就导致牵一发而动全身, 会重新拉取镜像，打破缓存；某些系统会误以为内容被篡改或更新了，造成意外后果;不利于镜像的同步和管理 更愿意的做法是:不改变现有版本(及之前版本)的镜像 manifest,在新的版本中添加对新架构的支持. We should figure out how to add this platform to only new tags, in a way that is minimally disruptive to the list maintenance process, and can be reused when adding new platforms in the future.\n这是他们提出的解决方法，只将 RISC-V 平台添加到**新标签（新的 manifest,新的镜像）**中，这样对镜像列表的维护干扰最小。\n为了保证镜像的稳定性，K3s 团队不修改已有镜像的多架构配置，而是只给新添加的镜像构建 RISC-V 版本，靠 git blame + 时间区间控制，确保 digest 不变且流程可复用。\n但是后续的讨论就围绕着该如何支持新的架构，官方开发者更倾向于对新的镜像加入新的架构而不去改动老的镜像，这是很好理解的策略。\n接下来我们将探索其他系统级别镜像,例如 Helm,Traefik\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n后续更新 可以发现 rancher 官方提供了关于 local-path-provisioner 在 riscv 上的镜像.\n所以大佬在 commit 中只是修改了其版本到29.\n故我们只需要将其拉取到本地并推送到私有镜像仓库即可.\n","permalink":"http://localhost:1313/posts/020k3sep05%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekubectl get pods -A -o wide\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eNAMESPACE     NAME                                      READY   STATUS             RESTARTS        AGE    IP           NODE                NOMINATED NODE   READINESS GATES\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edefault       redis-696579c6c8-v2wns                    1/1     Running            \u003cspan class=\"m\"\u003e4\u003c/span\u003e \u003cspan class=\"o\"\u003e(\u003c/span\u003e4m10s ago\u003cspan class=\"o\"\u003e)\u003c/span\u003e   7d6h   10.42.0.17   openeuler-riscv64   \u0026lt;none\u0026gt;           \u0026lt;none\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekube-system   helm-install-traefik-pv4hv                0/1     ImagePullBackOff   \u003cspan class=\"m\"\u003e0\u003c/span\u003e               14d    10.42.0.20   openeuler-riscv64   \u0026lt;none\u0026gt;           \u0026lt;none\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekube-system   local-path-provisioner-7b7dc8d6f5-48jjf   0/1     ImagePullBackOff   \u003cspan class=\"m\"\u003e0\u003c/span\u003e               14d    10.42.0.19   openeuler-riscv64   \u0026lt;none\u0026gt;           \u0026lt;none\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekube-system   helm-install-traefik-crd-ktfth            0/1     ImagePullBackOff   \u003cspan class=\"m\"\u003e0\u003c/span\u003e               14d    10.42.0.21   openeuler-riscv64   \u0026lt;none\u0026gt;           \u0026lt;none\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekube-system   metrics-server-668d979685-jthzj           0/1     ImagePullBackOff   \u003cspan class=\"m\"\u003e0\u003c/span\u003e               14d    10.42.0.22   openeuler-riscv64   \u0026lt;none\u0026gt;           \u0026lt;none\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekube-system   coredns-5f8bb7cf9f-5h5kj                  0/1     Running            \u003cspan class=\"m\"\u003e0\u003c/span\u003e               11s    10.42.0.36   openeuler-riscv64   \u0026lt;none\u0026gt;           \u0026lt;none\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running，\n只是还存在网络问题没有解决,所以并没有 Ready.\u003c/p\u003e","title":"K3sEP05——开发板上k3s存在的问题02_local-path-provisioner"},{"content":"引子 在做力扣的题目时，特别是遇到递归问题，需要传入参数，遇到切片的时候经常遇到需要克隆切片的情况，而遇到普通的int类型的时候却不需要。\n显而易见，这是一个值传递还是引用传递的问题，所以今天我来总结一下 Golang 中切片的底层原理，为什么在递归中需要克隆它。\n同时，在百度的一面中我也遇到了相关的切片题目，具体如下:\nfunc main() { original := []int{1, 2, 3} copied := original[:2] copied = append(copied, 4) original = append(original, 5) copied = append(copied, 6) original = append(original, 7) fmt. Printin(\u0026#34;original===\u0026#34;,original) fmt.Printin(\u0026#34;copied===\u0026#34;copied) } 在这里你可以按下暂停键，考虑一下此时两个切片的输出会是怎样的。带着你的答案与疑惑，继续看下去吧。\n切片 这里先简单区分一下数组与切片。\nvar res [3]int var res []int 这里第一个指定长度的是数组，而第二个没有指定的是切片；前者是值类型，后者是引用类型可以使用append操作。\n简单一句话，为了摆脱数组固定长度带来的束缚，我们使用更为灵活的切片slice。\n切片底层 结构与扩容机制 首先总结一句话：切片 a 本身是一个结构体，a 代表底层数组地址（结构体里的 Data 字段），\u0026amp;a 才是结构体变量 a 本身在内存中的地址。\ntype slice struct { array unsafe.Pointer // 指向底层数组的指针 len int // 切片的长度 cap int // 切片的容量 } 其扩容机制如下：\n当对切片进行 append 操作时，如果切片的长度小于其容量，Go 会直接在原底层数组的空间内添加元素，并更新切片的长度。 如果切片的长度等于或超过其容量，Go 会创建一个新的底层数组（通常是原来容量的 2 倍），将原数组的内容复制到新的数组中，并返回新的切片结构体。切片中的指针、长度和容量都会更新为新的数组和容量。 需要注意的是，为什么说切片是引用类型：切片本身是一个结构体（值类型），但它内部的指针指向底层数组。因为它内部包含了指针，所以切片被认为是引用类型。\n其在堆栈上的表现如下：栈中保存结构体，堆中保存底层数组。\na := []int{10, 20, 30} 栈区： +----------------------+ ← \u0026amp;a = 0xc00000e030 | 切片结构体 a | | Data: 0xc0000140a0 | →→→→ 指向堆上的底层数组 | Len: 3 | | Cap: 3 | +----------------------+ 堆区： +----------------------+ | 10 | 20 | 30 | ← a.Data = \u0026amp;a[0] = 0xc0000140a0 +----------------------+ append扩容的注意点 append是给原切片的当前长度的下一个位置添加元素，其扩容机制如上所述，但是需要注意的是\nnil切片在append时会被当作len=0, cap=0的切片处理，所以如果对nil切片进行append其会自动为你分配一个新的底层数组，即指向底层数组的指针会发生改变。 func main() { var res []int fmt.Printf(\u0026#34;res结构体的地址是 %p, res指向底层数组的地址是 %p\u0026#34;, \u0026amp;res, res) res = append(res, 1) fmt.Printf(\u0026#34;res结构体的地址 %p, res指向底层数组的地址是 %p\u0026#34;, \u0026amp;res, res) } append每次会生成一个新的结构体值，但是其变量本身在栈上的位置不变，但是指向底层数组的指针如果不扩容就不变。\npath = append(path, 1)可以验证\u0026amp;path不会发生改变——放到递归中就是每一层的path都是同一个path，其\u0026amp;path不变(即使底层数组变化\u0026amp;path也不变) 这意味着每次append之后返回的新的结构体中三个变量：底层数组指针，长度，容量；只有长度是一定在变化的，其他两者要看是否发生了扩容。 值得注意的是，在递归回溯类问题中父递归会根据长度来判断当前切片中的值有哪些——即使底层数组不变，但是长度发生了变化。这一点在后面常见问题有所体现。 扩容机制带来的坑（必须len\u0026gt;=cap去扩容）\ns1 := []int{1, 2, 3} s2 := s1[1:2] s3 := append(s2, 10) 可以思考这段代码三者的结果是什么，看似没有修改s1,其实其值已经被修改了；这也是后面回溯的题目我们为什么要克隆path的原因。\nappend(arr, arr \u0026hellip;) 这里的省略号代表着什么？回到 append 的最初底层定义func append(slice []Type, elems ...Type)\n第二个参数是可变参数，但是类型需要与 slice 切片中元素的类型相同\narr := []int{1, 2, 3} newArr := append(arr, arr...) // 如果不写...则会报错，因为后面的arr是切片，而前面的Type是int fmt.Println(newArr) // 输出: [1 2 3 1 2 3] 这样写的含义是使用 \u0026hellip; 展开切片 arr， 从而等效于append(arr, 1, 2, 3)。\n省略号仅适用于展开切片和表示可变参数。\nlen与cap 长度和容量作为Golang切片中的核心概念，通过尝试下面这段代码，我们可以略窥一二。\ns1 := make([]int, 3, 6) s2 = s1[1:3] s1[1] = 1 s2 = append(s2, 2) 如果此时打印s1与s2的内容，会是什么呢？请先思考。\n常犯的错误是:第三句执行完之后其底层数组是[0,1,0],然后append时没有发生扩容，所以二者的底层数组是相同的，打印出的二者应该都是[0,0,2]，但结果并不是，s1仍是[0,1,0],s2是[1,0,2]，所以为什么？\n请先记住这个概念:cap(slice) = 原始数组末尾 - slice 的起始位置\n起初s1长度为3，容量为6，底层数组前面3个值为0，后面3个未使用；当s2 = s1[1:3]时，s2切片的底层数组指针指向的是第二个位置，这就导致其长度为2,容量为5\n在修改s[1] = 1时，底层数组共享，二者都能看到这个修改（如果修改的是s[0]=1呢？可以想想，结果很明显s2看不到，因为其起始位置是第二个位置）\n最后append(s2, 2)时，并不会发生扩容，但是s2的长度要+1，二者也都能看到，第四个位置变为2.\n具体如图所示：\n接下来，如果我们继续向s2中append三个数，3，4，5，后果是什么呢？这里就涉及到append的扩容机制了，我们之前说过。\n当3，4添加之后，s2的长度已经来到了5，注意，此时其len=cap了！要发生扩容了！但是对于s1呢？仍然保持着cap=6\u0026gt;len=5,所以不扩容。\n最后二者分道扬镳，不再共享底层数组了（指针指向两个不同的地址），s2的容量也会扩大两倍来到10.\n切片的初始化 看完上面的长度与容量后，我们借此来讨论一下切片的初始化。\nmake初始化 对于 make 一般有以下三种不同的初始化，分别是:声明长度为0；声明指定长度；声明指定容量（当然既声明长度又声明容量也是可行的）\na := make([]int, 0) b := make([]int, n) c := make([]int, 0, n) 看到b有些亲切，我们平时做题时有时声明path就这样来写的。这三种写法有什么区别呢？\n显而易见，a的底层数组并没有开辟（长度为0），假如我们每次只append一个值，当我们进行多次append操作时，a会发生多次的扩容，这么多次的扩容会导致什么问题呢——原来旧的底层数组没有人用了，会引起GC的垃圾回收，从而影响性能。（关于GC的事，我们挖个坑后续补充） 而bc的底层已经开辟好了空间，有一点不同是，添加元素时需要对b进行b[i]=x这样的添加，对于c需要进行c = append(c, x)——因为 append 会给切片的末尾去添加元素。 带来的区别就是，遇到向切片中添加元素的情况时，b和c基本不会发生扩容，a在不断地扩容，性能大打折扣。 b和c对比呢？大家可以猜猜更倾向于谁 在性能上，b要优于c一些，仅仅是a little,因为c要不断地去append；但是，考虑到代码的书写方便，我们还是更喜欢c的append，因为b[i] = x这个在某些复杂的情况下得计算，不是简单的顺序。\n但是话又说回来，a就一定不好吗？虽然其会发生许多次的扩容，但是如果我们事先并不清楚该设置多少的长度或者容量，反而a在扩容次数较少的情况下会更好。\n那么这又是一个CPU和memory的二选一问题了。\n但是我们仍然建议你再声明任何切片的时候如果可以预知长度或者容量，就提前声明，即使你要遍历一遍传来的字符串等，因为如果不提前声明，扩容带来的性能损耗远高于遍历等操作。\n例如二维动态规划dp数组的声明常见写法\nn := len(text1) m := len(text2) dp := make([][]int, n+1) // 0~n n+1 for i := range dp { dp[i] = make([]int, m+1) } 如果没有后面这段for循环，我们只是创建了 n+1 个 nil 的行，直接访问 dp[0][0] 会直接 panic 数组越界。\n字面量初始化 s1 := []int{1, 2, 3, 4, 5} s2 := []int{} var s3 []int s1 是一种直接给值的初始化，长度=容量 s2 没有初始化，长度为0，容量为0 s3 与 s2 的区别是其只声明但未初始化，所以这是一个空切片 nil 从现有数组获得切片 这就是我们常见的“切割”操作，a[1:3]，代表着取a[1],a[2]这样一个左闭右开区间。\n不会创建新的底层数组，而是创建一个新的切片结构体 新结构体的指针指向自己指定的起始位置，例如a[1] 区分nil slice和empty slice 先说结论，nil slice一定是empty slice,但是empty slice不是nil slice.\nA nil slice equals nil, whereas an empty slice has a length of zero. A nil slice is empty, but an empty slice isn’t necessarily nil\n回到上面的初始化，其实最基础的是不是这几种写法\nvar s1[]string s2 := []string(nil) s3 := []string{} s4 = make([]string, 0) 对于1来说，是空指针，没有底层数组（平时做题自己上来就var res []int） 对于2来说，是一个语法糖，本质也是1 对于3来说，是空，但有底层数组，但不是nil 对于4，与3相同 但是，无论是 empty 还是 nil,对其进行 fmt.Println(s) 打印输出时，都会显示 []，这是 golang 设计的格式化约定。\n那么 empty 和 nil 这两个概念有什么区别呢？如果此时对二者进行 JSON 序列化，结果立马就会有所不同。\nvar s1 []float32 // nil customer1 := customer{ ID: \u0026#34;foo\u0026#34;, Operations: s1, } b, _ := json.Marshal(customer1) fmt.Println(string(b)) s2 := make([]float32, 0) // empty customer2 := customer{ ID: \u0026#34;bar\u0026#34;, Operations: s2, } b, _ = json.Marshal(customer2) fmt.Println(string(b)) // 结果如下 {\u0026#34;ID\u0026#34;:\u0026#34;foo\u0026#34;,\u0026#34;Operations\u0026#34;:null} {\u0026#34;ID\u0026#34;:\u0026#34;bar\u0026#34;,\u0026#34;Operations\u0026#34;:[]} 值得注意的是，append对两类切片的操作相同。\n为什么我们要分这么清呢？上面的JSON序列化已经告诉你了——对前端或接口定义要求严格的系统（比如 [] 是必须字段）就很关键\n未赋值nil 空数组[] 检查空 由此引申出一个问题，如何判断一个切片为空？是用x == nil吗？很明显不是，对于[]我们无法判断，所以我们应该从长度出发if len(x) == 0，完美地覆盖了两种情况。\ncopy的坑 开门见山：当你想把某个切片拷贝给另一个切片时，copy函数会选择两个切片中长度的较小值进行逐值拷贝。\n所以，如果我们还是像上面那样声明切片的话，很容易发生拷贝失效的问题（长度最小值为0了）\nsrc := []int{0, 1, 2} var dst []int copy(dst, src) fmt.Println(dst) 输出结果为 []， copy 不像 append 那样会进行扩容，所以发生了拷贝失效。\n所以还是劝自己写成声明长度或者容量的方式。\n做题中的问题 明白了切片的底层原理之后我们再来看看常见的问题，以下面的递归代码举例,本示例代码出自于力扣78子集问题，一道经典的回溯问题。\n回溯Clone func subsets(nums []int) [][]int { var res [][]int var path []int var dfs func(int) dfs = func(index int) { if index == len(nums) { res = append(res, slices.Clone(path)) return } // 不选 dfs(index + 1) // 选 path = append(path, nums[index]) dfs(index + 1) // 最后进行回溯 path = path[:len(path)-1] } dfs(0) return res } 需要注意的点在于这句res = append(res, slices.Clone(path))，可以发现每次我们收集结果的时候都进行了克隆——克隆的作用在于创建一个新的切片，复制path当前的内容。\n为什么要克隆？如果改成res = append(res, path)会怎么样？\n由于 path 切片在代码中作为全局变量，被传入闭包函数中会被全局共享，即每一层递归其实使用的是同一个path结构体，如果不克隆，每次收集的时候都收集到的是当前的path状态。\n而后面的递归过程中进行回溯操作，path 值发生变化，导致之前收集到的 res 受到影响。（我们默认不发生扩容）\n所以我们需要对其进行克隆，每次都保存一份当前那一刻的path的快照。\n何时回溯\u0026amp;为什么回溯 题外话，如果我们不使用闭包函数的写法，上面那段代码会是这样的。\nfunc subsets(nums []int) [][]int { var res [][]int dfs(nums, 0, []int{}, \u0026amp;res) return res } func dfs(nums []int, index int, path []int, res *[][]int) { if index == len(nums) { clone := slices.Clone(path) *res = append(*res, clone) return } // 不选当前元素 dfs(nums, index+1, path, res) // 选当前元素 path = append(path, nums[index]) dfs(nums, index+1, path, res) // 不需要回溯 } 疑问1:这里为什么要Clone？\n其实在本题的条件下，不需要Clone——因为这种函数写法是值传递，导致决策树每层递归的path都是独立的，不再是上一段中的全局变量了，我们即使后面在append修改，但修改的不是当前层的path；且收集结果只在根节点收集 那么在力扣上尝试一下是这样的 其实仔细看两个答案是一样的，只是力扣要求我们必须以他的答案顺序，那么为什么呢？在平时写代码时如果我们在后面修改了前面的元素，例如如果后面修改了path[0] = 1那么就会影响到我们已经收集好的结果。（默认没有发生扩容，底层数组共享） 总结：递归中虽然每层的 path 不同，但是仍可能共享着底层数组，发生修改后可能会影响已经收集过的结果，故为了代码的健壮性我们必须Clone 疑问2:既然每层的 path 切片不同，但是其底层数组相同，那为什么不需要回溯？\n值得注意的是，我们的 path 刚开始是空的，所以一旦发生 append 就一定会发生扩容(这一点在上面的 append 注意点中提到过)，而扩容后底层数组就会不一样—nil 切片在 append 时会被当作 len=0, cap=0的切片处理的情况。 而不需要回溯的原因在于每次我们都是值传递，即使底层数组共享，但是从子递归回到父递归时，还是父递归那一层的path状态——回到上层递归时，path会从下层的状态恢复到上层的状态，例如从[2]恢复到[],故自然不需要回溯，这样的写法自动给我们回溯了。 拿一个示例代码举例:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;slices\u0026#34; ) func main() { fmt.Println(subsets([]int{1, 2})) } func subsets(nums []int) [][]int { var res [][]int path := make([]int, 0, 10) // 初始容量为 10，不让他扩容 dfs(nums, 0, path, \u0026amp;res) return res } func dfs(nums []int, index int, path []int, res *[][]int) { fmt.Printf(\u0026#34;path: %p, \u0026amp;path: %p, len: %d, cap: %d\\n\u0026#34;, path, \u0026amp;path, len(path), cap(path)) if index == len(nums) { clone := slices.Clone(path) *res = append(*res, clone) return } // 不选当前元素 dfs(nums, index+1, path, res) // 选当前元素 fmt.Printf(\u0026#34;path: %p, \u0026amp;path: %p, len: %d, cap: %d\\n\u0026#34;, path, \u0026amp;path, len(path), cap(path)) path = append(path, nums[index]) fmt.Printf(\u0026#34;path: %p, \u0026amp;path: %p, len: %d, cap: %d\\n\u0026#34;, path, \u0026amp;path, len(path), cap(path)) dfs(nums, index+1, path, res) fmt.Printf(\u0026#34;path: %p, \u0026amp;path: %p, len: %d, cap: %d\\n\u0026#34;, path, \u0026amp;path, len(path), cap(path)) // 不需要回溯 } 关注第一次进入时的\u0026amp;path和最后一次的，发现二者是一样的，并且path的长度也恢复了，意味着path内的值也恢复了。\n这里也留了一个隐藏坑，具体见补充问题中。\n疑问3:这里为什么 res *[][]int 呢？\n因为函数的值传递，每次传入的res如果不加这个*就会导致每次传的都是一个副本，一个拷贝，即函数里面的res和你外面传过来的res不是一个切片结构体，那么在返回的时候自然无法获得正确结果——需要加上*进行 补充问题：为什么底层数组是一样的，但是path中保存的值却不一样呢？\n其实这也是一个非常关键的问题，涉及到了Golang对切片的设计哲学——数据共享，但视图独立。 因为我们底层数组一样，Golang的切片使用len变量来保证不同层级的视图独立——回到父递归时，由于其len还是0，所以我们的父递归path就只能看得到这个len=0的切片了，而不再是子递归中的len=1的切片。 对比两种写法可以发现，闭包中使用的path是全局变量，需要回溯，需要克隆；而独立声明函数的写法中的path对于每层（决策树）递归都是一个独立的切片（值传递的缘故）即使底层数组可能发生共享，但是其依靠len变量进行层级之间的分隔，使其做到每层递归都是独立的。\n一句话总结：切片是结构体，传参传结构；结构中有指针，共享底层\n类比问题 上面两种写法一种使用全局变量+闭包的写法，很明显必须回溯必须Clone；一种采用独立声明函数的方式。如果我再来一种完全闭包函数的写法呢？\nfunc subsets(nums []int) [][]int { var res [][]int // var path []int var dfs func(int, []int) dfs = func(index int, path []int) { // 对于每个元素都会选或者不选，这会形成一颗决策二叉树，走到叶子节点收集结果即可。 if index == len(nums) { res = append(res, slices.Clone(path)) return } // 不选 dfs(index + 1, path) // 选 path = append(path, nums[index]) dfs(index + 1, path) // 最后进行回溯,其实不需要回溯，原因同上，递归过程中每一层path独立根据len进行分割视图。 // path = path[:len(path)-1] } dfs(0, []int{}) return res } 也不需要回溯，原因同上。\n还有一种写法，在append上做出了一个小修改dfs(index+1, append(path, nums[index]))——把两步结合在了一步。\n我们把append结果直接传到了dfs参数中，并没有多写path = append(path, nums[index])。有什么不同呢？\n后者直接传递了一个新的切片结构体进去，子递归发生的修改不会影响父递归的path(这是这种写法的优点，父path还是len=0) 前者先append后，父递归的path已经是修改过的了，通过dfs再传入的是修改过的path； 总结递归回溯写法 虽然Golang的切片特性以及值传递的特点可以帮组我们忽略回溯的编写，但是一旦使用全局变量就一定得回溯。\n而使用全局变量这一点就类似于Java中的List,所以为了统一起见，我们都显式地将回溯语句写出来。\n与Java简单比较 Java中通常使用ArrayList\u0026lt;T\u0026gt;动态数组，可以做到随机访问，其底层也有一个数组，提供了add,remove,get,set等方法。\npublic class ArrayList\u0026lt;E\u0026gt; { private Object[] elementData; // 底层数组 private int size; // 当前元素数量 public boolean add(E e) { ensureCapacityInternal(size + 1); elementData[size++] = e; } } 也会发生扩容，使用Arrays.copyOf重新分配新数组。\n与切片不同的是，ArrayList是一个类，是一个引用类型，故传入参数是引用传递，不是Go中的值传递，所以对于上面那个*res,Java是不需要的，直接修改的原对象。\nList\u0026lt;Integer\u0026gt; arrayList = new ArrayList\u0026lt;\u0026gt;(); arrayList.add(1); // golang中得用append int val = arrayList.get(0); 补充Slices包的用法 Clone(slice)强制拷贝所有元素到新的底层数组，不共享内存 Equal(a, b) Compare(a, b) Index(slice, val) Delete(slice, i, j)删除下标范围[i,j)的元素，将前后两部分进行拼接 Sort(slice)对支持$\u0026lt;$运算符的进行升序排序，底层调用了sort.Slice Max(slice) 总结 切片，本身是个结构体，包含着指向底层数组的指针，长度，容量这三个属性；在函数值传递的过程中，在函数内部操作的是切片的副本，与原来的切片结构体地址不同，但是只要不发生扩容，底层的数组是相同的，但是仍需注意可能指针指向在数组的不同位置导致看到的值并不相同；如果想要相同切片，需要传入其地址；\nappend操作不影响\u0026amp;path即切片结构体自身的地址,在栈中（因为其长度总会发生变化），与上面相同，只要不发生扩容，底层数组指针就是相同的。\n我们强烈建议在预知切片的容量或者长度的情况下，对切片的初始化声明其容量或者长度，将大大地降低扩容带来的性能损耗。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n引用 《100 Go Mistakes and How to Avoid Them (Teiva Harsanyi)》 ChatGPT Leetcode ","permalink":"http://localhost:1313/posts/019golangep03_slice%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e在做力扣的题目时，特别是遇到递归问题，需要传入参数，遇到切片的时候经常遇到需要克隆切片的情况，而遇到普通的int类型的时候却不需要。\u003c/p\u003e\n\u003cp\u003e显而易见，这是一个值传递还是引用传递的问题，所以今天我来总结一下 Golang 中切片的底层原理，为什么在递归中需要克隆它。\u003c/p\u003e","title":"GolangEP03_slice底层原理及注意事项"},{"content":"引子 解决一些目前k3s在RiscV开发板上存在的问题。\nkubectl\u0026amp;crictl kubectl和crictl都是k3sCommandAPI中的命令，但是二者的运行结果却有所差异，我们可以从其差异中找到二者在使用上的异同。\n不过需要提前强调的是，kubectl命令是从kubelet的角度看的逻辑状态，而crictl是从底层容器运行时的视角看的底层容器状态。\n这是kubectl get pods的结果\nkubectl get pods NAME READY STATUS RESTARTS AGE redis-696579c6c8-v2wns 1/1 Running 1 (7m57s ago) 30h b6 0/1 ErrImagePull 0 (6d23h ago) 7d19h 这是crictl pods的结果\nPOD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME cf657c901892e 5 minutes ago Ready helm-install-traefik-pv4hv kube-system 5 (default) c60416658550d 5 minutes ago Ready local-path-provisioner-7b7dc8d6f5-48jjf kube-system 5 (default) 33c39dd34daff 5 minutes ago Ready helm-install-traefik-crd-ktfth kube-system 5 (default) 294e3dec339e2 5 minutes ago Ready metrics-server-668d979685-jthzj kube-system 5 (default) 8b67484f1bfe4 5 minutes ago Ready redis-696579c6c8-v2wns default 1 (default) 04ff59ad5464f 5 minutes ago Ready b6 default 5 (default) 2e1c2f055fb3e 5 minutes ago Ready coredns-b96499967-cpbrk kube-system 5 (default) e0a0516a940da 30 hours ago NotReady redis-696579c6c8-v2wns default 0 (default) ec8be446fcabf 7 days ago NotReady b6 default 0 (default) 首先从结果字段中来看,这些字段的含义如下：\n可以注意到，crictl pods中所获得的pods，并不是我们想象中的业务pods，而是Pod sandbox，这一点我们可以根据获取Pod的UID来判断——我们都知道每个Pod都会有自己的UID来进行区分。\nkubectl get pod redis-696579c6c8-v2wns -o jsonpath=\u0026#39;{.metadata.uid}{\u0026#34;\\n\u0026#34;}\u0026#39; a72e1056-01c3-4f74-80d9-3a7ed79fb3c2 可以发现其UID与当前crictl中的PodID根本不同，也进一步印证了我们的想法。同时，使用crictl inspectp \u0026lt;PODID\u0026gt;可以查到相同的uid，再次印证。\ncrictl inspectp 8b67484f1bfe4 { \u0026#34;status\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;8b67484f1bfe48c4de3cd5b8ee5f343013ae6236491622ee82fb8e865fce0d7f\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;attempt\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;redis-696579c6c8-v2wns\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;a72e1056-01c3-4f74-80d9-3a7ed79fb3c2\u0026#34; }, \u0026#34;state\u0026#34;: \u0026#34;SANDBOX_READY\u0026#34;, ..... 而其中的state完整结果其实是SANDBOX_READY,又证实了我们之前说的，crictl获得的是Pod sandbox。\n所以我会问为什么呢？这一定跟k3s中pod的启动流程有关吧。这里我们简单地描述一下pod的启动流程，后续会根据源码进行剖析。\nkubectl apply -f xx.yaml kubenetes API Server 调度Scheduled kubelet检测到新Pod启动 kubelet调用容器运行时(containerd) containerd创建Pod Sandbox 拉取业务容器镜像，启动容器 kubelet监听容器状态，返回给API Server 而Pod Sandbox是运行时为Pod所创建的一个隔离环境，在这个环境中要有network namespace,一个最小的容器pause容器作为网络占位符，只有这样，Pod内的多个容器才可以共享一个网络空间。\n所以回到最初，这两条命令的视角就不同，crictl是从容器运行时出发，关注的是Sandbox的状态；而kubectl是从更高的视角出发，其关注的是业务pods（包括业务容器）的运行状态，是从kubectl那里汇报的最新状态。\n检查关键组件容器状态 journalctl -u k3s -f # 结果 peneuler-riscv64 k3s[2438]: E0409 18:38:08.359021 2438 pod_workers.go:951] \u0026#34;Error syncing pod, skipping\u0026#34; err=\u0026#34;failed to \\\u0026#34;StartContainer\\\u0026#34; for \\\u0026#34;metrics-server\\\u0026#34; with ImagePullBackOff: \\\u0026#34;Back-off pulling image \\\\\\\u0026#34;rancher/mirrored-metrics-server:v0.5.2\\\\\\\u0026#34;\\\u0026#34;\u0026#34; pod=\u0026#34;kube-system/metrics-server-668d979685-jthzj\u0026#34; podUID=e7c219e4-2701-4924-9d7b-84fea6f8274b 4月 09 18:38:08 openeuler-riscv64 k3s[2438]: E0409 18:38:08.359159 2438 pod_workers.go:951] \u0026#34;Error syncing pod, skipping\u0026#34; err=\u0026#34;failed to \\\u0026#34;StartContainer\\\u0026#34; for \\\u0026#34;helm\\\u0026#34; with ImagePullBackOff: \\\u0026#34;Back-off pulling image \\\\\\\u0026#34;rancher/klipper-helm:v0.7.3-build20220613\\\\\\\u0026#34;\\\u0026#34;\u0026#34; pod=\u0026#34;kube-system/helm-install-traefik-crd-ktfth\u0026#34; podUID=3192257c-7e69-4c14-9b4e-d607ce188a88 4月 09 18:38:08 openeuler-riscv64 k3s[2438]: E0409 18:38:08.359193 2438 pod_workers.go:951] \u0026#34;Error syncing pod, skipping\u0026#34; err=\u0026#34;failed to \\\u0026#34;StartContainer\\\u0026#34; for \\\u0026#34;busybox\\\u0026#34; with ImagePullBackOff: \\\u0026#34;Back-off pulling image \\\\\\\u0026#34;riscv64/busybox:latest\\\\\\\u0026#34;\\\u0026#34;\u0026#34; pod=\u0026#34;default/b6\u0026#34; podUID=e423f26e-efd9-44bd-a65b-41dc722f3eb2 4月 09 18:38:09 openeuler-riscv64 k3s[2438]: E0409 18:38:09.604483 2438 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request 4月 09 18:38:09 openeuler-riscv64 k3s[2438]: W0409 18:38:09.639045 2438 garbagecollector.go:747] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request] 4月 09 18:38:11 openeuler-riscv64 k3s[2438]: E0409 18:38:11.357904 2438 pod_workers.go:951] \u0026#34;Error syncing pod, skipping\u0026#34; err=\u0026#34;failed to \\\u0026#34;StartContainer\\\u0026#34; for \\\u0026#34;local-path-provisioner\\\u0026#34; with ImagePullBackOff: \\\u0026#34;Back-off pulling image \\\\\\\u0026#34;rancher/local-path-provisioner:v0.0.21\\\\\\\u0026#34;\\\u0026#34;\u0026#34; pod=\u0026#34;kube-system/local-path-provisioner-7b7dc8d6f5-48jjf\u0026#34; podUID=a4b291fd-db16-498d-a136-9a1432f4649f 4月 09 18:38:14 openeuler-riscv64 k3s[2438]: E0409 18:38:14.357404 2438 pod_workers.go:951] \u0026#34;Error syncing pod, skipping\u0026#34; err=\u0026#34;failed to \\\u0026#34;StartContainer\\\u0026#34; for \\\u0026#34;coredns\\\u0026#34; with ImagePullBackOff: \\\u0026#34;Back-off pulling image \\\\\\\u0026#34;rancher/mirrored-coredns-coredns:1.9.1\\\\\\\u0026#34;\\\u0026#34;\u0026#34; pod=\u0026#34;kube-system/coredns-b96499967-cpbrk\u0026#34; podUID=04219540-713f-44d1-9d2d-7d5acf08222c ...... ps:pod_workers.go来自于k8s的源码\n可以发现首先存在的问题是有一些系统镜像在kube-system下的例如helm-install-traefik-crd-ktfth等存在镜像拉取失败问题，在K3sEP02解决RiscV开发板镜像无法拉取问题这篇文章中，我们解决了pause镜像拉取失败问题，当时pause镜像作为最基础的镜像如果拉取失败会导致整个节点无法使用crictl命令拉取镜像。\n而这些系统镜像虽然没有直接影响我们k3s的启动，但对于后续的操作产生了较大的影响例如无法暴露服务，Pod间无法进行通信等等问题。\n使用以下命令查看pod状态，发现有几个处于镜像拉取失败，并且可以与上面我们使用crictl pods命令的结果对应上。\nkubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default redis-696579c6c8-v2wns 1/1 Running 1 (148m ago) 32h 10.42.0.35 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system helm-install-traefik-crd-ktfth 0/1 ImagePullBackOff 0 8d 10.42.0.37 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-b96499967-cpbrk 0/1 ImagePullBackOff 0 8d 10.42.0.33 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-668d979685-jthzj 0/1 ImagePullBackOff 0 8d 10.42.0.36 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system helm-install-traefik-pv4hv 0/1 ImagePullBackOff 0 8d 10.42.0.39 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system local-path-provisioner-7b7dc8d6f5-48jjf 0/1 ImagePullBackOff 0 8d 10.42.0.38 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; default b6 0/1 ImagePullBackOff 0 (7d1h ago) 7d21h 10.42.0.34 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 其中CoreDNS是一个很核心的系统组件，如果没有它，我们的Pod间无法进行通信，所以接下来我们先处理CoreDNS镜像的拉取失败问题。\nCoreDNS镜像 CoreDNS作为Namespace为kube-system的系统级别镜像，用来实现动态可靠的服务发现. 两个Pod之间进行通信无需硬编码 Pod IP,只需要通过服务名称 service.default.svc.cluster.local 就可以发起请求 这其中 CoreDNS 会与 API Server 交互以获取服务信息——API Server 返回该 Service 的 ClusterIP 和端口。\nPodA 发起 DNS 查询-\u0026gt; CoreDNS -\u0026gt;查询调用 API Server -\u0026gt; API Server 响应 PodB 的 JSON 信息-\u0026gt; CoreDNS 构造 DNS 响应返回给 PodA 与此同时，如果某些 Pod 发生了重启或者迁移，CoreDNS 也会自动更新DNS记录。\n接下来我们看看如何将 CoreDNS 镜像移植到我们的开发板上。\n首先执行命令确定发现 Coredns 服务并不存在于当前的 k3s 中。\nkubectl -n kube-system get svc coredns Error from server (NotFound): services \u0026#34;coredns\u0026#34; not found 我们默认的 k3s 会去拉取 rancher/mirrored-coredns-coredns:1.9.1 这个镜像，所以我们要自己构建这个镜像在我们的私有仓库中，然后在开发板上拉取这个镜像进行替换使用。 类似于我们当时处理 pause 镜像，不同的是，pause 镜像在 k8s 的源码中是有其源码的，而 coreDNS 本质上是一个外部项目，所以我们需要从其 Github 的源码入手。\n1.方法一：本地交叉编译\n因为 CoreDNS 是用 Golang 编写的，而 Go 是可以指定架构进行编译的，并且支持 riscv64，所以我的初步思路是将其源码下载到我的mac上并指定为riscv64进行交叉编译。\ngit clone git@github.com:coredns/coredns.git git checkout v1.9.1 GOOS=linux GOARCH=riscv64 make 但是这样在构建的过程中会遇到问题\nlink: golang.org/x/net/internal/socket: invalid reference to syscall.recvmsg make: *** [coredns] Error 1 GOARCH=riscv64 时，Go 编译器在交叉编译时找不到适用于 RISC-V 架构的某些 syscall 实现，比如 syscall.recvmsg，这是 CoreDNS 或其依赖包 golang.org/x/net/internal/socket 里的系统调用。 这样的问题在网络上也很常见，如这个例子，可以发现 Go 标准库并没有做到实现所有架构的 syscall 支持，特别是一些底层的网络系统调用。\n下面给出源码中的 Makefile,可以发现其在构建时会去执行go get，这就是为什么会去找这个系统调用，也是为什么我选择先在mac上进行交叉编译（因为开发板的网络问题）\n# Makefile for building CoreDNS GITCOMMIT?=$(shell git describe --dirty --always) BINARY:=coredns SYSTEM:= CHECKS:=check BUILDOPTS?=-v GOPATH?=$(HOME)/go MAKEPWD:=$(dir $(realpath $(firstword $(MAKEFILE_LIST)))) CGO_ENABLED?=0 GOLANG_VERSION ?= $(shell cat .go-version) export GOSUMDB = sum.golang.org export GOTOOLCHAIN = go$(GOLANG_VERSION) .PHONY: all all: coredns .PHONY: coredns coredns: $(CHECKS) CGO_ENABLED=$(CGO_ENABLED) $(SYSTEM) go build $(BUILDOPTS) -ldflags=\u0026#34;-s -w -X github.com/coredns/coredns/coremain.GitCommit=$(GITCOMMIT)\u0026#34; -o $(BINARY) .PHONY: check check: core/plugin/zplugin.go core/dnsserver/zdirectives.go core/plugin/zplugin.go core/dnsserver/zdirectives.go: plugin.cfg go generate coredns.go go get .PHONY: gen gen: go generate coredns.go go get .PHONY: pb pb: $(MAKE) -C pb .PHONY: clean clean: go clean rm -f coredns 如何解决呢？可能得去修改 coreDNS 的源码关于系统调用这块了，但是本着不入侵编程的思想（懒比的思想）我们去找其他方法。\n2.方法二：找到官方提供的支持riscv的Release\n你别说，还真有，只是版本较新，来到了v1.12.1,所以我们暂时先用这个新版本，后续看是否会与开发板上需要的v1.9.1发生冲突。\nwget https://github.com/coredns/coredns/releases/download/v1.12.1/coredns_1.12.1_linux_riscv64.tgz tar -zxvf coredns_1.12.1_linux_riscv64.tgz 构建Docker镜像\nFROM alpine:latest # 换源 RUN sed -i \u0026#39;s#https\\?://dl-cdn.alpinelinux.org/alpine#http://mirrors.aliyun.com/alpine#g\u0026#39; /etc/apk/repositories WORKDIR /root COPY coredns /coredns CMD [\u0026#34;/coredns\u0026#34;] 构建镜像并推送至私有仓库，关于私有仓库见移植镜像这篇文章。\ndocker build --network host --platform=linux/riscv64 -f Dockerfile.coredns -t 192.168.173.76:6000/riscv64/coredns:1.0 . docker push 192.168.173.76:6000/riscv64/coredns:1.0 然后我们回到开发板上进行拉取并进行类似于pause镜像的适配crictl pull riscv64/coredns:1.0\n首先看看当前镜像Pod的描述状态\nkubectl get pods -n kube-system -l k8s-app=kube-dns kubectl describe pod \u0026lt;corednsPodName\u0026gt; -n kube-system 得到的结果如下:\nWarning Failed 17m kubelet Failed to pull image \u0026#34;rancher/mirrored-coredns-coredns:1.9.1\u0026#34;: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \u0026#34;docker.io/rancher/mirrored-coredns-coredns:1.9.1\u0026#34;: failed to resolve reference \u0026#34;docker.io/rancher/mirrored-coredns-coredns:1.9.1\u0026#34;: failed to do request: Head \u0026#34;https://registry-1.docker.io/v2/rancher/mirrored-coredns-coredns/manifests/1.9.1\u0026#34;: dial tcp [2a03:2880:f134:83:face:b00c:0:25de]:443: i/o timeout Warning Failed 15m kubelet Failed to pull image \u0026#34;rancher/mirrored-coredns-coredns:1.9.1\u0026#34;: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \u0026#34;docker.io/rancher/mirrored-coredns-coredns:1.9.1\u0026#34;: failed to resolve reference \u0026#34;docker.io/rancher/mirrored-coredns-coredns:1.9.1\u0026#34;: failed to do request: Head \u0026#34;https://registry-1.docker.io/v2/rancher/mirrored-coredns-coredns/manifests/1.9.1\u0026#34;: dial tcp [2a03:2880:f136:83:face:b00c:0:25de]:443: i/o timeout Warning Failed 15m (x4 over 19m) kubelet Error: ErrImagePull Warning Failed 15m (x6 over 18m) kubelet Error: ImagePullBackOff Normal Pulling 14m (x5 over 19m) kubelet Pulling image \u0026#34;rancher/mirrored-coredns-coredns:1.9.1\u0026#34; Normal BackOff 4m24s (x47 over 18m) kubelet Back-off pulling image \u0026#34;rancher/mirrored-coredns-coredns:1.9.1\u0026#34; 从k3s的源码 manifests 得知,这些系统级别的镜像（容器）例如traefik,coredns 都在这个目录中以 yaml 文件的形式存在. 这会在 k3s 启动时自动对其进行 Pod 的构建，相当于这是 k3s 自带的 Pod 们。\n具体在开发板上的路径是/var/lib/rancher/k3s/server/manifests\n现在，我们的思路变成——修改coredns.yaml文件使k3s在构建coredns容器时使用我们私有镜像仓库中的构建的来自于官方v1.12.1中适合riscv64的镜像。 于是我先将其打标签\n# 可以用crictl images | grep coredns查看 ctr -n k8s.io images tag \u0026lt;拉取下来的镜像名\u0026gt; docker.io/rancher/mirrored-coredns-coredns:1.9.1 非常值得注意的是，这里打标签前面必须要有docker.io,如果没有的话可能会出现无法识别本地镜像的问题.具体见Github上的相关问题。\n之后修改coredns.yaml,将其中镜像名称改为 docker.io/rancher/mirrored-coredns-coredns:1.9.1,拉取策略改为 Never ,并且在arg参数上面添加一行command: [\u0026quot;/coredns\u0026quot;]\n最后，删除对应的Pod，k3s会自动重新拉取镜像并构建此Pod，再来检查其运行状态与构建过程。\nkubectl delete pod -n kube-system -l k8s-app=kube-dns kubectl get pods -n kube-system -l k8s-app=kube-dns kubectl describe pod \u0026lt;corednsPodName\u0026gt; -n kube-system 最终得到结果如下\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 34s default-scheduler Successfully assigned kube-system/coredns-fcc987b6f-nkn92 to openeuler-riscv64 Normal Pulled 33s kubelet Container image \u0026#34;docker.io/rancher/mirrored-coredns-coredns:1.9.1\u0026#34; already present on machine Normal Created 33s kubelet Created container coredns Normal Started 33s kubelet Started container coredns Warning Unhealthy 2s (x18 over 33s) kubelet Readiness probe failed: HTTP probe failed with statuscode: 503 这里虽然出现503错误但是只是健康检测出现问题，可能是配置有误或者插件失败，但是至少容器coredns创建成功了. 并且再次执行journalctl -u k3s -f后可以发现之前的 coredns 创建失败的日志不见了。\n解决遗留问题 上面只是解决了CoreDNS组件的镜像拉取问题，但是拉取下来之后其还存在一些其他问题，如果你在上面的构建过程中也遇到了类似问题，请见下方处理方案。\n问题1：启动覆盖问题 首先，如果我们下一次重启k3s，就会发现 Coredns 这个容器又构建失败了，并且原因正是我们修改过的 coredns.yaml 文件。\nWarning Failed 72s (x2 over 73s) kubelet Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \u0026#34;-conf\u0026#34;: executable file not found in $PATH: unknown 这是因为k3s会在重启时自动恢复一些核心组件的yaml文件为默认值，覆盖掉我们的更改。 在K3s官方的Issue中也有类似的问题,而官方文档中的说法也提到了Auto-Deploying Manifests。\n所以，根据官方文档中的内容，我们应该先去更改k3s的启动命令，而我们的k3s是由systemd启动的，所以执行sudo systemctl edit k3s，在ExecStart的最后添加上--disable=coredns\nFor example, to disable traefik from being installed on a new cluster, or to uninstall it and remove the manifest from an existing cluster, you can start K3s with \u0026ndash;disable=traefik. Multiple items can be disabled by separating their names with commas, or by repeating the flag.\nExecStart=/usr/local/bin/k3s \\ server \\ --disable=coredns \\ 修改 yaml 文件使得 k3s 在启动时使用我们自定义的 yaml 文件。修改内容同上，在 arg 前加一行command[\u0026quot;/coredns\u0026quot;]\ncp coredns.yaml custom-coredns.yaml 重启服务\nsudo systemctl daemon-reexec sudo systemctl daemon-reload sudo systemctl restart k3s 至此，可以解决启动覆盖问题，即k3s不会在每次启动时都用默认的coredns.yaml来启动corednsPod,而是使用我们自定义的custom-coredns.yaml文件来进行。\n如果后续我们自己从头构建适合 riscv 的k3s可执行文件,直接从源码层面修改镜像名称即可,不用这么麻烦.\n问题2:Readiness probe failed 虽然我们使用了本地镜像并且修改了 k3s 默认的关于 coreDNS 的配置，但是仍然会遇到问题. 即上面提到的这个 Readiness probe 的网络问题.\nReadiness probe failed: HTTP probe failed with statuscode: 503 对于这个问题，目前我还没有找到合适的解决方法，我个人倾向于是开发板 iptables 的问题，在这里我放一些排查的手段以供后面的操作。\n# 查看当前的Corefile内容 kubectl -n kube-system get configmap coredns -o yaml # 进入Pod调试（但是当前的Pod不断地重启） kubectl -n kube-system exec -it \u0026lt;coredns-pod-name\u0026gt; -- sh curl -v localhost:8080/health # /etc/resolv.conf配置可能出现问题，无法访问到其中的DNS,如果可以进入Pod，检查是否可以访问设置的DNS wget -O- http://192.168.173.153 # 检查kube-proxy是否生效 kubectl -n kube-system get pods -l k8s-app=kube-proxy 故我们先继续解决其他系统 Pod 以及 Iptables 等问题。\n总结 对于 CoreDNS 系统组件,我们发现其官方给出了支持 riscv 的版本,所以无需我们自己重新编译. 我们选择的方法是本地构建对应的镜像并拉取到开发板上,并修改掉源码中的名称为我们构建的名称,最终实现了镜像的替换.\n但是这是否太过于依赖于本地的构建?我们应该能做到从某个能够拉取的地址自动拉取,可以将docker.io这个修改为我们自己的服务器吗? 这是我们后续要探索的方向之一.\nexport INSTALL_K3S_EXEC=\u0026#34;--system-default-registry=your.registry.com\u0026#34; INSTALL_K3S_SKIP_DOWNLOAD=true bash k3s-install.sh 后续更新 最近在 k3s 的官方源码 fork 中找到了一位大佬的 fork,其对于 coredns.yaml 只进行了以下的修改:\nimage: \u0026#34;%{SYSTEM_DEFAULT_REGISTRY}%rancher/mirrored-coredns-coredns:1.11.3\u0026#34; image: \u0026#34;%{SYSTEM_DEFAULT_REGISTRY}%coredns/coredns:1.11.3\u0026#34; 系统默认镜像一般是docker.io,所以其只是改了个名字?\n后续我们在自行构建的时候,暂时决定将这些系统镜像全部放在自己的私有镜像仓库中, 然后在启动 k3s 的时候指定默认仓库为私有镜像仓库.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/018k3sep04%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8A%E7%9A%84k3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e解决一些目前k3s在RiscV开发板上存在的问题。\u003c/p\u003e\n\u003ch2 id=\"kubectlcrictl\"\u003ekubectl\u0026amp;crictl\u003c/h2\u003e\n\u003cp\u003ekubectl和crictl都是k3sCommandAPI中的命令，但是二者的运行结果却有所差异，我们可以从其差异中找到二者在使用上的异同。\u003c/p\u003e","title":"K3sEP04——开发板上的k3s存在的问题01之CoreDNS镜像"},{"content":"引子 某天我想要在我的macos上使用homebrew安装riscv编译工具链为了交叉编译一些镜像文件。\nbrew tap riscv-software-src/riscv brew install riscv-tools 即使其中遇到了一些网络问题，但都可以解决，直到遇到一个在我看来很神奇的问题，Homebrew与Xcode的版本有关，而Xcode的升级又必须依赖macos系统的升级。这就逼迫我这种不太喜欢升级系统的人不得不升级一下macos的系统。\n所以这一顿操作结束后我就在思考，为什么？\nHomebrew简介 还记得大三第一次使用mac的时候，我觉得这不就是一个笔记本形态的ipad吗？下载软件也是从AppStore或者浏览器搜到的DMG文件，直到某天我在下载某个Github上的软件时看到有一段专门为macos写的命令brew install xxx，我才知道原来Homebrew才是macos下载东西的利器！\n如果您没有使用macos，或许您没有听过Homebrew，不过简单地类比就是我们在linux下使用的apt，dnf，yum等包管理器。如果您了解linux的包管理器，apt和dnf在执行安装时都是直接安装二进制文件，而Homebrew采取了两种方式一种是安装二进制文件，一种是本地编译构建，因为其本质是一个用Ruby编写的Git仓库项目。\n形象地描述一下Homebrew的执行原理——Homebrew这个软件安装助手其自身叫Brew,他要为我们安装软件时需要找到软件安装说明书即Formula软件安装脚本（用ruby编写），诸多的软件对应着诸多的安装脚本，我们使用Core这样一个Git仓库来保存这些脚本。当安装命令下达时，管家查找本地的Core仓库，寻找对应的脚本，找到后查看脚本中是否设置了对应的Bottle(二进制文件)链接，如果有则直接下载这个Bottle，如果没有，则利用安装脚本中指定的链接下载源码进行本地构建。\n让我们根据清华源中给出的安装步骤再来看看Homebrew的执行原理。\nexport HOMEBREW_BREW_GIT_REMOTE=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git\u0026#34; export HOMEBREW_CORE_GIT_REMOTE=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git\u0026#34; export HOMEBREW_INSTALL_FROM_API=1 echo \u0026#39;export HOMEBREW_API_DOMAIN=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles/api\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zprofile echo \u0026#39;export HOMEBREW_BOTTLE_DOMAIN=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zprofile export HOMEBREW_API_DOMAIN=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles/api\u0026#34; export HOMEBREW_BOTTLE_DOMAIN=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles\u0026#34; pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple # 安装 git clone --depth=1 https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/install.git brew-install /bin/bash brew-install/install.sh rm -rf brew-install # 加入环境变量 test -r ~/.zprofile \u0026amp;\u0026amp; echo \u0026#39;eval \u0026#34;$(/opt/homebrew/bin/brew shellenv)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zprofile 在这里我们设置了Homebrew_Brew和Homebrew_Core的地址使用清华的镜像仓库，这样在使用Core仓库时就不会遇到网络问题；设置了API_DOMAIN和BOTTLE_DOMAIN的镜像地址，并且将FROM_API设置为1，意味着开启了API模式——接着上面的解释，之前Homebrew必须将Core仓库全部Clone到本地，每次更新时也会更新极为庞大的仓库，所以自2021年后，推出的API安装模式，不再需要Clone Core仓库，而是通过API访问https://api.homebrew.sh/找到指定的JSON文件，brew管家会解析这个JSON文件知道其Bottole在哪，用哪个架构，版本号是多少等各种信息。\n最终下载好的二进制包默认在Cellar目录下，可执行文件会被设置一个软链接，让我们在终端可以直接运行此软件。最后补充一点，Homebrew对比apt,dnf来说更像是一个用户的软件助手，而非系统管家，因为其不需要root权限。\n所以到这里对于Homebrew我们就有了一个大致的了解，后续我可能会对比一下这些包管理工具的异同，敬请期待。\n为什么需要升级Xcode 首先解释为什么需要Xcode,前面说到，如果没有Bottle,Homebrew会进行本地构建，这里更关键的是，Homebrew本身不处理编译过程，而是依赖于系统工具链，对于macos来说就是Xcode的clang和make，通过xcode-select --install安装。\n我的场景是要安装riscv的编译工具链，因为使用了brew tap，添加了第三方的formula仓库，会去Github上clone这个仓库，所以其一定会触发本地编译构建过程。\n其次为什么要升级，执行brew doctor这个诊断指令，系统会提示你版本过低。\nError: Your Command Line Tools are too outdated. Please update them via Software Update or `xcode-select --install`. 并且Xcode提供的CLT工具链会调用macos的系统库，所以我们必须要升级macos的系统。\n总结Homebrew操作 写到这里，我对Homebrew的理解已经比较清晰了，所以同时来总结一下其可能的操作流程。\n# 查看基本信息 brew info \u0026lt;appName\u0026gt; # 查看依赖项 brew deps \u0026lt;appName\u0026gt; brew deps --tree \u0026lt;appName\u0026gt; # brew install \u0026lt;appName\u0026gt; # 如果需要tap指定第三方仓库,然后再安装 brew tap # 列出安装详细过程 brew install \u0026lt;appName\u0026gt; --verbose # 列出已安装的 brew list \u0026lt;appName\u0026gt; --version # 检查配置操作和潜在问题 brew doctor # 更新brew本体和formula brew update # 升级所有安装软件 brew upgrade 举例说明,这个RiscV工具链是本地构建的。\nbrew info riscv-tools ==\u0026gt; riscv-software-src/riscv/riscv-tools: stable 0.2 RISC-V toolchain including gcc (with newlib), simulator (spike), and pk http://riscv.org Installed /opt/homebrew/Cellar/riscv-tools/0.2 (5 files, 54.5KB) * Built from source on 2025-04-03 at 18:05:56 From: https://github.com/riscv-software-src/homebrew-riscv/blob/HEAD/riscv-tools.rb ==\u0026gt; Dependencies Required: riscv-gnu-toolchain ✔, riscv-isa-sim ✔, riscv-pk ✔ (base) lutao@MagicTech Downloads % brew deps riscv-tools dtc gmp isl libmpc lz4 mpfr riscv-software-src/riscv/riscv-gnu-toolchain riscv-software-src/riscv/riscv-isa-sim riscv-software-src/riscv/riscv-pk xz zstd 这个Go语言来自于Bottle\n(base) lutao@MagicTech Downloads % brew info go ==\u0026gt; go: stable 1.24.2 (bottled), HEAD Open source programming language to build simple/reliable/efficient software https://go.dev/ Installed /opt/homebrew/Cellar/go/1.24.2 (14,109 files, 283.8MB) * Poured from bottle using the formulae.brew.sh API on 2025-04-03 at 17:31:38 From: https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git/Formula/g/go.rb License: BSD-3-Clause 启发 对于我们日后镜像的迁移构建，其带来了一定的启发\n把每一步构建流程脚本化、配置化 提高可复现性，适配 CI/CD 缓存和复用已构建的镜像或软件包 构建前先 resolve 所有依赖 类似 bottle 仓库 + fallback 编译方式 那么本篇文章到这里就结束了，这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/017macos%E7%B3%BB%E7%BB%9F%E5%8D%87%E7%BA%A7%E4%B8%8Ehomebrew%E7%9A%84%E5%85%B3%E7%B3%BB/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e某天我想要在我的macos上使用homebrew安装\u003ca href=\"https://github.com/riscv-software-src/homebrew-riscv/\"\u003eriscv编译工具链\u003c/a\u003e为了交叉编译一些镜像文件。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebrew tap riscv-software-src/riscv\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebrew install riscv-tools\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e即使其中遇到了一些网络问题，但都可以解决，直到遇到一个在我看来很神奇的问题，Homebrew与Xcode的版本有关，而Xcode的升级又必须依赖macos系统的升级。这就逼迫我这种不太喜欢升级系统的人不得不升级一下macos的系统。\u003c/p\u003e","title":"DebugEP02——MacOs系统升级与Homebrew的关系"},{"content":"引子 了解K3s的都知道，Pod，容器，镜像在K3s中的重要作用，所以我们首先要克服的难点正是如何让镜像们适配RiscV架构。通常，我们拉取镜像都来自于dockerhub,里面搜索确实有一些RiscV镜像，例如riscv64/nginx,riscv64/redis等，但是经检查，其实它并不是官方认证的镜像，也几乎没有被维护或者使用，所以我们需要自己去进行镜像的适配。 拿nginx镜像举例:\ndocker search riscv64/nginx\n检查是否有可用的镜像\nriscv64/nginx Official build of Nginx. 0 nginx/nginx-ingress NGINX and NGINX Plus Ingress Controllers fo… 102 nginx/nginx-prometheus-exporter NGINX Prometheus Exporter for NGINX and NGIN… 49 nginx/nginx-ingress-operator NGINX Ingress Operator for NGINX and NGINX P… 2 nginx/unit This repository is retired, use the Docker o… 65 nginx/nginx-quic-qns NGINX QUIC interop 1 nginxproxy/nginx-proxy Automated nginx proxy for Docker containers … 161 nginx Official build of Nginx. 20719 [OK] 可以发现riscv64/nginx的Stars数量为0，也不是官方镜像，所以拉取失败。（即使使用代理地址，开发板上也会遇到网络问题，因为代理地址中也没保存非官方的镜像）\n由此可见，自行构建合适的镜像迫在眉睫。\n私有镜像仓库 Docker为用户提供了一种构建私有镜像仓库的能力，利用其registry镜像，我们可以搭建一个私人的镜像仓库，由此我们可以不再依赖于远程拉取镜像，在搭建K3s集群时，可以选择一个节点作为数据存储节点，既存储各个Agent与Server的数据，也存储我们需要的专属镜像数据。\n由于registry这个镜像暂时也不适配RiscV架构（会出现经典的manifest问题）\n所以我们先在arm(macos)上尝试一下私有镜像如何搭建。\n私有镜像搭建实验 本次实验将Mac主机私有镜像库的物理机，一台RiscV开发板作为拉取镜像的机器。\n首先，在Mac上构建Registry容器。\n# 这里我选择6000端口作为本机端口（5000被系统占用，无法彻底杀死） docker run -d -p 6000:5000 --restart=always --name registry registry:2 构建成功后，如果你使用DockerDesktop可以在其中容器部分发现容器已经在正常运行了。我们再进行一下相关测试curl http://\u0026lt;macIP\u0026gt;:6000/v2/,其会返回\u0026quot;repository[]\u0026quot;信息，证明成功。\n这时我们随便拉下来一个镜像进行测试docker pull busybox:latest 对这个镜像进行打标签，并推送到私有仓库中。(打标签操作本质上是复制一份镜像并为其改名)\n# 注意，将后续的macIP改为自己本机的IP地址 # 原本tag命令不需要IP，但是如果指向私有地址，则需要带上私有地址IP docker tag busybox:latest \u0026lt;macIP\u0026gt;:6000/riscv64/busybox:1 # 将此镜像推送到私有仓库中 docker push \u0026lt;macIP\u0026gt;:6000/riscv64/busybox:1 结果如下\nThe push refers to repository [192.168.173.76:6000/riscv64/busybox] be632cf9bbb6: Pushed 1: digest: sha256:c109a60479ed80d63b17808a6f993228b6ace6255064160ea82adfa01c36deba size: 527 同理可以使用上面的curl命令测试当前私有仓库中的镜像列表。\n**其次，本次实验为了方便暂时使用HTTP，没有使用HTTPS,即带有TLS的方式。**并且本次实验的主机与开发板连在同一个子网（热点）下，后续我们会探究使用https的方式。\n所以，我们需要在本机与开发板上的docker的配置文件中同时添加\n{ \u0026#34;insecure-registries\u0026#34;: [\u0026#34;\u0026lt;macIP\u0026gt;:6000\u0026#34;] } 然后，重启Dockersudo systemctl restart docker\n最后，来到RiscV开发板上拉取镜像docker pull \u0026lt;macIP\u0026gt;:6000/riscv64/busybox:1，使用docker images进行检查发现其存在。\n这样我们就做到了从私有仓库拉取了一个我们在开发板上拉取不到的镜像到开发板上，这对于我们日后的集群搭建工作起到了关键的作用。\n融入k3s 由于k3s的容器运行时采用containerd这个轻量化的原生工具而不建议使用docker，所以我们需要针对k3s的registries.yaml文件进行修改,将镜像地址全部修改为我们上面搭建的私有仓库地址。\nmirrors: docker.io: endpoint: - \u0026#34;http://192.168.173.76:6000\u0026#34; # 这里后续我们要思考如何将静态的IP地址写成动态或者如何面对IP变化的情况 - \u0026#34;docker.1ms.run\u0026#34; # 保险起见设置一个其他的镜像仓库 rewrite: \u0026#34;^riscv64/(.*)\u0026#34;: \u0026#34;riscv64/$1\u0026#34; \u0026#34;192.168.173.76:6000\u0026#34;: endpoint: - \u0026#34;http://192.168.173.76:6000\u0026#34; 之后再使用crictl命令进行拉取时就可以直接crictl pull \u0026lt;镜像名称\u0026gt;,注意，由于我们的镜像前面都是标有仓库地址的，所以在crictl pull时直接使用镜像名称即可。\n常用操作 检查私有仓库中有哪些镜像。\ncurl -X GET http://192.168.173.76:6000/v2/_catalog # 结果如下 {\u0026#34;repositories\u0026#34;:[\u0026#34;busybox\u0026#34;,\u0026#34;mac_riscv64/alpine\u0026#34;,\u0026#34;riscv64/busybox\u0026#34;]} 多平台构建适配的镜像 首先我们先抛开k3s,使用docker来进行移植适配操作，毕竟二者没有太大的差异，而docker可以提供更强大的性能。操作流程来自于Docker官方文档中的使用QEMU进行多平台镜像构建方法，这是官方文档中三种方法之一，希望后面我们可以尝试另外两种方法。\n这次我们要适配的镜像是redis和nginx，操作的主机是Macbook M2,使用Docker Desktop方便操作。\n提前配置 根据文档中的说法，我们需要先从classic镜像模式转换为containerd镜像模式，这在Docker Desktop上很好操作。\n操作流程 1.新建目录保存redis等中间件的源码，为其编写的Dockerfile，以及可能自定义的配置文件等文件。\nmkdir -p multi-platform 2.为了防止容器内部下载速度过慢，提前下载好redis的源码文件并解压到本机。\nwget https://download.redis.io/releases/redis-7.2.4.tar.gz tar -xzvf redis-7.2.4.tar.gz 3.编写Dockerfile文件，这里要注意我们使用alpine:latest作为基础镜像，这个镜像是我尝试的几个基础镜像中适配riscv最好的；同时需要替换默认的apk包镜像源，否则会遇到could not connect server问题；为了设置密码等一些自定义配置，我们自行编写一个配置文件到当前目录中redis.conf\n# 允许所有 IP 连接（不要绑定 127.0.0.1） bind 0.0.0.0 # 关闭 protected mode，否则外部无法连接 protected-mode no # 设置访问密码 requirepass password # 设置持久化 RDB 保存策略 save 900 1 save 300 10 save 60 10000 # RDB 文件名 dbfilename dump.rdb # RDB 文件保存路径 dir ./ # 日志级别 loglevel notice # 日志输出到标准输出（容器中建议这样） logfile \u0026#34;\u0026#34; # 后台运行，容器中不能开启 daemonize no # 最大连接数（可选） maxclients 10000 # 设置内存上限（可选） # maxmemory 256mb # maxmemory-policy allkeys-lru 这是Dockerfile:\nFROM alpine:latest # 替换默认 apk 源为镜像源 RUN sed -i \u0026#39;s#https\\?://dl-cdn.alpinelinux.org/alpine#http://mirrors.aliyun.com/alpine#g\u0026#39; /etc/apk/repositories # 安装 Redis 构建依赖 RUN apk update \u0026amp;\u0026amp; apk add --no-cache build-base jemalloc-dev linux-headers # 拷贝 Redis 源码（假设 redis-7.2.4 文件夹和 Dockerfile 同目录） COPY redis-7.2.4 /redis WORKDIR /redis # 编译 Redis RUN make # 拷贝配置文件 COPY redis.conf /etc/redis.conf # 设置默认启动命令 CMD [\u0026#34;src/redis-server\u0026#34;, \u0026#34;/etc/redis.conf\u0026#34;] 4.构建跨平台镜像。这一步本质上用到了buildx和QEMU，但是Docker Desktop为我们把这些底层都封装了起来，所以如果你是用Docker Desktop启动的Docker就不用在意;构建的同时如果遇到网络问题，请在参数中添加--network host命令，这会将宿主机网络作为容器内部网络，绕过网络限制。这个解决思路来自于stackoverflow\n# 这里我们直接-t打好了标签，不用再tag了 docker build --network host --platform=linux/riscv64 -f Dockerfile.redis -t 192.168.173.76:6000/riscv64/redis:1.1 . 5.推送到本地私有镜像仓库中，再从RiscV开发板中拉取该镜像,这一步和上面私有镜像仓库构建相同。\ndocker push 192.168.173.76:6000/riscv64/redis:1.1 # riscv下 docker pull 192.168.173.76:6000/riscv64/redis:1.1 6.运行redis-server容器，并在主机上运行redis-cli来测试\ndocker run -d --name redis-test -p 6379:6379 192.168.173.76:6000/riscv64/redis:1.1 # 主机运行 redis-cli -h \u0026lt;riscvIP\u0026gt; -p 6379 \u0026gt; AUTH password \u0026gt; set foo bar \u0026gt; get foo 得到最终结果\u0026quot;bar\u0026quot;,证明我们成功了！\n同理我们也可以测试一下nginx镜像，下面是其简单的dockerfile\nFROM alpine:latest # 替换源（可选，也可以用清华源） RUN sed -i \u0026#39;s#dl-cdn.alpinelinux.org#mirrors.aliyun.com#g\u0026#39; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add --no-cache nginx # 拷贝配置文件和静态资源 COPY nginx.conf /etc/nginx/nginx.conf COPY html/ /usr/share/nginx/html/ # 暴露端口 EXPOSE 80 # 启动 Nginx CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 其中的nginx.conf\nworker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } } } 对应的html文件内容:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Welcome to RISC-V Nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: \u0026#34;Segoe UI\u0026#34;, sans-serif; background-color: #f4f4f4; text-align: center; padding-top: 100px; } h1 { color: #333; } p { color: #777; } .tag { display: inline-block; margin-top: 20px; padding: 8px 16px; background-color: #007acc; color: white; border-radius: 4px; font-weight: bold; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;🚀 Hello from RISC-V Nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Your nginx server is up and running.\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;tag\u0026#34;\u0026gt;riscv64 / alpine / nginx\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 同理打包推送拉取。运行容器后，在我的mac主机上访问http://\u0026lt;riscvIP\u0026gt;:8080可以看到上述html文件的效果\n优化镜像 当前redis镜像大小为146MB\n(base) lutao@MagicTech multi-platform % docker images | grep redis 192.168.173.76:6000/riscv64/redis 1.1 e8e1336c3ccf 13 hours ago 146MB 我们可以采用多阶段构建以精简最终镜像，使其不包含构建工具和源代码。\n# ---------- 构建阶段 ---------- FROM alpine:latest AS builder # 使用阿里源加速 RUN sed -i \u0026#39;s#http.*/alpine#http://mirrors.aliyun.com/alpine#g\u0026#39; /etc/apk/repositories # 安装构建依赖 RUN apk update \u0026amp;\u0026amp; apk add --no-cache build-base jemalloc-dev linux-headers # 拷贝源码 COPY redis-7.2.4 /redis WORKDIR /redis # 编译 Redis RUN make # ---------- 运行阶段 ---------- FROM alpine:latest # 使用阿里源（可选） RUN sed -i \u0026#39;s#http.*/alpine#http://mirrors.aliyun.com/alpine#g\u0026#39; /etc/apk/repositories # 安装运行所需的 jemalloc RUN apk add --no-cache jemalloc # 拷贝 redis-server 和 redis-cli（如果你需要 CLI 工具） COPY --from=builder /redis/src/redis-server /usr/local/bin/ COPY --from=builder /redis/src/redis-cli /usr/local/bin/ # 拷贝配置文件 COPY redis.conf /etc/redis.conf # 设置默认启动命令 CMD [\u0026#34;redis-server\u0026#34;, \u0026#34;/etc/redis.conf\u0026#34;] 最终构建过程花费293.4s\n最终镜像大小只有10.2MB，相较于之前的146MB缩小了许多。\n同时我们也可以构建一个只有redis-cli的镜像，对于融合进k3s集群中或许可以在server节点上使用redis-server镜像，然后在其他agent节点的pod中部署redis-cli对其进行访问。\n总结 搭建了一个简易的私有镜像仓库和移植了redis-server镜像到RiscV机器上，算是迈出了不错的一步，后面的工作主要围绕着如何将其更深入地融入到k3s中以及深入探究docker的多平台构建其底层原理展开。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/016k3sep03%E7%A7%BB%E6%A4%8D%E9%95%9C%E5%83%8F%E5%88%B0riscv%E5%BC%80%E5%8F%91%E6%9D%BF/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e了解K3s的都知道，Pod，容器，镜像在K3s中的重要作用，所以我们首先要克服的难点正是如何让镜像们适配RiscV架构。通常，我们拉取镜像都来自于dockerhub,里面搜索确实有一些RiscV镜像，例如\u003ccode\u003eriscv64/nginx,riscv64/redis\u003c/code\u003e等，但是经检查，其实它并不是官方认证的镜像，也几乎没有被维护或者使用，所以我们需要自己去进行镜像的适配。\n拿nginx镜像举例:\u003c/p\u003e","title":"K3sEP03——移植镜像到RiscV开发板"},{"content":"引子 开篇提问，大家平时用终端的次数多吗？\n如果大多数时候你在虚拟机上工作，那不用多说，如果是macos，估计iTerm你已经耳熟能详，如果是windows，也许是Powershell?（本人平时windows只用来打游戏了）\n不管怎么样，在我们使用这些终端的时候，通常都会面对一个大大的黑框，然后就在这个黑框里不断地操作，那么此时如果我想要保留当前框中的内容的同时又想开另外一个框来进行其他操作呢？我想，这一定是所有使用终端的人都会遇到的问题。\n系统中的默认终端编辑器给我们了这样一种做法：New Tab，与浏览器窗口的打开一样，一个接着一个，但这还不够好，如果我想要同时观察两个窗口中的内容呢？总不能一会儿点这个，一会儿又点回另一个吧，那这样的操作肯定是有点累赘的。\n所以就有了tmux这个终端窗口器，它可以做到在一个窗口内，分割出多个子窗口，这个子窗口其实叫做窗格，接下来我们会简单的说一下其中的概念，不过我想当你亲自上手使用一段时间后，这些概念反倒没有那么重要了。\n我对于tmux的了解来源于Missing课程，其中也包含了Vim的操作等非常重要的课程。\n简介 Tmux中有几个概念需要了解，从大往小来说分别是：会话-\u0026gt;窗口-\u0026gt;窗格 下面我们逆序来分别介绍一下他们。\n窗格 如上图所示，窗格就是我在一个窗口内分出的四个块，甚至可以做到不同尺寸（稍后见配置文件），这每一个窗格我们其实都可以把他当作一个新的终端，这样就可以完美解决在引子中我们提到的问题，可以同时观察终端中运行的不同命令的结果。\n我们可以垂直划分也可以水平划分窗口来创建不同的窗格。（操作见下方快捷键）\n窗口 窗口就是最开始普通的终端编辑器中的大窗口的概念，在tmux中我们不仅可以在一个窗口内创建多个窗格，还可以在一个会话内创建多个窗口，就像下图中红色框出的部分，在这一个会话中我创建了5个不同的窗口并且对其进行不同的命名以区分每个窗口的作用，是不是比不停的去New Tab要整齐地多？\n这些窗口已经自动排好了序，那如何在这些窗口中切换呢？虽然tmux给了相应的快捷键，但是我更改了配置文件，现在可以做到直接点击切换窗口。\n会话 我们上面说过，一个会话包含多个窗口，而我们每次进行操作本质上是进入了一个会话中，我们一般以一个会话包含一系列类似的工作窗口。\n如何创建和退出会话呢，这里就要用到一些命令了(当然快捷键也可以)\n# 新建 tmux new -s \u0026lt;会话名\u0026gt; # 退出会话，但是仍在后台执行没有销毁 tmux detach # 进入会话 tmux attach -t \u0026lt;会话名\u0026gt; # 列出会话 tmux ls # 杀死 tmux kill-session -t \u0026lt;会话名\u0026gt; or \u0026lt;会话id\u0026gt; # 切换会话 tmux switch -t \u0026lt;会话名\u0026gt; or \u0026lt;会话id\u0026gt; # 重命名 tmux rename-session -t \u0026lt;会话名\u0026gt; \u0026lt;new-name\u0026gt; 配置文件 tmux本身是一个快捷键很多的工具，而其快捷键又极其依赖操作前缀，默认是Ctrl/Command+b,而低头看看自己的键盘，这两个键离得是不是有点稍远了点，不是那么好按啊。以及其新建窗格的按键是什么%,切换又是;这种有些很难记且很难按的设置，所以我们需要个性化一下，来改变~/.tmux.conf这个文件的内容。\n下面我列出的配置是专门对于我macos的配置，其实与windows或者linux大查不查，有些地方不一样而已。\n将前缀修改为Ctrl/Command+a,下面以Pre代替 窗口管理\n启用base-index 1让窗口编号从1开始 Pre+c 新建窗口，Pre+t切换下一个窗口；Pre+space/Backspace下/上一个窗口 Pre+v/s垂直/水平分割当前窗口产生窗格 Pre+,对窗口进行重命名操作 窗格管理\n启用了vim模式，可以使用Pre + h/j/k/l在窗格之间导航 Pre+a切换到上一个窗格 setw -g mouse on启用了鼠标模式，这样我们可以实现点选窗口，拖拽窗格大小，还能够选择一段代码（其状态会变为黄色）自动复制到剪贴板上，方便复制（注意这样点选后会自动来到最新的终端位置） 并且开启了状态栏，可以在底部显示时间信息。 其他的操作大家可以自行查阅资料。\n# vim style tmux config # use C-a, since it\u0026#39;s on the home row and easier to hit than C-b set-option -g prefix C-a unbind-key C-a bind-key C-a send-prefix set -g base-index 1 # Easy config reload bind-key R source-file ~/.tmux.conf \\; display-message \u0026#34;tmux.conf reloaded.\u0026#34; # vi is good setw -g mode-keys vi # mouse behavior setw -g mouse on set-option -g default-terminal screen-256color bind-key : command-prompt bind-key r refresh-client bind-key L clear-history bind-key space next-window bind-key bspace previous-window bind-key enter next-layout # use vim-like keys for splits and windows bind-key v split-window -h bind-key s split-window -v bind-key h select-pane -L bind-key j select-pane -D bind-key k select-pane -U bind-key l select-pane -R # smart pane switching with awareness of vim splits bind -n C-h run \u0026#34;(tmux display-message -p \u0026#39;#{pane_current_command}\u0026#39; | grep -iqE \u0026#39;(^|\\/)vim$\u0026#39; \u0026amp;\u0026amp; tmux send-keys C-h) || tmux select-pane -L\u0026#34; bind -n C-j run \u0026#34;(tmux display-message -p \u0026#39;#{pane_current_command}\u0026#39; | grep -iqE \u0026#39;(^|\\/)vim$\u0026#39; \u0026amp;\u0026amp; tmux send-keys C-j) || tmux select-pane -D\u0026#34; bind -n C-k run \u0026#34;(tmux display-message -p \u0026#39;#{pane_current_command}\u0026#39; | grep -iqE \u0026#39;(^|\\/)vim$\u0026#39; \u0026amp;\u0026amp; tmux send-keys C-k) || tmux select-pane -U\u0026#34; bind -n C-l run \u0026#34;(tmux display-message -p \u0026#39;#{pane_current_command}\u0026#39; | grep -iqE \u0026#39;(^|\\/)vim$\u0026#39; \u0026amp;\u0026amp; tmux send-keys C-l) || tmux select-pane -R\u0026#34; bind -n \u0026#39;C-\\\u0026#39; run \u0026#34;(tmux display-message -p \u0026#39;#{pane_current_command}\u0026#39; | grep -iqE \u0026#39;(^|\\/)vim$\u0026#39; \u0026amp;\u0026amp; tmux send-keys \u0026#39;C-\\\\\u0026#39;) || tmux select-pane -l\u0026#34; bind C-l send-keys \u0026#39;C-l\u0026#39; bind-key C-o rotate-window bind-key + select-layout main-horizontal bind-key = select-layout main-vertical set-window-option -g other-pane-height 25 set-window-option -g other-pane-width 80 set-window-option -g display-panes-time 1500 set-window-option -g window-status-current-style fg=magenta bind-key a last-pane bind-key q display-panes bind-key c new-window bind-key t next-window bind-key T previous-window bind-key [ copy-mode bind-key ] paste-buffer # Setup \u0026#39;v\u0026#39; to begin selection as in Vim bind-key -T copy-mode-vi v send -X begin-selection bind-key -T copy-mode-vi y send -X copy-pipe-and-cancel \u0026#34;xclip -sel clip\u0026#34; # Update default binding of `Enter` to also use copy-pipe unbind -T copy-mode-vi Enter bind-key -T copy-mode-vi Enter send -X copy-pipe-and-cancel \u0026#34;xclip -sel clip\u0026#34; # Status Bar set-option -g status-interval 1 set-option -g status-style bg=black set-option -g status-style fg=white set -g status-left \u0026#39;#[fg=green]#H #[default]\u0026#39; set -g status-right \u0026#39;%a%l:%M:%S %p#[default] #[fg=blue]%Y-%m-%d\u0026#39; set-option -g pane-active-border-style fg=yellow set-option -g pane-border-style fg=cyan # Set window notifications setw -g monitor-activity on set -g visual-activity on # Enable native Mac OS X copy/paste set-option -g default-command \u0026#34;/bin/bash -c \u0026#39;which reattach-to-user-namespace \u0026gt;/dev/null \u0026amp;\u0026amp; exec reattach-to-user-namespace $SHELL -l || exec $SHELL -l\u0026#39;\u0026#34; # Allow the arrow key to be used immediately after changing windows set-option -g repeat-time 0 最终激活配置可以直接Pre+R这是我们设置好的激活快捷键；或者你直接重启tmux服务\ntmux kill-server， tmux 简单流程 总结一下使用的流程，来到我们的终端后，执行如下操作\ntmux new -s t1 # 新建一些窗口 Pre+c # 对当前窗口进行垂直或水平划分 Pre+v/c # 进行终端操作即可 # 退出tmux会话 tmux detach # 再次进入 tmux ls tmux a -t \u0026lt;会话名\u0026gt; ","permalink":"http://localhost:1313/posts/015tmux%E7%BB%88%E7%AB%AF%E7%BC%96%E8%BE%91%E5%99%A8/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e开篇提问，大家平时用终端的次数多吗？\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/xm/hdw.png\"\u003e\u003c/p\u003e\n\u003cp\u003e如果大多数时候你在虚拟机上工作，那不用多说，如果是macos，估计iTerm你已经耳熟能详，如果是windows，也许是Powershell?（本人平时windows只用来打游戏了）\u003c/p\u003e","title":"Tmux终端窗口器"},{"content":"引子 继上次我们验证K3s在虚拟机上的集群部署，我们这次直接在RiscV开发板上实验部署K3s。\n值得注意的是，k3s官方支持的架构里面是没有RiscV的，所以有了我们即将开展的适配工作，同时k3s的底层源码大部分都是由Golang构成，而Golang是适配RiscV的，所以我们针对RiscV的K3s移植工作是有可行性的。\n本次实验利用了这两篇文章中的配置:玩转RISCV开发板01-烧录OpenEuler国产镜像,玩转RISCV开发板02-配置好容器化环境，关键在于使用了RiscV+Openeuler的架构系统组合，来自于SIG兴趣小组的23.09的镜像，以及其已经貌似适配好的k3s包。（具体请参考上面的两篇文章）\n实验环境 LiCheePi4A 16+256G OpenEuler23.09镜像,下载地址 实验过程 我们之前使用sudo dnf install -y k3s成功安装了k3及其依赖\n[root@openeuler-riscv64 ~]# sudo dnf install -y k3s mainline 3.8 kB/s | 3.0 kB 00:00 epol 5.4 kB/s | 3.0 kB 00:00 ceph-user 4.6 kB/s | 3.0 kB 00:00 chromium-user 4.4 kB/s | 3.0 kB 00:00 libreoffice 4.6 kB/s | 3.0 kB 00:00 rust167-user 5.8 kB/s | 3.0 kB 00:00 mesa-supplements 3.7 kB/s | 3.0 kB 00:00 oetestsuite 5.1 kB/s | 3.0 kB 00:00 update 4.6 kB/s | 3.0 kB 00:00 Dependencies resolved. ================================================================================ Package Arch Version Repo Size ================================================================================ Installing: k3s riscv64 1.24.2+rc1+k3s2-3.oe2303 epol 59 M Installing dependencies: container-selinux noarch 2:2.163-1.oe2303 mainline 39 k k3s-selinux noarch 1.1.stable.1-1.oe2303 epol 20 k policycoreutils riscv64 3.4-1.oe2303 mainline 156 k selinux-policy noarch 38.6-4.oe2303 mainline 24 k selinux-policy-targeted noarch 38.6-4.oe2303 mainline 6.8 M Transaction Summary ================================================================================ Install 6 Packages Total download size: 66 M Installed size: 87 M Downloading Packages: (1/6): container-selinux-2.163-1.oe2303.noarch. 22 kB/s | 39 kB 00:01 (2/6): selinux-policy-38.6-4.oe2303.noarch.rpm 13 kB/s | 24 kB 00:01 (3/6): policycoreutils-3.4-1.oe2303.riscv64.rpm 84 kB/s | 156 kB 00:01 (4/6): selinux-policy-targeted-38.6-4.oe2303.no 4.1 MB/s | 6.8 MB 00:01 (5/6): k3s-selinux-1.1.stable.1-1.oe2303.noarch 14 kB/s | 20 kB 00:01 (6/6): k3s-1.24.2+rc1+k3s2-3.oe2303.riscv64.rpm 4.2 MB/s | 59 MB 00:14 -------------------------------------------------------------------------------- Total 4.2 MB/s | 66 MB 00:15 Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Running scriptlet: selinux-policy-targeted-38.6-4.oe2303.noarch 1/1 Preparing : 1/1 Installing : policycoreutils-3.4-1.oe2303.riscv64 1/6 Running scriptlet: policycoreutils-3.4-1.oe2303.riscv64 1/6 Created symlink /etc/systemd/system/multi-user.target.wants/restorecond.service → /usr/lib/systemd/system/restorecond.service. Installing : selinux-policy-38.6-4.oe2303.noarch 2/6 Running scriptlet: selinux-policy-38.6-4.oe2303.noarch 2/6 Running scriptlet: selinux-policy-targeted-38.6-4.oe2303.noarch 3/6 Installing : selinux-policy-targeted-38.6-4.oe2303.noarch 3/6 Running scriptlet: selinux-policy-targeted-38.6-4.oe2303.noarch 3/6 Running scriptlet: container-selinux-2:2.163-1.oe2303.noarch 4/6 Installing : container-selinux-2:2.163-1.oe2303.noarch 4/6 Running scriptlet: container-selinux-2:2.163-1.oe2303.noarch 4/6 Running scriptlet: k3s-selinux-1.1.stable.1-1.oe2303.noarch 5/6 Installing : k3s-selinux-1.1.stable.1-1.oe2303.noarch 5/6 Running scriptlet: k3s-selinux-1.1.stable.1-1.oe2303.noarch 5/6 Installing : k3s-1.24.2+rc1+k3s2-3.oe2303.riscv64 6/6 Running scriptlet: selinux-policy-targeted-38.6-4.oe2303.noarch 6/6 Running scriptlet: container-selinux-2:2.163-1.oe2303.noarch 6/6 Running scriptlet: k3s-selinux-1.1.stable.1-1.oe2303.noarch 6/6 Running scriptlet: k3s-1.24.2+rc1+k3s2-3.oe2303.riscv64 6/6 Verifying : container-selinux-2:2.163-1.oe2303.noarch 1/6 Verifying : policycoreutils-3.4-1.oe2303.riscv64 2/6 Verifying : selinux-policy-38.6-4.oe2303.noarch 3/6 Verifying : selinux-policy-targeted-38.6-4.oe2303.noarch 4/6 Verifying : k3s-1.24.2+rc1+k3s2-3.oe2303.riscv64 5/6 Verifying : k3s-selinux-1.1.stable.1-1.oe2303.noarch 6/6 Installed: container-selinux-2:2.163-1.oe2303.noarch k3s-1.24.2+rc1+k3s2-3.oe2303.riscv64 k3s-selinux-1.1.stable.1-1.oe2303.noarch policycoreutils-3.4-1.oe2303.riscv64 selinux-policy-38.6-4.oe2303.noarch selinux-policy-targeted-38.6-4.oe2303.noarch Complete! 验证部署Server节点 接下来我们需要验证部署Server节点，由于之前我们安装好的k3s只是一个运行脚本，所以我们需要执行 INSTALL_K3S_SKIP_DOWNLOAD=true k3s-install.sh 并执行kubectl get nodes检查集群中的节点。\n[root@openeuler-riscv64 ~]# INSTALL_K3S_SKIP_DOWNLOAD=true k3s-install.sh [INFO] Skipping k3s download and verify [INFO] Skipping installation of SELinux RPM [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s [ 4166.015532] bridge: filtering via arp/ip/ip6tables is no longer available by default. Update your scripts to load br_netfilter if you need this. [ 4166.085229] Bridge firewalling registered [ 4436.489721] IPVS: Registered protocols (TCP, UDP) [ 4436.501236] IPVS: Connection hash table configured (size=4096, memory=32Kbytes) [ 4436.507135] IPVS: ipvs loaded. [ 4438.223653] IPVS: [rr] scheduler registered. [root@openeuler-riscv64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION openeuler-riscv64 Ready control-plane,master 3m37s v1.24.2+k3s- 这个过程中跳过了k3s的下载和架构验证，创建了系统 kubectl, crictl,ctr 三个命令的系统链接到 k3s. 创建了 kill all，uninstall 脚本，创建了环境变量，服务文件。\n需要提醒的是，此时我们只有一个节点一个开发板，而在 K3s 的源码中默认Server既是Server又是Agent，这样才能在单机的情况下运行。\n之后我们验证其关于 pod,容器等具体功能是否支持。\n我们可以写一个简单的 yaml 文件，指定使用riscv64/busybox:latest这个镜像，执行kubectl apply -f b1.yaml进行部署 可以发现其创建成功了，但是在执行kubectl get pods时显示的状态一直是ContainerCreated 我们需要使用kubectl describe pod b1来观察这个pod创建的具体过程。\nps:由于本记录没有与实验同步进行（这是我需要吸取的一个教训）所以没有相应的截图可以贴出。 这里拿一个类似的情况举例,发现pause镜像无法拉取导致其他 Pod 无法正常启动.\nWarning FailedCreatePodSandBox 38s (x129 over 108m) kubelet (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to get sandbox image \u0026#34;carvicsforth/pause:v3.10-v1.31.1\u0026#34;: failed to pull image \u0026#34;carvicsforth/pause:v3.10-v1.31.1\u0026#34;: failed to pull and unpack image \u0026#34;docker.io/carvicsforth/pause:v3.10-v1.31.1\u0026#34;: failed to resolve reference \u0026#34;docker.io/carvicsforth/pause:v3.10-v1.31.1\u0026#34;: failed to do request: Head \u0026#34;https://registry-1.docker.io/v2/carvicsforth/pause/manifests/v3.10-v1.31.1\u0026#34;: dial tcp 208.101.60.87:443: i/o timeout 可以发现其成功 Scheduled 了，但是拉取镜像 pause:3.6 时遇到了 manifest 问题，其实就是pause镜像并不支持 RiscV架构. 于是我们就来深入研究一下 pause 镜像这个东西。\nPod需要一个中间容器Infa来实现，而这个Infra容器正是pause容器\nPause在K8s的源码位置 /kubernetes/build/pause/linux/pause.c，其作用就是\n无限循环调用 pause 系统调用并休眠直到收到信号。 作为 PID 为1的进程，专一于维护命名空间和进程管理。 故我们需要先解决这个 pause 镜像的适配问题，由于其源码就是一段C代码而已，所以我们直接尝试在开发板上进行镜像的适配。\n适配Pause镜像 如果没有安装编译工具链，请先安装dnf install -y gcc\n将上述 pause.c文件下载或传输到开发板上,这里使用ssh相关命令\nscp -r 本机文件路径 目标主机名@IP:目标路径 以下步骤参考整合于 Deepseek 与 ChatGpt 的回答。\n1.直接编译现有代码\ngcc -static -Os -o pause pause.c 2.检查并测试\nldd ./pause # 测试版本显示 ./pause -v 3.容器化使用\nFROM scratch COPY pause /pause ENTRYPOINT [\u0026#34;/pause\u0026#34;] 4.测试容器\n# 构建镜像 docker build -t my-pause . # 运行测试 docker run --rm -it --name test-pause my-pause # 在另一个终端发送停止信号 docker stop test-pause # 应看到正常退出 5.docker保存镜像为tar文件\ndocker save my-pause \u0026gt; pause-riscv.tar 6.在k3s节点上加载镜像\n# 先查看当前k3s使用的containerd命名空间,我的结果是k8s.io k3s ctr namespace ls # 再加载 k3s ctr images import pause-riscv.tar # or k3s ctr -n k8s.io images import pause-riscv.tar 7.将此镜像作为本地镜像\n# 如果没有这个文件夹，自行创建 cd /var/lib/rancher/k3s/agent/images mkdir -p /var/lib/rancher/k3s/agent/images # 放置我们自己的pause镜像 cp ./pause-riscv.tar /var/lib/rancher/k3s/agent/images 8.将此镜像打标签以适应原本拉取时所要的镜像名称\n# 先查看当前我们制作的镜像名称 ctr images ls # 重新打标签 后面的名称是之前k3s拉取时拉取失败的镜像名称 ctr -n k8s.io images tag \u0026lt;上一条命令的名称\u0026gt; docker.io/rancher/mirrored-pause:3.6 9.再使用 crictl,ctr 命令检查镜像\n将这两个命令理解为docker命令即可，用来管理镜像。\nctr --namespace=k8s.io images list | grep pause crictl images | grep pause 10.重新尝试busybox镜像\nkubectl apply-f b1.yaml # 再用crictl命令,使用debug标签观察其过程 crictl --debug pull riscv64/alpine # 最后检查pod是否运行成功 kubectl get pods -o wide 最终发现busybox和alpine镜像均可以成功拉取，不会再出现pause镜像不适配的问题。\n其他镜像可能会因为网络问题，适配问题无法拉取成功，这是我们后续需要进行的工作。\n回到源码 回到源码之中，在cli/cmds/agent.go源码中，关于pause-image命令的代码如下\nPauseImageFlag = \u0026amp;cli.StringFlag{ Name: \u0026#34;pause-image\u0026#34;, Usage: \u0026#34;(agent/runtime) Customized pause image for containerd or docker sandbox\u0026#34;, Destination: \u0026amp;AgentConfig.PauseImage, Value: \u0026#34;rancher/mirrored-pause:3.6\u0026#34;, } 所以在启动agent节点时可以直接在命令行中指定\nsudo k3s agent \\ --pause-image=your-repo/your-pause:3.6-riscv64 \\ --server=https://\u0026lt;K3s-Server-IP\u0026gt;:6443 \\ --token=\u0026lt;Your-Token\u0026gt; 源码中关于pause的构建Dockefile如下：\nARG BASE FROM ${BASE} ARG ARCH ADD bin/pause-linux-${ARCH} /pause USER 65535:65535 ENTRYPOINT [\u0026#34;/pause\u0026#34;] 可供我们构建时参考。\n总结 至此，我们完成在 RiscV+Openeuler 开发板上 k3s 的 Server 节点部署工作，并成功解决了其拉取 pause 镜像出现的架构不适配问题。 我们采用的方法是本地自行构建适合 riscv 的 pause 镜像并让 k3s 使用这个镜像启动.\n后续目标再仔细研究 k3s/k8s 源码中有关于 Pod 创建的过程，看看其对 pause的具体应用在哪里. 如何能够将 pause 这部分的缺陷从源码的部分补充完整，可以不再需要直接从本地构建，而是可以直接部署。\n更新(5.8) 从大佬的思路中得到启发,我们可以修改源码中的pkg/cli/cmds/const_linux.go文件的 DefaultPauseImage = \u0026quot;rancher/mirrored-pause:3.6\u0026quot; 换为我们自己仓库的地址,例如carvicsforth/pause:v3.10-v1.31.1(这是大佬的地址),这样就可以避免了本地构建.\n","permalink":"http://localhost:1313/posts/014k3sep02%E8%A7%A3%E5%86%B3riscv%E5%BC%80%E5%8F%91%E6%9D%BF%E9%95%9C%E5%83%8F%E6%97%A0%E6%B3%95%E6%8B%89%E5%8F%96%E9%97%AE%E9%A2%98/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e继上次我们验证K3s在虚拟机上的集群部署，我们这次直接在RiscV开发板上实验部署K3s。\u003c/p\u003e\n\u003cp\u003e值得注意的是，k3s官方支持的架构里面是没有RiscV的，所以有了我们即将开展的适配工作，同时k3s的底层源码大部分都是由Golang构成，而Golang是适配RiscV的，所以我们针对RiscV的K3s移植工作是有可行性的。\u003c/p\u003e","title":"K3sEP02——解决RiscV开发板镜像无法拉取问题"},{"content":"本篇文章用来记录自己在虚拟机上搭建k3s集群的整个过程，并且使用了kine这个k3s专属的数据存储方法（代替了k8s中的etcd存储方式）\n实验环境 本机: macos M2 虚拟机: 采用Multipass这个虚拟机软件，专门为Ubuntu而制作的虚拟化软件，可以很轻松地在macos上使用。\n一台虚拟机Agent01作为数据库的后端服务器 1G/8G 一台虚拟机Server作为Server 2G/8G 一台虚拟机Agent02作为Agent 1G/4G 操作系统: Ubuntu22.04\n由于是实验环境，我们全部的操作直接在root权限下进行，如果接下来某些操作遇到权限问题请添加 sudo。\n那让我们直接摇滚进来吧！\n部署 以下步骤来自于AI的指导并成功部署运行。\nMYSQL节点 由于k3s的轻量化要求，使用了kine作为存储方案，旨在可以不使用etcd进行集群管理，可以外置其他数据库，例如,sqlLite,mysql,postgresql等。\n数据库里面存放了原本etcd会存放的信息，例如各个节点的信息。\napt update apt install mysql-server -y mysql -u root -p 配置数据库\ncreate database k3s; create user \u0026#39;k3s\u0026#39; @ \u0026#39;%\u0026#39; identified by \u0026#39;yourpassword\u0026#39;; # 替换为自己设置的密码 grant all privileges on k3s.* to \u0026#39;k3s@\u0026#39;%\u0026#39;\u0026#39;; flush privileges; 修改Mysql配置文件允许远程连接\nsed -i \u0026#39;s/127.0.0.1/0.0.0.0/g\u0026#39; /etc/mysql/mysql.conf.d/mysqld.cnf systemctl restart mysql 最后，使用hostname -I记录一下这台虚拟机的IP地址，方便后续连接操作。\n使用kine部署k3s server 使用--datastore-endpoint参数指定外接数据库的地址。\ncurl -sfL https://get.k3s.io | sh -s - server \\ --datastore-endpoint=\u0026#34;mysql://k3s:yourpassword@tcp(yourMysqlIP:3306)/k3s\u0026#34; # 替换为自己的密码和上一步的IP地址 ps:这个方法可能会遇到网络问题，我们还可以采用离线的方式安装k3s，详情请见另一篇文章\n检查Server的状态\nsystemctl status k3s kubectl get nodes 同时可以回到数据库节点检查表信息。\nuse k3s; show tables; select * from kine limit5 /G; 可以发现有kine这个表。\n部署Agent节点 首先从Server中获取Token，cat /var/lib/rancher/k3s/server/node-token\n将agent加入集群中由Server管理\ncurl -sfL https://get.k3s.io | K3S_URL=\u0026#34;https://yourServ erIP:6443\u0026#34; K3S_TOKEN=\u0026#34;K10eb92a0a28e01cdcb43aebadf54e4c647274509bb903266f8b5753acce221234d::server:ecda1824570feac881c9b27b98584c76\u0026#34; sh - 验证集群功能 在Server上执行\nkubectl get nodes -o wide 可以看到Server和Agent节点都存在。\n到这里就初步部署成功。\n添加镜像代理 如果我们使用registry模式，即从docker.io,dockerhub上拉取镜像， 我们需要同时在Server和Agent上添加 /etc/rancher/k3s/registries.yaml文件\n在其中添加如下配置，以使代理的方式拉取镜像。其中镜像地址填入 endpoint 下方，目前可用的镜像地址\nmirrors: docker.io: endpoint: - \u0026#34;https://docker.1ms.run\u0026#34; 可能出现的问题 在运行过程中如果你的本机IP地址变化，可能会出现数据库Mysql的IP封禁问题\n使用下面命令查看问题所在\nsudo journalctl -u k3s --no-pager --lines=50 可以发现Error 1129: Host '192.168.64.2' is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts'\u0026quot;，由于太多次连接Mysql失败触发Mysql的安全机制，封禁了k3s Server的IP地址。\n所以我们需要从数据库层面解禁。\nmysql\u0026gt; FLUSH HOSTS; Query OK, 0 rows affected, 1 warning (0.03 sec) mysql\u0026gt; SELECT host FROM performance_schema.host_cache WHERE host=\u0026#39;192.168.64.2\u0026#39;; Empty set (0.06 sec) 之后便可以正常运行了。\n总结 后续文章准备探索离线安装k3s以及在open-Rv这个组合上一系列的k3s适配工作流程展示。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/013k3sep01%E6%A8%A1%E6%8B%9Fkine%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","summary":"\u003cp\u003e本篇文章用来记录自己在虚拟机上搭建k3s集群的整个过程，并且使用了kine这个k3s专属的数据存储方法（代替了k8s中的etcd存储方式）\u003c/p\u003e\n\u003ch2 id=\"实验环境\"\u003e实验环境\u003c/h2\u003e\n\u003cp\u003e本机: macos M2\n虚拟机: 采用\u003ca href=\"https://canonical.com/multipass\"\u003eMultipass\u003c/a\u003e这个虚拟机软件，专门为Ubuntu而制作的虚拟化软件，可以很轻松地在macos上使用。\u003c/p\u003e","title":"K3sEP01——模拟Kine集群部署"},{"content":"这是第一期的奇思乱想，暂且将这个专栏作为我的博客中非技术分类下的随意输出内容，灵感来源于JJLin的专辑《和自己对话》，所以就让我在这个专栏里面真正地与自己对话，每个周六的下午，带上耳机接一杯温水，开始与自己对话，一场完全脱离AI的自我对话。\n我有罪 结束了在百度并不那么尽如人意的实习之后，我回到了学校，迎面而来的是如火如荼的暑期实习；坦白讲我并没有好好地提前准备关于暑期实习的东西，众所周知，现在的国内校招环境需要实习，项目，八股，算法，场景。现在我将分析一下自己犯下的几宗罪吧，至于这到底是不是罪，后面的内容中会有我与自己争辩的地方。\n就先拿算法来说吧，这是一个日积月累的东西，虽然网络上常常讲将hot100来回刷上两遍就好了，但是我觉得那并不足够，对于我这个跨考boy来说肯定不够，所以我比较后悔在百度实习的这段时期没有坚持地刷力扣，懈怠了，总是以实习太累了为理由推脱这本该进行的训练，这是其一罪。\n再拿八股来看，Mysql的八股自己也算看过一遍，Redis虽然没有Mysql看45讲那样细致，但也算看过一遍；Golang的八股说实话，不多，但是Java的八股还是蛮多的，这又是一个矛盾点，后面再辩。总之，对于八股，我就像对于考研时的政治一样，甚至还不如政治，认为需要背诵的东西在理工科的学习工作中是令我厌恶的，这是第二罪。因为这明显是为了自己的懒惰做推脱，如果现在反问自己到底懂这些东西吗，如果立即问一些简单的知识点，我是无法回答上来的，那么，我又如何能胜任心中想要的某份工作呢？\n实习，还是绕不过这一条，在极其幸运的加持下进入了一个大型互联网公司进行自己的第一份实习，又在不幸之下游离于团队的边缘，总结一句就是自己的实力其实是不够的，所以机会即使偶然送到自己的手边，也会随之而去。嗯，的确机会是留给有准备的人的，但是是不是该在后面加一句，是否能把握住机会也是得真正地准备充分的。所以，这是其三罪。\n项目，这里我想说的项目并不是黑马点评之类的，而是自己在学校的项目，说实话，空天院的项目自己在没有有效监督的情况下懈怠了太多太多，除了让自己学了Docker，K8s，Golang这些理论内容，实质性的进展几乎为0，而作为自己的毕设项目却因为比不上真正的实习项目被自己搁浅，没有积极沟通，这是其罪四。\n其罪五其实与个人有关，初中的时候就对玩游戏这件事情无法把持，以此失去了初恋（当然这不是全部原因）还好及时悬崖勒马考上了个不错的高中，但是回想起来这对我的影响还是蛮大的；到了现在，仍然迷恋游戏，从当初的峡谷变成了云顶，云顶对我来说好像赌博一样有吸引力，输了一把总会有着不甘心的心态让自己不断地陷进去，在睡觉前总会暗下决心，明天一定不会再玩了，一定一定，甚至某次将游戏删除了只会消停几天就会下载回来，然后陷入自我怀疑和自我放弃的死循环中，我觉得这是我自己的自控力和决心问题，很讽刺的是，从小学围棋以静心思考为目的开始的我，却一点也没有再静心思考了，在短视频媒体的裹挟下，本不多的耐心更是被不断消耗殆尽。\n我要辩 从哪里辩起呢？思来想去也不知道，那不如直接好几个角度随便辩吧。\n工作 那我问你，经过百度的实习，你到底知道自己想要什么吗？据我观察，百度的研发工作时间通常是10-9-5，如果新的需求比较紧，有时也会干到10点，紧急一点会到12点甚至以后。那这就意味着，我一天的生活就是两眼一睁八点多，或者九点？坐地铁或者骑车来到公司，通勤时间从一个小时到一个半小时不等。十点多来到公司，一天的工作后我们折中一下九点半下班，再次通勤到家，洗漱一下看看视频就到十一点十二点了，甚至没有一个小时的时间给自己留出来打游戏哈哈；或者等到十点免费打车回家，同样的，洗漱结束后也就十一点多了，又该睡觉了。\n到了周末会清闲吗？也许会，但如果身上的担子重一点，还要把公司的电脑带回家，防止线上出现什么问题，到了周末如果我一个人在出租屋里我大概率就会睡到中午，点个外卖，打一下午游戏，如果晚上有球赛可以去看看球赛，然后第二天继续打打游戏，看看电视剧综艺，再去外面吃个饭，结束了周末的两天。又开始根本没有wlb的周内生活。\n循环往复，好处是什么呢？就是高工资，一个月我们取个25k吧，到手可能20k（当然职级更高会更高，我以自己的视角代入来看。）。那么我失去了什么呢？每天都要高强度对着电脑，一坐就可能是一个小时，甚至更久，这样久坐久坐；每天下班都没有自己的生活，如果我想坚持跑步呢？我这个人跑完步没有一个小时是睡不好的，这样的下班时间根本不可能坚持夜跑，这样势必会影响到自己的睡眠。\n自然，天下没有两全的东西，我的小领导在百度呆了七年了，和爱人一起在北京买了房，今年已经成功收房装修了，以后也算是在北京有了自己的家。这么看来，也是蛮不错的是吧。所以，说了这么多，我真的知道自己想要什么吗？其他的工作例如国企，银行这些刻板印象中可以做到wlb的工作，是否真的可以做到wlb呢？如果也是一样的加班呢，工资反倒比私企要少很多。但我想应该客观地进行比较，累钱多和闲钱少，自己究竟要选择哪个呢？\n即使自己嘴上说着随遇而安，哪里要自己去哪里，但是如果真的有选择可以做，总归要做选择的吧。（辩着辩着给自己留下了一个此时难以回答的问题）\n找实习 最近在招聘暑期实习，我只投了一些，唯一一个我觉得可能适配度不错的虾皮直接在简历初筛阶段就给我挂掉了，舍友与我投的同一个岗位他的简历初筛过了给了笔试，很不幸，被卡本科了。所以，我不得不去胡思乱想，卷这些所谓的技术我得到了什么？每天自己的生活就是泡在有关找工作的技术里了，感觉少了什么，像是得了一种空心病，就是一种不断追求一个世俗给自己强加的目标，如果没有这个目标，自己是不是一下子就不知道要干什么了，不知道自己的意义与价值了？为了逃避这个问题，我总对自己说，过一天是一天，活着就好，要那么多欲望干什么！但这真能说服自己吗？\n遗憾的是，在这里我还是只能回答自己一个模棱两可的答案，我不知道自己能否控制住我的欲望，过一天算一天的心态好像完全违背了所在的社会指向。但是对于空心病这个问题，我觉得还是可以说说。说实话，18岁之前，一个名为高考的剑悬在头上，让我的目标不会偏离它太远，总会让我有一个东西去追逐，本科阶段，我的目标就是跨考，由于之前的高考失利，这个目标更为强烈了，所以也追逐地更彻底了。成功后进入研究生阶段，头上的目标变成了就业，好像又回到了18岁之前，这个目标有了更多的徘徊的余地，不是考研那样one shot,一旦失败就几乎意味着必须开启下一年的努力，所以在不这么紧张的环境下，对我而言滋生出的紧张情绪并没有那么严重，简单来讲就是我没有考研的时候那么一击必胜的信念。\n这就又回到了天梯的思维，我的一生就是不断地去攀爬着这个天梯吗？好的大学，好的学历，好的工作，后面呢？一切都是要更好，就像竞技体育中的更高更快更强，不断地自律。但是，我真的必须去费劲攀爬这个天梯吗，不可以休息一会儿吗，不可以去寻找其他的出路吗？人生肯定不止攀爬天梯这一条路对吧。现在感觉，我需要不断地去说服自己，如果要自律，就从不断地自律中汲取可以支持自己不断自律的养分，即找到正反馈。\n如果不想攀爬天梯，就会暂时丢掉人生的目标，头顶的目标不见了，从我现在天梯人的视角来看，会不会立即像无头苍蝇一样愣在原地，无所适从？所以我也逐渐理解女生的追星行为，就是为自己找了一个头顶的目标，生活至少有个盼头对吧，不至于愣在原地。那么丢掉天梯，我猜爱好就成为了自己的追求吧。你看，我的核心思想还是没有转变，头顶要有一个悬着的目标推着自己前进，不论是前进也好，左转右转也罢，都是一种前进。说服自己，从目标里获取养分。\nOver 今天就暂时说到这里吧，这个专栏就是一气呵成，一下午写完的随便说说，与自己和解，与自己对话的栏目，没有任何的编排，没有任何的美化考虑，Just Write by Myself.\n","permalink":"http://localhost:1313/posts/012%E5%A5%87%E6%80%9D%E4%B9%B1%E6%83%B3ep01/","summary":"\u003cp\u003e这是第一期的奇思乱想，暂且将这个专栏作为我的博客中非技术分类下的随意输出内容，灵感来源于JJLin的专辑《和自己对话》，所以就让我在这个专栏里面真正地与自己对话，每个周六的下午，带上耳机接一杯温水，开始与自己对话，一场完全脱离AI的自我对话。\u003c/p\u003e","title":"LT的奇思乱想Ep01"},{"content":"我的2024 时光如梭啊，一转眼2025年都已经过了快两个月了，我才来写这一篇关于我的2024年总结，因为2024的结尾发生了一些事情吧。\n往年都是以视频的形式，今年，以个人博客的方式来回顾吧。同样也会用回顾朋友圈的形式来辅助实现回顾。\n1月 一月距离现在有些遥远了，感觉自己一月份的时候和现在完全不是一个模样。一月的自己还在研一的上学期的末尾，还在学习Java的项目和代码随想录的题目，对自己未来的研究方向也没有清晰的目标,还在做着去年的年度总结\u0026hellip;\n所以我翻找出了我的朋友圈，看看一月份都发生了什么吧\n我分享了很多的歌曲啊，从陶喆到王菲甚至最后到皇后乐队，那时的我还没有放弃健身，每周至少还会进行三到四练，那时的我还是没有太多的烦恼，顶多为了自己的毕设和还没有实习而稍微苦思，去年的一月怎么感觉已经距我如此的遥远了呢？唉，难免感伤悲怀一下。\n行笔至此，我觉得回顾一整年的命题对我来说永远是伪命题，因为我永远都是站在现在的视角之下去看以前或者未来的事情，我无法跳脱出当前视角的局限性，现在的我是兴奋的心情那么我会对某些事物带有偏向好的方向去回顾，反之亦然。所以，博客的首要目的还是输出给自己看到，就像我很喜欢的林俊杰的专辑《和自己对话》，是一个记录自己思路和反哺的过程。\n当然，如果幸运的有别人能够看到并进行思想上的交流，那也是值得骄傲的。\n2月 2月啊，是过年的时候，也是我从上大学以来每年回家的时候。这么四五年来，除了毕业，我一年只有过年的时候才会回家，也许是因为从小家庭环境的因素，我对家的依恋并不是那么强烈，甚至刚回家四五天就不想在家里待下去了。\n所以我是不恋家的，甚至有点想要逃离，以至于在我考虑就业或者升学的城市时我从来没有将陕西作为我的首选，反倒是打着一种随遇而安，哪里的公司要我我就去哪里这样一种游荡的心态。与之伴随的是我的知足常乐的心态，任何事情做到差不多自己满意度超过一个阈值就好了（当然我也知道这种阈值是自我安慰的一个手段罢了）\n还是看看我的二月都发了什么朋友圈吧。\n还是一堆歌曲，特别是关于龙的歌曲，那应该是在龙年春晚的时候我发的，觉得这些歌曲才应该在春晚唱响哈哈。还有曼联的图片，对于曼联其实他并不是我的主队，但是在英超就想找个队看，由于C罗的原因选择了曼联，即使发生了一些事情我也懒得去管，我只负责看球并不是某种迷。\n对了，打雷姐的这张专辑我觉得彻底把我征服了，是我去年听过最好听的英文专辑了。\n3月 2月底回到了学校，比正式开学早了两周，原因上面也有所提及。那时的我应该还是在学习Java看着黑马程序员刷着代码随想录。\n9月 9月对于还在学校的我来说是一个不错的月份，因为它代表着新，一个新的学年开始了，而我自己的身份也从研一变成了研二，研二和大二差别还是蛮大的，毕竟研究生只有三年，最终目标一般来说只有就业，和读博并不能作为并行的选择，而本科有四年，考研和工作在一定程度上还是可以兼容一下的。意味着研二一下子就来到了研究生生涯中最为紧张的一年。\n不过新鲜血液的到来还是为平淡的研究生生涯增添了一些调味剂，即使它并不是想象中的甜味。在这个月，我们组终于有了工位，虽然只是很紧凑的一排，大概一个人的宽度只能横着放下两台macbook13英寸的笔记本，和后面一排的同学几乎椅子靠椅子，进出还需要互相谦让，但至少终于有了工位。虽然环境简陋，但是不得不说，它比起宿舍，具有很好的学习氛围，学习的效率会提高很多。于是我也将自己的主机，显示屏，插线板全部搬到了工位，准备开始自己今后的学习。\n当然我也是有一部分的私心的，一点不知好歹自以为是的私心，不过这涉及到了其他人，所以在这里就暂且不表。今后的自己再看到的时候也一定不会忘记的。\n在这个月我彻底意识到科研院的工作是多么的累赘，进展地是如此的缓慢，幸好那里考核不严格，我才可以从中脱出身来，深耕其他。不过我还是不能落下科研院的工作，毕竟这是我自己未来的毕设，关乎着我的毕业。所以我学习了其中的docker,golang,k8s等知识，其中golang为我今后的经历埋下了伏笔。\n也是在这个月，我立下了“大干百天，我要成为golang高手”的豪言壮语。哈哈，现在想起来我也还没成为golang高手，但好在有所建树。\n10月 10月按照道理来讲是国庆节，是一个放松的日子，但是我是知道我自己的，如果没有人陪我一起，我在节假日是很难出去放松的，即使让我窝在宿舍打游戏也不会出去逛，对于这一点我给自己的理由是，不喜欢凑热闹。\n践行着自己成为golang高手的目标，我继续在工位雷打不动的学习，国庆节也就休息了两三天吧，从golang的基础知识到跟着两本英文书籍做一个golang的小项目，恍惚间，十月份过的如此之快。\n学习的同时当然少不了游戏，这个月如果我没有记错的话我从盗版网站上下载了黑魂3与只狼这两款游戏，前者是我在玩完法环后自然而然想要体验的，后者我的第一印象还在2019，2020年，各大LOL主播（因为我当时还很喜欢玩LOL）都在玩只狼，被虐的死去活来。完成法环和黑神话的我自然对这两款魂游充满向往，但是我承认我先选择了盗版游玩，因为自己当时并不想花那个不菲的流量钱（我们学校流量比较贵，如果退款那可是损失惨重）\n黑魂三在玩的时候由于下载的版本无法植入中文字幕，全英文，根本无法体验剧情，玩了不到五个小时就放弃了。还好只狼中有中文字幕，让我继续了下去。只狼的游玩一直持续到了十一月多，现在回想起来，只狼真是一个很神奇的游戏，乒乒乓乓的打击声让我对每一次的战斗都如痴如醉，我现在还在感叹，仅仅20多个G的游戏居然这么出色！甚至在我进行二周目游玩时我认为他的出色已经可以与黑神话分庭抗礼，甚至在玩法上高出一筹。\n11月 11月时也就是我大干百天的后期阶段了，这是的我虽然掌握了一些Golang的知识跟着书籍做了两个小项目，然后也在看Mysql的掘金小册学习对应的八股知识。但是还是会感到焦虑，因为明年的三月份就要投暑期实习了，届时我的履历会非常的单薄，没有实习经历，本科双非加跨考，这如果在简历的排序中一定是比较下等的了。\n加上刷牛客时老是刷到一些实习的帖子，在群友的经典言论“投简历的最好时机永远是现在”的鼓励下（我现在觉得这句话还是十分的重要，不管是为你带来勇气也好还是实际中的真实作用也罢），我梳理了自己的简历，把在科研院做的东西包装成了一个项目，但是我也知道，那里目前的进度仅限于自学，了解，离真正的实习还差的十万八千里。因为投的是golang简历，所以项目上我也没有写苍穹外卖和黑马点评，而是把之前的两个玩具Golang项目放了上去。\n之后我便开始在Boss上投简历，对于Golang的公司经常出现的大厂就是百度，滴滴，字节，因为字节有面评的关系，我不太敢投，所以投了许多百度的岗位。幸运的是，接到了一次面试的机会，这算是我对于大厂的第一次面试，半个处女面了，现在回想起来，当时的自己十分地紧张，语速稍快，特别是看到算法题的时候整个人都懵了，看了一分钟根本没有思路，好在在提示下完成了，后来回过神来就是一个还算平常的动态规划算法。\n幸运的是，一面通过了，可惜的是，在二面的前两天，HR告诉我岗位关闭了。当时我也是感叹，造化弄人啊，不过能有这样一次面试的机会也是十分感激了。自己还需努力罢了。\n这个月我又开始了一个新的游戏大作:博德之门3，关于博德之门3这部游戏我的总体评价是非常适合我的口味，毕竟我自己是比较喜欢玩回合制以及卡牌类型的游戏，例如我虽然不会去玩原神但我会去尝试崩坏星穹铁道，因为他是回合制。一直到这个月的结尾，我的一周目结束了，磕磕绊绊用了近80个小时，期间学到最大的本领是按快速保存按钮，每到一个重要的剧情或者地点之前，已经养成了自动保存的好习惯。\n就这样，在大干一百天的后期，伴随着每天晚上和周末的博德之门与只狼的游玩，度过了一个有些幸运又有些可惜的11月。\n12月 12月对我来说是2024年最棒的几个月之一，原因很简单，我收到了我真正意义上的第一份实习offer，还是来自于百度这样一家大公司的offer，虽然在我写这篇文章的时候我在公司的处境并不是那么的好，但是我依然非常感谢我的一面和二面的面试官，特别是我的二面面试官，也是我现在的领导。\n遥想11月的时候，抱着试一试的心态同时也为了检验自己的学习成果，我投递了这个岗位的offer。一面通过后二面推进的也挺快，谁曾想二面开始前被告知岗位关闭了，我猜测是有更优秀的候选人。但是12月的奇迹就在这时发生了，我的领导捞起了我的简历，特别是在捞的时候给我打了两次电话，第一次电话我都没有接到，所以无论怎样我对我的领导还是非常的感恩，因为我知道以自己的实力其实是够不到这样的公司的。\n第一次实习，真的是非常梦幻的感觉，来到了大名鼎鼎的西二旗，来到了家喻户晓的公司百度，走进这雄伟的行政大楼，一切都是那么的不真实（是的，就是这么刘姥姥进大观园的感受）。领到了四五年前的电脑，不过现在依然够用（感叹一下M系列的macbook的强大），来到了如此宽敞的L型工位，哇，awesome，与在学校或者科研所的生活完全不是一个量级，让我觉得自己是多么的幸运。\n第一周熟悉环境，有了自己的带教，接触了部门的成员与项目的代码，但是进入百度需要通过一个训练营的考核才可以进行编码等操作，所以第一周我就在这样一种新鲜感中度过了。这也为以后发生的问题埋下了伏笔。\n第二周时这种新鲜感已经少了很多了，我也通过了测试开始正式的接触整个项目的代码。果然，大公司的代码和自己学的项目区别还是很大的，实在是太庞杂了。在这一周我也接到了一个写脚本的需求，此时的我并没有太了解清楚整个项目的流程，对于带教的讲述也是半知半解迷迷糊糊。在写需求的同时也要去线上进行排查错误，在这期间我承认自己并没有之前那样的工作状态了，显得有些松散了。所以在这方面的工作做的并不是很好，在没有完全理解项目流程的情况下用了足足一周去写这个简单的脚本。\n到了十二月的末尾，一个对我来讲非常不好的事情发生了，我的带教被调到其他的团队去了，用他的原话来讲才带了我三四天就走了。一时间我的直系上司变成了我的领导，众所周知领导都是特别忙的，一般回复我的消息都是要两个小时左右，而且对于我这种实习的简单问题确实也是精力有限。所以我一时间变成了无头苍蝇，至于后续发生了什么，那就是今年的事情了，我也就暂时将十二月发生的事情写到这里。\n总之，十二月对我来说是一个还算美妙的月份，最重要的事情就是找到了百度心怡的实习offer，感恩且知足。\n","permalink":"http://localhost:1313/posts/011%E6%80%BB%E7%BB%932024/","summary":"\u003ch1 id=\"我的2024\"\u003e我的2024\u003c/h1\u003e\n\u003cp\u003e时光如梭啊，一转眼2025年都已经过了快两个月了，我才来写这一篇关于我的2024年总结，因为2024的结尾发生了一些事情吧。\u003c/p\u003e\n\u003cp\u003e往年都是以视频的形式，今年，以个人博客的方式来回顾吧。同样也会用回顾朋友圈的形式来辅助实现回顾。\u003c/p\u003e","title":"总结我的2024"},{"content":"前言 距离第一篇关于“戒用”AI的文章已经过去了将近三周了，也该回来好好说说自己关于AI的一些感悟了，当然还会夹杂关于其他事情的感悟。（所以这是一篇纯主观的文章，类似于流水账）\n让我们开始吧！\n戒用AI 关于“戒用”AI，我的选择是在有的领域戒用而有的领域没有戒用。\n写完第一篇文章后，我在头几天戒用了AI，特别是涉及到自己学习中的核心部分，比如自己学习的小项目遇到错误了，我尽力地去先用谷歌进行查询错误原因并使用断点调试来处理错误,确实能够感受到自己在处理错误方面比直接使用AI要好一些，因为可以先进行自己的思考，尝试先自己去解决出现的问题。\n那么我在什么方面没有戒用AI呢？关于文字生成方面，我并没有停止使用AI。这是一个矛盾的决定，一方面如果你现在给我关于文字的要求，比如写一段怎么样的话，我一时间毫无头绪，甚至有些拒绝思考，根本无法压抑直接去寻找AI帮助的欲望；另一方面，正如现在写博客一样，我认为书写文字的能力还是十分重要的，我不应该将这个能力全部丧失，将这个任务全部交给AI来处理。\n当然，在阅读英文文献时我也经常调用例如豆包这样的AI辅助工具，还是同一个矛盾点：使用AI来看文献确实很香，能够帮我看懂很多我看不懂的句子和单词，但是这对我的英文写作能力是否算是一种打击呢？\n最后，关于代码层面的使用我最终还是没有完全地戒掉，遇到的有些错误在谷歌上根本搜不出来，甚至类似的也没有那么类似（所以别再说你遇到的所有错误一定都是别人遇到过的，你在互联网上一定能够找到解决方案），我觉得关于代码错误，每个人有自己独特的错误，不同环境下的不同错误，千人千面。\n当遇到这种搜不到的错误时，真的会很手足无措，这也又引发出了我另一个矛盾：遇到这样的错误确实我可以仔细地阅读手册，弄懂自己涉及到的每一个细节，直到处理掉这个错误，但是这个过程到底会花多少时间，其中遇到的挫折是否会直接打消我学习下去的热情；还是说我直接请求AI来进行辅助，即使最终他也无法解决这个问题，但是至少会比前者花费更少的时间，带来的挫败感也没有那么强烈。（是的，前者同时也能学到更多的知识，而后者更多的是遵循着AI给出的步骤进行操作罢了）\n所以，我到底该如何面对AI呢？\n我自己面对AI的\u0026quot;规矩\u0026quot; 在感受了三周的挣扎矛盾之后，我认为AI，准确的说大语言模型，已经深深地嵌入了我的生活中，我无法完全地剥离它。但是为了避免出现第一篇文章中我自己所说的离开了AI什么也不会，在自己的核心竞争力编程领域被替代这些情况，我为自己对于大语言模型的使用设定了一些\u0026quot;规矩\u0026quot;:\n编程方面: 遇到问题后先进行STFW\u0026amp;RTFM，并进行断点调试（如果可以的话），尽量自己先去尝试解决问题，即使这会花费更多的时间，但是从一个未来的程序员的职业出发，我认为这是值得的。 在无法解决时寻找AI进行辅助，或许他会给出我自己没有想到的新思路。 写作方面: 我认为这一方面要加大力度使用AI，利用我自认为还不错的prompt来充分发挥其\u0026quot;语言\u0026quot;模型的能力，它只是我思想的代笔而已。 但是不能全部依赖它进行写作，要坚持写一些笔记和博客作为自己的文字输出，让自己感受文字的力量。 阅读方面: 在阅读文献时，没有必要一上来就用AI工具对全文进行翻译，自己先去阅读英文原文，遇到无法直接理解的语段使用工具辅助。 我能想到的规矩，目前就这么多了，但这也引发了我对大语言模型的更大的兴趣，今后在有空闲时间的日子里，我一定要去了解其中的原理（但不是从0开始那样搭建知识体系）和最新的发展状况。\n其他感悟 文章的最后，再说些题外话吧。在我宣布\u0026quot;大干100天\u0026quot;之后，没想到我真的可以在第一段实习就可以进入一家大厂，真的让我感叹自己的运气真的不错。明天就要去报道了，我现在还是忐忑不安的心情，因为自己的Golang水平真的算不上多好，毕竟我也只是从9月份才开始学习Golang，照着书本做了两三个玩具项目而已。\n但是机会已经摆在眼前了不是吗？现在再慷慨激昂的话我也说不出了，只能说，抓住眼前这千载难逢的机会吧！即使我现在是三线程作战的模式，不过，只有硬着头皮往前硬抗了，不是吗？\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/010aithought02/","summary":"\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cp\u003e距离第一篇关于“戒用”AI的文章已经过去了将近三周了，也该回来好好说说自己关于AI的一些感悟了，当然还会夹杂关于其他事情的感悟。（所以这是一篇纯主观的文章，类似于流水账）\u003c/p\u003e","title":"AIThought02_三周过后，我对于AI的使用习惯发生了什么样的变化"},{"content":"前言 做完了三个Golang小的项目之后，发现自己很难复现出来，所以我反思了一下对于这些项目我要进行一个逐一的复盘，从短链接到GreenLight电影搜索API项目，忘了的部分就再回去看书，我对于自己的告诫是永远不要认为任何东西看一遍就万事大吉了。\n牢骚发完了，我们来看看这个项目最终实现的后端是什么效果，通常我们在浏览网页时会遇到一些长链接，复制到其他地方会很不方便——比如我在微信朋友圈分享自己的博客某个链接会因为太长只识别到前面的字符，后面的字符当作普通的文字了，导致不能直接点击跳转。所以我们使用这个短链接生成工具生成一个短的链接。\n接下来是展示示例:\n可以发现我们将B站视频中的长链接作为origin_url并且为其自定义了custom_code为\u0026quot;text\u0026quot;,利用短链接生成器生成了下方的一个短链接\u0026quot;http://localhost:8888/text\u0026quot;,使用这个短链接就可以访问刚才的网站（当然这个localhost:8888后续我会想着更改为一个实际的IP地址，这样就可以做到不止自己一个人使用）\n好了看完这个示例就让我们进入项目的内部一探究竟。\n本次开发环境为本地Macos15.1,Go1.23.1,使用GolandIDE进行开发。\n项目深究 本次项目深究将从项目的流程图开始，再进入其中的具体部分的逻辑以及代码。\n下面是整个架构的简单示意图:\n数据库迁移 首先新建database文件夹，在其下建立migrate文件夹——这个database文件夹用来存放所有的数据库SQL语句。\n数据库驱动我们使用Postgresql,并且使用docker启动，启动过程中如果遇到本地冲突详见这一篇文章\n我们将相关的命令写入到Makefile中\n.PHONY: lanch_postgres lanch_postgres: docker run --name postgres_urls \\ -e POSTGRES_USER=LTX \\ -e POSTGRES_PASSWORD=iutaol123 \\ -e POSTGRES_DB=urldb \\ -p 5432:5432 \\ -d postgres 命令行执行make lanch_postgres后成功启动一个Docker容器搭载Postgresql数据库，可以进入容器中进行测试。\n接着我们需要进行数据库的迁移，这里要使用到go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@latest这个迁移工具，可能你会问为什么要进行数据库迁移:\n版本控制数据库结构,可以使用migrate命令进行回滚 自动化数据库变更 简化部署流程 执行命令migrate create -seq -ext=.sql -dir=./database/migrate init_schema，会在我们的database/migrate路径下生成两个sql文件:\n我们将建urls表的SQL语句写在up中\nCREATE TABLE IF NOT EXISTS urls ( \u0026#34;id\u0026#34; BIGSERIAL PRIMARY KEY, \u0026#34;original_url\u0026#34; TEXT NOT NULL, \u0026#34;short_code\u0026#34; TEXT NOT NULL UNIQUE, \u0026#34;is_custom\u0026#34; BOOLEAN NOT NULL DEFAULT FALSE, \u0026#34;expired_at\u0026#34; TIMESTAMP NOT NULL, \u0026#34;created_at\u0026#34; TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ); 这个表的字段包括\n主键id，BIGSERIAL自动序列化递增 原始的url，TEXT 用于存储可变长度的字符串，没有长度限制 独特的短码，可以由用户输入，作为短url中的后缀例如:http:/localhost:8888/short_code 是否由用户自定义，即上方短码是否由用户输入 短url的过期时间 创建时间 同时我们为short_code和expired_at创建二级索引\n因为short_code是UNIQUE的。 需要定期检查短链接是否过期，所以设置索引加速查询速度。 CREATE INDEX idx_short_code ON urls(short_code); CREATE INDEX idx_expired_at ON urls(expired_at); ","permalink":"http://localhost:1313/posts/009golang%E9%A1%B9%E7%9B%AE%E6%A2%B3%E7%90%86%E7%9F%AD%E9%93%BE%E6%8E%A5%E9%A1%B9%E7%9B%AE01/","summary":"\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cp\u003e做完了三个Golang小的项目之后，发现自己很难复现出来，所以我反思了一下对于这些项目我要进行一个逐一的复盘，从短链接到GreenLight电影搜索API项目，忘了的部分就再回去看书，我对于自己的告诫是永远不要认为任何东西看一遍就万事大吉了。\u003c/p\u003e","title":"009Golang项目梳理:短链接项目01"},{"content":"前言 欢迎来到我的Debug系列第一集，我目前还没有完全想好这个系列该怎么去组织规划，但是我想记录下来遇到的大大小小的bug以及debug的过程应该于我自己而言是一件蛮有意思的事情。\n即使人们常说我们所遇到的bug别人一定都遇到过，去Google一定都能解决，但是如果遇到相同bug的人并没有发表于网络呢？那我们可能真的搜索不到这个bug。并且每个人的操作环境总会有大大小小的差异，所以我还是认为每个人的每个bug都是独一无二的，要说搜索解决方法，不如说是去搜索类似的问题来启发自己。\n所以，让我们来记录这一切，万一哪天能启发到别人呢？让自己感受文字的温暖吧。\nPostgresql的连接问题 问题背景 我在启动一个新的项目时，使用docker启动Postgresql，使用了下方这样一个docker命令写在了我的Makefile中。\nlanch_postgres: docker run --name postgres_urls \\ -e POSTGRES_USER=LTX \\ -e POSTGRES_PASSWORD=Lutaol123 \\ -e POSTGRES_DB=urldb \\ -p 5432:5432 \\ -d postgres 命令的含义是运行一个名为postgres_urls的容器，特权用户为LTX（这里直接使用特权用户操作数据库了），密码为Lutaol123,数据库名称为urldb,端口映射主机的5432映射到容器中的5432端口，-d使容器在后台持续运行——以postgres这个镜像为基础的容器。\n执行make lanch_postgres后成功启动了这个容器，接着来到数据库迁移（使数据库操作变得可以回滚，变得像Git一样的可以进行版本控制）具体如何迁移见官方链接，最终效果就是在容器中的数据库执行了一条建表的语句。\n这一阶段的makefile如下:\ndatabaseURL=\u0026#34;postgres://LTX:password@localhost:5432/urldb?sslmode=disable\u0026#34; migrate_up: migrate -path=\u0026#34;./database/migrate\u0026#34; -database=${databaseURL} up 可是执行makefile，make migrate_up时报错:\npostgres://LTX:Lutaol123@localhost:5432/urldb?sslmode=disable migrate -path=\u0026#34;./database/migrate\u0026#34; -database=postgres://LTX:Lutaol123@localhost:5432/urldb?sslmode=disable up error: failed to open database, \u0026#34;postgres://LTX:Lutaol123@localhost:5432/urldb?sslmode=disable\u0026#34;: pq: role \u0026#34;LTX\u0026#34; does not exist make: *** [migrate_up] Error 1 问题处理过程 错误信息 错误信息是打开数据库失败，\u0026ldquo;LTX\u0026quot;这个用户不存在。\n经检查可以发现path是没有问题的，意味着我们写的数据库连接URL是正确的。而最后的pq: role \u0026quot;LTX\u0026quot; does not exist才是真正的问题所在。\n但是上面我们使用docker创建这个Postgresql容器时不是成功了吗？这里怎么又不存在了呢？\n尝试解决 检查makefile中的命令有无错误 通过错误信息可以得知，我们的路径信息都是正确的，即迁移命令和docker命令中的信息是吻合的。 所以一定不是这里的问题。 检查Docker容器 这里我们取巧，直接去Dockerdesktop桌面化环境中去检查，打开指定的容器后，输入命令psql -U LTX -d urldb进入到数据库中，发现进入成功了？那用户怎么会不存在呢？ 再执行一系列的Postgresq命令检查当前连接信息\\conninfo显示正常连接, 检查数据库信息\\l显示所有的数据库发现存在urldb; 但是查询数据库中的所有表信息时出现了错误，\\dt显示并没有表？ 所以，很明显迁移命令没有执行成功，因为表都没有。\n看到这里你也许会有疑惑，为什么数据库创建好了呢？\n这和最初的docker命令有关，指定了POSTGRES_DB和POSTGRES_USER之后容器启动时会自动创建我们所指定的数据库和超级用户。 可以使用\\du命令检查用户权限 最终解决 那到底为什么没有连接成功呢？我突然一想，自己之前做过一个项目也是使用的Postgres，并且我是在本地使用的，不会这个迁移命令给我连接到了本地了吧?!\n# 进入本地的数据库 psql postgres # 检查当前数据库信息 \\l 坏啦！怎么本地数据库里面有urldb这个数据库啊!那一定是连接到本地了（因为我之前并没有操作本地Postgres）。再连接这个数据库查查表信息确认一下。\n# 连接urldb数据库并检查表信息 \\c urldb \\dt 这里Owner变成了ltx_urldb是因为后来我改了一次用户名（因为当初我还以为是用户名的问题）\n哦豁，真的是迁移到本地的Postgresql中了，并没有去连容器中的。\n错误原因 那到这里，错误原因也很明显了，就是databaseURL=\u0026quot;postgres://LTX:password@localhost:5432/urldb?sslmode=disable\u0026quot;这个数据库链接指向的是localhost，如果本地的运行着，自然就连接到本地了。\n这里做个回调小实验，将本地的关闭然后再次尝试向容器中迁移。\n# 因为我是用homebrew安装的 brew services stop postgresql 再次执行迁移命令发现成功了\n进入容器中也查到了相应的表。\n总结 一个localhost引发的本地数据库和容器数据库之间的冲突。\n其中更底层的原因我询问了GPT:\nPostgreSQL 默认在 /var/run/postgresql/.s.PGSQL. 路径监听 Unix 套接字,如果 PostgreSQL 客户端检测到本地套接字文件，就会优先通过它连接。 如果本地套接字不可用，则尝试通过 localhost:5432 使用 TCP/IP 连接。并使用端口映射到容器中的5432端口。 好的到这里我们的这次Postgresql的容器连接问题就到这里结束了。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/008debug%E7%B3%BB%E5%88%9701-postgresql/","summary":"\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cp\u003e欢迎来到我的Debug系列第一集，我目前还没有完全想好这个系列该怎么去组织规划，但是我想记录下来遇到的大大小小的bug以及debug的过程应该于我自己而言是一件蛮有意思的事情。\u003c/p\u003e","title":"DebugEP01——Postgresql"},{"content":"前言 让我们看看关于 chan 有哪些常见的操作。\n创建channel ch1 := make(chan int) ch2 := make(chan int, 2) 底层实际上调用的是makechan方法\n发送数据到channel ch \u0026lt;- 1 底层实际调用的是chansend1,而chansend1最终也是调用chansend,将block参数设置为true——当前发送操作是阻塞的\n从channel中读取数据 i \u0026lt;- ch i, ok \u0026lt;- ch 底层实际调用的是chanrecv1和chanrecv2，最终都去调用了chanrecv。\n源码 接下来我们进入源码中一探究竟。\nhchan结构体 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 synctest bool // true if created in a synctest bubble closed uint32 timer *timer // timer feeding this chan elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G\u0026#39;s status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } qcount:通道中存储的数据元素总数 dataqsiz:环形队列大小 buf:执行环形队列的内存缓冲区，存放实际数据——环形缓存区域，本质上是一个带有头尾指针的固定长度的数组 sendx,recvx:发收操作的队列位置 recvq,sendq:等待队列 可以发现，hchan 使用环形队列表示缓冲区并且采用 lock 确保并发访问的安全性。\nwaitq队列 type waitq struct { first *sudog last *sudog } 管理等待发送或接受的 goroutine 队列——存储阻塞在通道上的 goroutine 信息。 可以实现快速插入last和删除first，如果通道阻塞，goroutine会封装为 sudog 并挂入 waitq 队列中。\nsudog结构体 type sudog struct { g *g next *sudog prev *sudog elem unsafe.Pointer // data element (may point to stack) acquiretime int64 releasetime int64 ticket uint32 isSelect bool success bool waiters uint16 parent *sudog // semaRoot binary tree waitlink *sudog // g.waiting list or semaRoot waittail *sudog // semaRoot c *hchan // channel } 用于管理 Goroutine 在通道操作时的阻塞与唤醒操作,sudog 充当连接 Goroutine 和通道的桥梁\nc *hchan:表示当前阻塞的通道 elem:指向与通道操作相关的元素，例如保存发送数据 next,prev:waitq 的双向链表中的 sudog 节点 waitlink,waittail:用于 semaRoot（信号量队列）的队列链接。 parent:用于 semaRoot 的二叉树链接 g:当前阻塞的 Goroutine 对象 isSelect:当前 sudog 是否参与了 select 操作，可能存在唤醒竞争问题 success:当前通道操作是否完成 acquiretime,releasetime:sudog 被加入的时间和被唤醒的时间 可以发现有一些是跟信号量有关的操作semaRoot，是同步操作，与本节关系不太大。\n至此我们可以得知，hchan管理通道的核心状态，waitq双向链表作为等待队列连接多个阻塞的Goroutine,这些Goroutine由sudog记录，专门用于在 Goroutine 被挂起时保存其状态，并将其与特定资源（例如通道、锁或其他同步原语）关联起来。\n作为发送方，检查缓冲区是否未满，如果没满就写入缓冲；如果满了就阻塞到waitq中；如果由接收方等待，则唤醒接收方 作为接收方，检查缓冲区是否非空，如果非空直接读取数据，如果为空阻塞waitq；如果有发送者，唤醒发送方。 enqueue func (q *waitq) enqueue(sgp *sudog) { sgp.next = nil x := q.last if x == nil { sgp.prev = nil q.first = sgp q.last = sgp return } sgp.prev = x x.next = sgp q.last = sgp } 入队操作，将一个 sudog（阻塞的 goroutine 结构）添加到 waitq 队列中——通道在运行时实现发送和接受阻塞的核心机制：\n获取当前队列尾节点，如果为空意味着队列为空，将sudog作为第一个节点同时也是最后一个节点 如果队列非空，改变prev为当前尾节点并将sudog作为新的队尾。 dequeue func (q *waitq) dequeue() *sudog { for { sgp := q.first if sgp == nil { return nil } y := sgp.next if y == nil { q.first = nil q.last = nil } else { y.prev = nil q.first = y sgp.next = nil // mark as removed (see dequeueSudoG) } if sgp.isSelect { if !sgp.g.selectDone.CompareAndSwap(0, 1) { // We lost the race to wake this goroutine. continue } } return sgp } } 出队操作，返回被移除的 sudog\n如果队列头为空，意味着队列为空 如果只有一个结点，移除后队列设置为空 如果有多个节点，移除头结点并更新队列（这里的核心逻辑都是找到第二个节点） 如果该 goroutine 是由于select放入队列，则需要检查是否有竞争条件——看是否有其他的 goroutine 先一步唤醒了该 sgp，如果是则不能将其移除，继续尝试出队下一个结点。 makechan()函数 func makechan(t *chantype, size int) *hchan { elem := t.Elem // compiler checks this but be safe. if elem.Size_ \u0026gt;= 1\u0026lt;\u0026lt;16 { throw(\u0026#34;makechan: invalid channel element type\u0026#34;) } // 检查通道结构体是否正确对齐，保证运行时安全性 if hchanSize%maxAlign != 0 || elem.Align_ \u0026gt; maxAlign { throw(\u0026#34;makechan: bad alignment\u0026#34;) } mem, overflow := math.MulUintptr(elem.Size_, uintptr(size)) if overflow || mem \u0026gt; maxAlloc-hchanSize || size \u0026lt; 0 { panic(plainError(\u0026#34;makechan: size out of range\u0026#34;)) } var c *hchan switch { case mem == 0: // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case !elem.Pointers(): c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.Size_) c.elemtype = elem c.dataqsiz = uint(size) if getg().syncGroup != nil { c.synctest = true } lockInit(\u0026amp;c.lock, lockRankHchan) if debugChan { print(\u0026#34;makechan: chan=\u0026#34;, c, \u0026#34;; elemsize=\u0026#34;, elem.Size_, \u0026#34;; dataqsiz=\u0026#34;, size, \u0026#34;\\n\u0026#34;) } return c } 根据给定的通道类型和大小，初始化一个通道的底层结构体 hchan 并分配必要的内存.\n1 验证通道中的元素类型和通道对齐 2 计算缓冲区大小:元素大小 × 容量，检查是否溢出或超出允许的最大分配值 3 分配内存并初始化: 如果缓冲区为0只分配hchan的结构体内存； 如果存储元素不包含指针，将hchan和缓冲区内存一次性分配在一起； 如果存储元素包含指针，分开分配，保证垃圾回收器能够正确追踪指针（详情见垃圾回收博文） 4 初始化通道，设置元素大小，类型，通道容量，初始化锁 5 最终返回通道hchan指针 send()函数 func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if c.synctest \u0026amp;\u0026amp; sg.g.syncGroup != getg().syncGroup { unlockf() panic(plainError(\u0026#34;send on synctest channel from outside bubble\u0026#34;)) } if raceenabled { if c.dataqsiz == 0 { racesync(c, sg) } else { // Pretend we go through the buffer, even though // we copy directly. Note that we need to increment // the head/tail locations only when raceenabled. racenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz } } if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } 处理数据从发送端到接收端的实际传输\n1 进行调试和同步测试，确保通道的发送和接收在相同的同步组中 2 触发数据竞争检测:如果无缓冲，调用racesync同步收发；如果有缓冲，通过racenotify模拟数据传输和缓冲区索引更新 3 调用sendDirect将发送者sudog的数据直接复制到接收者Gorontine的sudog结构中 4 标记发送成功，记录性能追踪的释放时间，调用goready唤醒接收者Goroutine chansend()函数 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceBlockForever, 2) throw(\u0026#34;unreachable\u0026#34;) } if debugChan { print(\u0026#34;chansend: chan=\u0026#34;, c, \u0026#34;\\n\u0026#34;) } if raceenabled { racereadpc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(chansend)) } if c.synctest \u0026amp;\u0026amp; getg().syncGroup == nil { panic(plainError(\u0026#34;send on synctest channel from outside bubble\u0026#34;)) } if !block \u0026amp;\u0026amp; c.closed == 0 \u0026amp;\u0026amp; full(c) { return false } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;send on closed channel\u0026#34;)) } if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } if c.qcount \u0026lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { racenotify(c, c.sendx, nil) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026amp;c.lock) return true } if !block { unlock(\u0026amp;c.lock) return false } // Block on the channel. Some receiver will complete our operation for us. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) gp.parkingOnChan.Store(true) reason := waitReasonChanSend if c.synctest { reason = waitReasonSynctestChanSend } gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), reason, traceBlockChanSend, 2) KeepAlive(ep) // someone woke us up. if mysg != gp.waiting { throw(\u0026#34;G waiting list is corrupted\u0026#34;) } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) if closed { if c.closed == 0 { throw(\u0026#34;chansend: spurious wakeup\u0026#34;) } panic(plainError(\u0026#34;send on closed channel\u0026#34;)) } return true } 本方法向c目标通道发送数据ep，并标志是否采用阻塞方式，记录调用方的pc。返回发送成功或者失败\n1 通道为nil的处理 2 竞态检测 3 通道未关闭且已满 4 获取通道互斥锁，如果通道已关闭立即解锁并抛出panic 5 优先处理接收处理:如果有Goroutine正在等待接收，直接调用send发送——会绕过缓冲区 6 如果缓冲区未满，将数据写入缓冲区 7 如果是阻塞模式（没有等待的接收者且缓存区已满或无缓存），那么当前Goroutine会阻塞，创建一个sudog结构体mysg记录发送者的信息，直到接收者处理当前发送操作。 8 如果发送者被唤醒，验证状态正常后进行资源的清理释放上面的sudog避免内存泄漏；并进行性能统计记录阻塞时间。 总结来看，一共分为三种可能的发送方式\n同步发送（有正在等待的） 异步发送（写入缓存） 阻塞发送（放入wait队列中） chanrecv()函数 异常检查 func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { if debugChan { print(\u0026#34;chanrecv: chan=\u0026#34;, c, \u0026#34;\\n\u0026#34;) } if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceBlockForever, 2) throw(\u0026#34;unreachable\u0026#34;) } if c.synctest \u0026amp;\u0026amp; getg().syncGroup == nil { panic(plainError(\u0026#34;receive on synctest channel from outside bubble\u0026#34;)) } if c.timer != nil { c.timer.maybeRunChan() } if !block \u0026amp;\u0026amp; empty(c) { if atomic.Load(\u0026amp;c.closed) == 0 { return } if empty(c) { // The channel is irreversibly closed and empty. if raceenabled { raceacquire(c.raceaddr()) } if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } ....... } 如果通道为空且不阻塞，立即返回；如果通道为空且阻塞，不能从空的通道接收会直接挂起 同步组校验与定时器检查 进行快速路径检查（这里有些模糊） 同步接收 var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } lock(\u0026amp;c.lock) if c.closed != 0 { if c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } else { if sg := c.sendq.dequeue(); sg != nil { recv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true, true } } 对通道加锁 如果通道关闭且缓冲区为空，如不符合接收recv请求的状态，直接返回。 如果sendq中有sudog等待的发送者，调用recv函数完成同步接收 异步接收 if c.qcount \u0026gt; 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026amp;c.lock) return true, true } 如果缓冲区有数据，从缓冲区根据recvx索引读取 更新通道的信息（索引，缓冲区数据数量） 返回true，表示接收成功 阻塞接收 if !block { unlock(\u0026amp;c.lock) return false, false } gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) if c.timer != nil { blockTimerChan(c) } gp.parkingOnChan.Store(true) reason := waitReasonChanReceive if c.synctest { reason = waitReasonSynctestChanReceive } gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), reason, traceBlockChanRecv, 2) 如果sendq中没有待发送的goroutine且缓冲区为空，即上面那两个if都不满足，则进入阻塞阶段。 与chansend中逻辑相似，将当前goroutine休眠并创建一个新的sudog保存被挂起的goroutine当前状态。 // someone woke us up if mysg != gp.waiting { throw(\u0026#34;G waiting list is corrupted\u0026#34;) } if c.timer != nil { unblockTimerChan(c) } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg) return true, success 被调度器唤醒后完成阻塞接收，进行参数检查，解除通道的绑定并释放创建的这个sudog。\n总结 参考资料 https://github.com/golang/go/blob/master/src/runtime/chan.go#L176 https://juejin.cn/post/7125610595801530376#heading-4 ","permalink":"http://localhost:1313/posts/007golangep02_%E5%B9%B6%E5%8F%9102chan/","summary":"\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cp\u003e让我们看看关于 chan 有哪些常见的操作。\u003c/p\u003e\n\u003ch3 id=\"创建channel\"\u003e创建channel\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ech1\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"nb\"\u003emake\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kd\"\u003echan\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ech2\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"nb\"\u003emake\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kd\"\u003echan\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e底层实际上调用的是\u003ca href=\"/posts/007golangep02_%E5%B9%B6%E5%8F%9102chan/#makechan%e5%87%bd%e6%95%b0\"\u003emakechan\u003c/a\u003e方法\u003c/p\u003e\n\u003ch3 id=\"发送数据到channel\"\u003e发送数据到channel\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ech\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e底层实际调用的是\u003ca href=\"/posts/007golangep02_%E5%B9%B6%E5%8F%9102chan/#chansend%e5%87%bd%e6%95%b0\"\u003echansend1\u003c/a\u003e,而chansend1最终也是调用\u003ca href=\"/posts/007golangep02_%E5%B9%B6%E5%8F%9102chan/#chansend%e5%87%bd%e6%95%b0\"\u003echansend\u003c/a\u003e,将block参数设置为true——当前发送操作是\u003cstrong\u003e阻塞的\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"从channel中读取数据\"\u003e从channel中读取数据\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ei\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"nx\"\u003ech\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003eok\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"nx\"\u003ech\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e底层实际调用的是\u003ca href=\"/posts/007golangep02_%E5%B9%B6%E5%8F%9102chan/#chanrecv%e5%87%bd%e6%95%b0\"\u003echanrecv1\u003c/a\u003e和chanrecv2，最终都去调用了chanrecv。\u003c/p\u003e\n\u003ch2 id=\"源码\"\u003e源码\u003c/h2\u003e\n\u003cp\u003e接下来我们进入源码中一探究竟。\u003c/p\u003e\n\u003ch3 id=\"hchan结构体\"\u003ehchan结构体\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e \u003cspan class=\"nx\"\u003ehchan\u003c/span\u003e \u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003eqcount\u003c/span\u003e   \u003cspan class=\"kt\"\u003euint\u003c/span\u003e           \u003cspan class=\"c1\"\u003e// total data in the queue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003edataqsiz\u003c/span\u003e \u003cspan class=\"kt\"\u003euint\u003c/span\u003e           \u003cspan class=\"c1\"\u003e// size of the circular queue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003ebuf\u003c/span\u003e      \u003cspan class=\"nx\"\u003eunsafe\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ePointer\u003c/span\u003e \u003cspan class=\"c1\"\u003e// points to an array of dataqsiz elements\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003eelemsize\u003c/span\u003e \u003cspan class=\"kt\"\u003euint16\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003esynctest\u003c/span\u003e \u003cspan class=\"kt\"\u003ebool\u003c/span\u003e \u003cspan class=\"c1\"\u003e// true if created in a synctest bubble\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003eclosed\u003c/span\u003e   \u003cspan class=\"kt\"\u003euint32\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003etimer\u003c/span\u003e    \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003etimer\u003c/span\u003e \u003cspan class=\"c1\"\u003e// timer feeding this chan\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003eelemtype\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003e_type\u003c/span\u003e \u003cspan class=\"c1\"\u003e// element type\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003esendx\u003c/span\u003e    \u003cspan class=\"kt\"\u003euint\u003c/span\u003e   \u003cspan class=\"c1\"\u003e// send index\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003erecvx\u003c/span\u003e    \u003cspan class=\"kt\"\u003euint\u003c/span\u003e   \u003cspan class=\"c1\"\u003e// receive index\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003erecvq\u003c/span\u003e    \u003cspan class=\"nx\"\u003ewaitq\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// list of recv waiters\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003esendq\u003c/span\u003e    \u003cspan class=\"nx\"\u003ewaitq\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// list of send waiters\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"c1\"\u003e// lock protects all fields in hchan, as well as several\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"c1\"\u003e// fields in sudogs blocked on this channel.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"c1\"\u003e//\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"c1\"\u003e// Do not change another G\u0026#39;s status while holding this lock\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"c1\"\u003e// (in particular, do not ready a G), as this can deadlock\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"c1\"\u003e// with stack shrinking.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"nx\"\u003elock\u003c/span\u003e \u003cspan class=\"nx\"\u003emutex\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eqcount:通道中存储的数据元素总数\u003c/li\u003e\n\u003cli\u003edataqsiz:环形队列大小\u003c/li\u003e\n\u003cli\u003ebuf:执行环形队列的内存缓冲区，存放实际数据——环形缓存区域，本质上是一个带有头尾指针的固定长度的数组\u003c/li\u003e\n\u003cli\u003esendx,recvx:发收操作的队列位置\u003c/li\u003e\n\u003cli\u003erecvq,sendq:等待队列\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以发现，hchan 使用\u003cstrong\u003e环形队列表示缓冲区\u003c/strong\u003e并且采用 \u003cstrong\u003elock 确保并发访问的安全性\u003c/strong\u003e。\u003c/p\u003e","title":"GolangEP02_并发中的Channel"},{"content":"前言 Gorm框架介绍 日常使用 使用注意事项 方法声明顺序 查询举例：\ndb.Where(maps).Offset(pageNum).Limit(pageSize).Find(\u0026amp;tags) db.Select(\u0026#34;id\u0026#34;).Where(\u0026#34;name = ?\u0026#34;, name).First(\u0026amp;tag) 注意这几个方法的调用顺序不能颠倒，因为GORM 按链式调用的顺序逐步构建 SQL 查询。所以在Gorm中查询常见的顺序是——SELECT语句最先，条件方法其次，分页时先用Offset再用Limit，最后执行查询方法例如Find、First、Create、Delete。\n源码 来自于GORMv1.9.16版本（并非最新版本）\nOpen() 首先看到源码中的注释\nOpen initialize a new db connection, need to import driver first\n意味着需要先import一个数据库驱动例如mysql\n然后我们拿一个普通的调用作为示例 db, err := gorm.Open(\u0026quot;mysql\u0026quot;, \u0026quot;user:password@/dbname?charset=utf8\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;)\n进入源码我们分层次来看\n第一部分 func Open(dialect string, args ...interface{}) (db *DB, err error) 传入数据库类型和源信息参数，返回一个db连接池和错误\nif len(args) == 0 { err = errors.New(\u0026#34;invalid database source\u0026#34;) return nil, err } var source string var dbSQL SQLCommon var ownDbSQL bool 先判断是否传入了数据库源参数，如果没有传入则返回错误；其次声明了三个变量：\nsource:数据库连接的源信息，从args提取 dbSQL:是一个接口的实例——标准的*sql.DB对象,其中SQLCommon的注释为 SQLCommon is the minimal database connection functionality gorm requires. Implemented by *sql.DB.\n抽象底层数据库连接对象 ownDbSQL:表示dbSQL是否由当前GORM实例独立管理 第二部分 这里是一个switch分支，通过判断传入的args第一个参数类型\nswitch value := args[0].(type) { case string: var driver = dialect if len(args) == 1 { source = value } else if len(args) \u0026gt;= 2 { driver = value source = args[1].(string) } dbSQL, err = sql.Open(driver, source) ownDbSQL = true case SQLCommon: dbSQL = value ownDbSQL = false default: return nil, fmt.Errorf(\u0026#34;invalid database source: %v is not a valid type\u0026#34;, value) } string时:将驱动设置为dialect 如果只有一个参数，那么就是源信息；如果不止一个参数，那么第一个作为驱动，第二个作为源 使用标准库sql打开数据库连接，传入驱动与源 并将ownDbSQL设置为true，表示当前实例需要管理这个数据库连接的生命周期 SQLCommon时:说明传入的是一个已经初始化好的数据库连接对象。 将传入的连接对象直接赋值给 dbSQL 标记为 false，表示传入的连接由外部管理，GORM 不负责其生命周期 二者都不是时，参数无效 第三部分 db = \u0026amp;DB{ db: dbSQL, logger: defaultLogger, callbacks: DefaultCallback, dialect: newDialect(dialect, dbSQL), } db.parent = db if err != nil { return } // Send a ping to make sure the database connection is alive. if d, ok := dbSQL.(*sql.DB); ok { if err = d.Ping(); err != nil \u0026amp;\u0026amp; ownDbSQL { d.Close() } } return 创建DB的实例\n将数据库连接对象 dbSQL 赋值给 DB 的 db 字段，作为后续 SQL 操作的基础 设置默认日志器，用于记录 SQL 执行信息 设置默认回调函数集，用于拦截和处理数据库操作（如查询、插入等） 初始化数据库dialect对象，用于适配不同的数据库（如 MySQL、PostgreSQL）。newDialect 根据传入的 dialect 名称和连接对象 dbSQL，返回一个实现特定dialect功能的实例 为了支持链式调用，将自身作为 parent。 这一顿属性填充完后，进行了一次连接验证:\n使用类型断言检查是否为标准的sql.DB类型 如果是，进行连接验证——如果ping出错并且由GORM管理生命周期，则关闭连接。 至此，Open方法代码结束了，总结一下就是声明三个变量来存储数据库的有关信息；根据源参数确定driver,source,dbSQL,ownDbSQL这些内容；最后创建好连接对象并使用ping检查连接是否成功。\nOpen提供了两种初始化方式\n直接传入驱动名称和连接信息 传入已经初始化好的数据库对象（自定义连接池） Open支持多种数据库，通过dialect对象统一处理差异。\nSQLCommon,SqlDb,sqlTx接口 SQLCommon 从上面的Open代码可以看出，这个SQLCommon比较重要 type SQLCommon interface { Exec(query string, args ...interface{}) (sql.Result, error) Prepare(query string) (*sql.Stmt, error) Query(query string, args ...interface{}) (*sql.Rows, error) QueryRow(query string, args ...interface{}) *sql.Row } 其抽象了底层的数据库操作，如执行非查询的语句，预编译查询，执行结果为多行的查询，执行结果为某行的查询。\nsqlDb type sqlDb interface { Begin() (*sql.Tx, error) BeginTx(ctx context.Context, opts *sql.TxOptions) (*sql.Tx, error) } 抽象了对数据库连接对象（通常是 *sql.DB）的事务操作方法\n开启一个新事务 开启一个带有上下文和配置的事务 这里的上下文我们在之前的Context源码中提到过，提供了取消和超时的能力，允许对长时间运行的事务进行控制。 opts指定事务的隔离级别 sqlTx 抽象了事务对象（*sql.Tx）的控制方法\ntype sqlTx interface { Commit() error Rollback() error } 显然有着提交和回滚事务两个方法。\n","permalink":"http://localhost:1313/posts/006golanggorm%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81/","summary":"\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003ch2 id=\"gorm框架介绍\"\u003eGorm框架介绍\u003c/h2\u003e\n\u003ch1 id=\"日常使用\"\u003e日常使用\u003c/h1\u003e\n\u003ch2 id=\"使用注意事项\"\u003e使用注意事项\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e方法声明顺序\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e查询举例：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003edb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eWhere\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003emaps\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003eOffset\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003epageNum\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003eLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003epageSize\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003eFind\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nx\"\u003etags\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003edb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eSelect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003eWhere\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;name = ?\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003eFirst\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nx\"\u003etag\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e注意这几个方法的调用顺序不能颠倒，因为GORM 按\u003cstrong\u003e链式调用的顺序逐步构建 SQL 查询\u003c/strong\u003e。所以在Gorm中查询常见的顺序是——\u003cem\u003eSELECT语句最先，条件方法其次，分页时先用Offset再用Limit，最后执行查询方法例如Find、First、Create、Delete。\u003c/em\u003e\u003c/p\u003e","title":"Golang源码小试牛刀:Gorm框架源码小探01"},{"content":"引子 今天心血来潮，给自己梳理梳理golang中这个十分重要的东东——context，毕竟也是在面试中考察到，但是反问自己的时候却感觉一点也说不出来什么有价值的东西，所以今天我来一次刨根究底。\n源码解析 Package context defines the Context type, which carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes.\n这是来自于官方的最新说明，意味着 context 携带终止期限，取消信号，以及跨API，进程之间通信等信息。\n即context设计的核心目标是\n任务取消机制：跨API和Goroutine的任务取消信号传播机制 超时控制：超时或截止时间控制任务生命周期 数据共享：在不同函数调用间共享元数据 明确了这些，我们进入到源码中。\nContext接口及错误变量 下面这段是来自于Chatgpt的精简版（去掉了长长的英文注释），可以发现Context是一个接口，其中包含了四个方法等待实现。context.Context\n其中第二个比较特殊，返回的是一个channel。\ntype Context interface { Deadline() (deadline time.Time, ok bool) // 返回任务截止时间 Done() \u0026lt;-chan struct{} // 返回取消信号的 channel Err() error // 返回上下文取消的原因 Value(key any) any // 获取上下文中的值 } Done 返回一个仅接收的内部通道，当取消信号发到通道后，此通道得关闭，同时context取消； 关闭通道是唯一一个所有消费者 goroutine 都能够感知到的通道操作 case \u0026lt;-ctx.Done() 如果 Done 通道还未关闭，Err 返回 nil；如果关闭了，就会返回其关闭原因，具体见下面的两个错误类型变量 ctx.Err() 说到这里我们需要补充一下，整个context包中一共有以下这些上下文类型,接下来就会根据这几种类型分别阐述。\nemptyCtx cancleCtx timerCtx valueCtx 以及cancleCtx下的一系列衍生体 定义完了接口，接下来是两个错误类型变量:\nvar Canceled = errors.New(\u0026#34;context canceled\u0026#34;) var DeadlineExceeded error = deadlineExceededError{} // type deadlineExceededError struct{} func (deadlineExceededError) Error() string { return \u0026#34;context deadline exceeded\u0026#34; } func (deadlineExceededError) Timeout() bool { return true } func (deadlineExceededError) Temporary() bool { return true } 根据源码中的注释可以得知\nCanceld是上下文被取消时的错误类型，是一个简单的 error 对象，由 errors.New 创建。 DealineExceeded表示上下文超出截止时间的错误类型，具体实现是 deadlineExceededError，一个实现了 error 接口的结构体，可以为这个错误类型提供更加丰富的信息。 下面三个实现的来自于error的接口表示这是由于超时而引起的错误，暂时性的。 看到这里，我想你已经猜出为什么要定义这两个变量了，正是能够适配 context 接口中的 Err() 方法，至于到底怎么具体实现的 Err() 方法我们后面再说。\nemptyCtx结构 type emptyCtx struct{} func (emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (emptyCtx) Done() \u0026lt;-chan struct{} { return nil } func (emptyCtx) Err() error { return nil } func (emptyCtx) Value(key any) any { return nil } 显而易见，这是一个空的上下文实现，通常用作上下文链的根节点或者占位符。 注释中也提到，这是一个没有任何状态信息，不包含取消、截止时间、或值存储功能，不能通过取消函数手动取消的上下文。\nBackgroundCtx \u0026amp; todoCtx结构 type backgroundCtx struct{ emptyCtx } func (backgroundCtx) String() string { return \u0026#34;context.Background\u0026#34; } type todoCtx struct{ emptyCtx } func (todoCtx) String() string { return \u0026#34;context.TODO\u0026#34; } 这两个接口就用到了emptyCtx,提供了最基础的上下文功能，为开发中的代码提供一个临时的上下文对象，避免为空。\nBackground 通常用于上下文链的根节点。 todo 通常用作占位符 推荐使用 todo，二者在行为上是一致的，但是 todo 的语义更加明确，让人们知道到这是一个待确定的地方；而 background 更常见在顶层的调用上，是根。 对应的还有两个方法来生成二者。\nfunc Background() Context { return backgroundCtx{} } func TODO() Context { return todoCtx{} } cancelCtx:支持显示取消的结构 type cancelCtx struct { Context mu sync.Mutex // protects following fields done atomic.Value // of chan struct{}, created lazily, closed by first cancel call children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call cause error // set to non-nil by the first cancel call } func (c *cancelCtx) Value(key any) any { if key == \u0026amp;cancelCtxKey { return c } return value(c.Context, key) } func (c *cancelCtx) Done() \u0026lt;-chan struct{} { d := c.done.Load() if d != nil { return d.(chan struct{}) } c.mu.Lock() defer c.mu.Unlock() d = c.done.Load() if d == nil { d = make(chan struct{}) c.done.Store(d) } return d.(chan struct{}) } func (c *cancelCtx) Err() error { c.mu.Lock() err := c.err c.mu.Unlock() return err } 哦，原来你小子嵌套了Context祖宗，你是儿子啊！然后你还多定义了什么：\nmu，很常见的保护并发访问 done 存储一个懒加载的通道，上下文被取消时关闭此通道；（这里的atomic.Value先放放） children,存储派生于该上下文的子上下文，形成一个上下文链（取消时通知他们） err，错误，通常用上面那两个错误变量 casuse，错误原因。 后面的这几个就是实现Context接口中的方法\nvalue:如果键是 cancelCtxKey，直接返回 cancelCtx 本身；否则交给父上下文处理 Done:先从 done.Load 获取已有通道，如果有了，直接返回；如果没有，创建一个新通道并存储（注意加锁后又尝试加载了一次，防止其他协程创建了通道）；相同上下文返回唯一通道 这是一种双检查锁的模式 Err:保证线程安全地返回错误状态 接下来是此结构中最重要的 cancel 方法，用于取消当前上下文，并传播取消信号到所有的子上下文，整个流程大致总结如下：\n设置 err 和 cause 表示上下文已经被取消 关闭 Done 通道以通知所有监听者 遍历递归取消所有子上下文（链条） 最后清理资源 // 是否需要将当前上下文从其父上下文的 children 集合中移除，取消的错误信息，取消的原因 func (c *cancelCtx) cancel(removeFromParent bool, err, cause error) { if err == nil { panic(\u0026#34;context: internal error: missing cancel error\u0026#34;) } if cause == nil { cause = err } // 锁定状态，防止重复取消 c.mu.Lock() if c.err != nil { c.mu.Unlock() return // already canceled } c.err = err c.cause = cause // 检查done通道是否已创建，如果没有就存储一个，否则调用close关闭通道 d, _ := c.done.Load().(chan struct{}) if d == nil { c.done.Store(closedchan) } else { close(d) } // 至此，通知所有子上下文关闭,递归地调用cancel方法 for child := range c.children { // NOTE: acquiring the child\u0026#39;s lock while holding parent\u0026#39;s lock. child.cancel(false, err, cause) } // 清空子上下文集合 c.children = nil c.mu.Unlock() if removeFromParent { removeChild(c.Context, c) } } CancelFunc函数类型 // A CancelFunc tells an operation to abandon its work. // A CancelFunc does not wait for the work to stop. // A CancelFunc may be called by multiple goroutines simultaneously. // After the first call, subsequent calls to a CancelFunc do nothing. type CancelFunc func() 是一个函数类型，表示一个无参数、无返回值的函数，并发安全，一次性触发（只有第一次调用会有效）。\n之所以这里定义为函数类型，就是为了让其他需要使用到CancelFunc的方法通过闭包来绑定具体的逻辑。稍后我们在一些具体方法中就能看到。\nWithCancel \u0026amp; WithCancelCause func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { c := withCancel(parent) return c, func() { c.cancel(true, Canceled, nil) } } type CancelCauseFunc func(cause error) func WithCancelCause(parent Context) (ctx Context, cancel CancelCauseFunc) { c := withCancel(parent) return c, func(cause error) { c.cancel(true, Canceled, cause) } } WithCancel 传入父上下文，创建一个取消上下文，返回此上下文和一个取消函数 可以发现正如我们上面所说，利用闭包，直接返回了一个取消函数cancel来触发上下文的取消 WithCancelCause 将WithCancel中的CancelFunc换为CancelCasuseFunc，带有取消原因。 下面的闭包函数中可以传入一个取消原因cause error func withCancel(parent Context) *cancelCtx { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } c := \u0026amp;cancelCtx{} c.propagateCancel(parent, c) return c } 这个withCancel就是上面两个方法分别调用的核心方法来创建一个cancelCtx上下文实例。\n先判断父上下文是否为空（上下文链必须有一个根），随后创建一个cancelCtx上下文实例，调用propagateCancel关联父子并监听父上下文的取消事件。\n说到这里可能有些懵逼了，那我们就来看看这个 propagateCancel 是怎么个事！\npropagateCancel 顾名思义，传播，用于将子取消逻辑与父取消逻辑联系起来形成一个链。具体代码逻辑的解释见注释。\n// propagateCancel arranges for child to be canceled when parent is. // It sets the parent context of cancelCtx. func (c *cancelCtx) propagateCancel(parent Context, child canceler) { c.Context = parent // parent作为基础（即上面的嵌套Context） done := parent.Done() // 获取parent的Done通道，记录着取消信号的channel if done == nil { // 如果为空，说明没有取消信号，parent永远不会取消 return // parent is never canceled } select { case \u0026lt;-done: // ? // parent is already canceled child.cancel(false, parent.Err(), Cause(parent)) return default: } // 检查parent类型是否为cancelCtx或派生类型 if p, ok := parentCancelCtx(parent); ok { // parent is a *cancelCtx, or derives from one. p.mu.Lock() if p.err != nil { // 如果错误类型不为空，证明parent已被取消了 // parent has already been canceled child.cancel(false, p.err, p.cause) } else { // 否则将child加入到parent的子集中 if p.children == nil { p.children = make(map[canceler]struct{}) } p.children[child] = struct{}{} } p.mu.Unlock() return } // 如果parent实现了AfterFunc方法，这个之后我们会说，是一种定时回调 if a, ok := parent.(afterFuncer); ok { // parent implements an AfterFunc method. c.mu.Lock() stop := a.AfterFunc(func() { child.cancel(false, parent.Err(), Cause(parent)) }) c.Context = stopCtx{ Context: parent, stop: stop, } c.mu.Unlock() return } // 用于处理无法直接关联的上下文，goroutine同时监听parent和child的Done通道 goroutines.Add(1) go func() { select { case \u0026lt;-parent.Done(): child.cancel(false, parent.Err(), Cause(parent)) case \u0026lt;-child.Done(): } }() } 所以，回到最开始的 withCancel 中，构建了一个取消传播链，可以调用 CancelFunc 来触发取消逻辑，释放资源。\nCause 从Context中提取取消原因(cause)\nfunc Cause(c Context) error { if cc, ok := c.Value(\u0026amp;cancelCtxKey).(*cancelCtx); ok { cc.mu.Lock() defer cc.mu.Unlock() return cc.cause } return c.Err() } afterFuncCtx 在cancelCtx的基础上实现回调。\ntype afterFuncCtx struct { cancelCtx once sync.Once // 确保 f 只被执行一次 f func() // 要在上下文完成时调用的回调函数 } AfterFunc传入回调函数f，并将afterFuncCtx关联到父上下文\nfunc AfterFunc(ctx Context, f func()) (stop func() bool) { a := \u0026amp;afterFuncCtx{ f: f, } // 将 afterFuncCtx 作为子上下文，关联到父上下文 ctx a.cancelCtx.propagateCancel(ctx, a) // 返回的 stop 函数用于停止回调的执行 return func() bool { stopped := false a.once.Do(func() { stopped = true }) if stopped { a.cancel(true, Canceled, nil) } return stopped } } timerCtx 这是cancelCtx的扩展，支持设置截止时间的上下文类型，多了 timer 和 deadline 字段。\ntype timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } // 返回当前截止时间以及标志位 func (c *timerCtx) Deadline() (deadline time.Time, ok bool) { return c.deadline, true } func (c *timerCtx) String() string { return contextName(c.cancelCtx.Context) + \u0026#34;.WithDeadline(\u0026#34; + c.deadline.String() + \u0026#34; [\u0026#34; + time.Until(c.deadline).String() + \u0026#34;])\u0026#34; } // 调用cancelCtx的cancel实现取消，多了定时器 func (c *timerCtx) cancel(removeFromParent bool, err, cause error) { c.cancelCtx.cancel(false, err, cause) if removeFromParent { // Remove this timerCtx from its parent cancelCtx\u0026#39;s children. removeChild(c.cancelCtx.Context, c) } c.mu.Lock() if c.timer != nil { c.timer.Stop() c.timer = nil } c.mu.Unlock() } WithTimeout \u0026amp; WithDeadline 基于超时持续时间创建上下文；基于指定的截止时间创建上下文\nfunc WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { return WithDeadlineCause(parent, d, nil) } 可以发现最终还是调用的是 WithDeadlineCause 这个方法\n1.检查父上下文是否有更早的截止时间 2.创建timerCtx 3.计算剩余时间 4.设置定时器 5.返回上下文和取消函数\nfunc WithDeadlineCause(parent Context, d time.Time, cause error) (Context, CancelFunc) { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } if cur, ok := parent.Deadline(); ok \u0026amp;\u0026amp; cur.Before(d) { // The current deadline is already sooner than the new one. return WithCancel(parent) } c := \u0026amp;timerCtx{ deadline: d, } // 关联到父上下文并计算截止时间的剩余时间 c.cancelCtx.propagateCancel(parent, c) dur := time.Until(d) if dur \u0026lt;= 0 { c.cancel(true, DeadlineExceeded, cause) // deadline has already passed return c, func() { c.cancel(false, Canceled, nil) } } // 设置定时器，到时间后自动取消上下文 c.mu.Lock() defer c.mu.Unlock() if c.err == nil { c.timer = time.AfterFunc(dur, func() { c.cancel(true, DeadlineExceeded, cause) }) } return c, func() { c.cancel(true, Canceled, nil) } } valueCtx 实现键值对存储功能的一种上下文类型,适合携带少量元数据。\ntype valueCtx struct { Context key, val any } Value:如果当前上下文没有匹配的键，则调用 value 函数继续在父上下文中查找。\nvalue:递归辅助方法，用于在上下文链中查找指定key对应的值\nfunc (c *valueCtx) String() string { return contextName(c.Context) + \u0026#34;.WithValue(\u0026#34; + stringify(c.key) + \u0026#34;, \u0026#34; + stringify(c.val) + \u0026#34;)\u0026#34; } func (c *valueCtx) Value(key any) any { if c.key == key { return c.val } return value(c.Context, key) } func value(c Context, key any) any { for { switch ctx := c.(type) { // 如果是valueCtx类型 case *valueCtx: if key == ctx.key { return ctx.val } c = ctx.Context //向父上下文继续找 // 如果是cancelCtx类型 case *cancelCtx: if key == \u0026amp;cancelCtxKey { return c } c = ctx.Context // 如果是withoutCancelCtx类型 case withoutCancelCtx: if key == \u0026amp;cancelCtxKey { // This implements Cause(ctx) == nil // when ctx is created using WithoutCancel. return nil } c = ctx.c // 如果是timerCtx类型 case *timerCtx: if key == \u0026amp;cancelCtxKey { return \u0026amp;ctx.cancelCtx } c = ctx.Context // 如果是backgroundCtx，todoCtx类型即根上下文 case backgroundCtx, todoCtx: return nil default: return c.Value(key) } } } withValue方法 func WithValue(parent Context, key, val any) Context { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } if key == nil { panic(\u0026#34;nil key\u0026#34;) } if !reflectlite.TypeOf(key).Comparable() { panic(\u0026#34;key is not comparable\u0026#34;) } return \u0026amp;valueCtx{parent, key, val} } 常用的Go Context 类型对比 类型 功能描述 支持取消 支持超时/截止时间 支持键值对存储 emptyCtx 根上下文类型。用于创建 context.Background() 和 context.TODO()，无任何功能。 否 否 否 cancelCtx 支持取消功能，通过 WithCancel 或 WithCancelCause 创建，可以传播取消信号。 是 否 否 timerCtx 基于 cancelCtx，支持取消和超时，通过 WithTimeout 和 WithDeadline 创建。 是 是 否 valueCtx 支持键值对存储，通过 WithValue 创建，通常嵌套在其他类型上下文中使用。 否 否 是 平时是如何使用的 这里举例说明context以及其中的常见方法在代码中是如何使用的。\nWithTimeout\u0026amp;WithDeadline 在这段代码中，context.WithTimeout 起到了控制数据库操作是否超时的作用\nfunc (m MovieModel) Insert(movie *Movie) error { // 插入一条新记录的SQL语句，并返回信息（Postgresql专有) query := ` INSERT INTO movies (title, year, runtime, genres) VALUES ($1, $2, $3, $4) RETURNING id, created_at, version` // 创建一个代表着占位符的movie中的属性切片 args := []interface{}{movie.Title, movie.Year, movie.Runtime, pq.Array(movie.Genres)} // Create a context with a 3-second timeout // 如果数据库操作在3s内没有完成，操作自动取消，返回超时错误 ctx, cancle := context.WithTimeout(context.Background(), 3*time.Second) defer cancle() // 使用QueryRowContext方法执行,利用传入的ctx进行SQL查询，并使用Scan方法将返回值注入到movie的三个属性中 return m.DB.QueryRowContext(ctx, query, args...).Scan(\u0026amp;movie.ID, \u0026amp;movie.CreatedAt, \u0026amp;movie.Version) } 总结 Golang中经常使用 Context，以上四种 Context 类型各有其作用，通常利用其自带的函数例如WithTimeout(),Background()来进行控制操作。\n其中值得注意的是 cancleCtx 的取消机制中的传播链，存在回调机制，值得我多去思考。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/005golangep01_%E5%B9%B6%E5%8F%9101_context/","summary":"\u003ch2 id=\"引子\"\u003e引子\u003c/h2\u003e\n\u003cp\u003e今天心血来潮，给自己梳理梳理golang中这个十分重要的东东——context，毕竟也是在面试中考察到，但是反问自己的时候却感觉一点也说不出来什么有价值的东西，所以今天我来一次刨根究底。\u003c/p\u003e","title":"GolangEP01_并发之绕不过的context"},{"content":"前言 这里照例列出所用设备配置\nmacbookair LicheePi4A,16+128 基础配置 如果你看完了上篇文章，那么一定成功连接到了wifi，那么我们就先用ssh从主机的终端连接到开发板（毕竟我只有一个显示器，开发板你不配）\n先打开开发板，并连接显示器，按照上一节的做法查看wifi是否连接，如果使用的nmcli这个命令它会自动连接的。接着使用hostname -I查看当前IP地址。拿着这个IP地址使用ssh去连接开发板。\n# 将IP换为具体的ip地址 ssh openeuler@IP 接下来，我们开始设置最基础的配置，例如密码，镜像源，以及一些小工具等等。\n密码与换源 首先，更改这个复杂的初始密码openEuler12#$,输入passwd，修改当前用户的密码（注意需要至少3个不同的字符类型）\n其次，关于包管理软件，使用dnf和yum都可以，但是推荐使用dnf，毕竟是yum的上游版本；关于是否需要换源，可以先去/etc/yum.repos.d/openEuler.repo这个文件中查看自己当前使用的源地址，其实里面的内容就是我们在01中下载镜像的地址。这是兴趣小组维护的一个很棒的地址，所以我认为没必要更换源。\n如果你想更换，这是openEuler官网所列出的镜像源,可以选择并替换掉上述文件中的链接内容。\n安装git 这个不用赘述sudo dnf install git\n如果遇到从开发板下载github上的东西特别慢的情况，可以使用这个网站https://doget.nocsdn.com/#/托管的国内地址来下载，速度不错。\n也可以使用这个代理地址git config --global url.\u0026quot;https://gh-proxy.com/github.com/\u0026quot; .insteadOf \u0026quot;https://github.com/\u0026quot;\n得到结果如下:\ncat .gitconfig [url \u0026#34;https://gh-proxy.com/github.com/\u0026#34;] insteadOf = https://github.com/ tmux分屏 这个软件如果你是一个linux或者macos的“老玩家”，相信你并不陌生。有了他我们就可以不再打开一个又一个的终端窗口，而是优雅的进行分屏操作。\n惊喜的是，软件源中居然存在这个软件，向兴趣小组的成员致敬。🫡\nsudo dnf install tmux 下载成功。 简单地试用一下，关于tmux的个性化配置，请参考我的这篇文章。\ntmux new -s \u0026#34;riscv\u0026#34; 输入CTRL+b作为前导键，再按下c新建一个窗口，会得到这样的效果 其实我现在在macos使用ssh的情况下已经是tmux套tmux的情况了😁\n其他的操作请参考[这篇文章]。\nDNS配置 sudo vi /etc/resolv.conf #结果如下 # Generated by NetworkManager nameserver 192.168.1.1 可以发现这是由NetworkManager自动根据我们所连接的wifi生成的 DNS 地址,推荐修改为8.8.8.8\n配置Golang环境 首先，来到Golang的官网release界面找到能够适配RISC-V的Golang版本，这里我使用的是go1.22.9.linux-riscv64.tar.gz,可以看到其支持的操作系统为Linux,架构为riscv64。\n为什么不选择最新的go1.23?因为还没有适配riscv的包。（更新一下，现在有了）\n# 使用该命令下载对应压缩包 curl -O https://go.dev/dl/go1.22.9.linux-riscv64.tar.gz # 如果遇到下载文件的错误，比如意外地下载为了其他格式，建议使用以下命令 curl -L -O https://go.dev/dl/go1.22.9.linux-riscv64.tar.gz #下载成功后，可以用以下命令检查文件格式 file go1.22.9.linux-riscv64.tar.gz # 对软件包进行解压 tar -zxvf go1.22.9.linux-riscv64.tar.gz # 配置golang的系统环境 vi ~/.bashrc # 在其中加入下面这两行之一 export PATH=$PATH:~/go/bin export PATH=$PATH:$HOME/go/bin # 或者你可以聪明地一步到位 echo \u0026#39;export PATH=$PATH:~/go/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # 重新加载 source ~/.bashrc # 检查是否成功 go env 如果成功看到一系列的go的环境变量，就说明我们就将golang环境配置好了。\n接下来可以编写一个简单的main.go文件进行测试\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello, RV64\u0026#34;) } go run main.go\nps:如果在执行上面的代码过程中遇到warning: GOPATH set to GOROOT (/home/openeuler/go) has no effect这个问题，意味着他在提醒你使用module进行管理项目的依赖关系，我们这里只是简单的测试一下，后期遇到的golang项目会使用mod进行管理。\nGOPATH must be set to get, build and install packages outside the standard Go tree.\n具体原因在于GOPATH是工作目录，用于存放Go项目的源代码、依赖包；而GOROOT是Go语言的安装目录，二者当然最好要不一样的路径。来自于Stackoverflow的解释\n配置docker 首先，下载docker engine,可以发现，官网中的列表中是没有Openeuler下的riscv64架构的适配的。\n所以我们直接采用dnf包管理的方式下载，(在这里对于为Openeuler-riscv64架构进行docker适配的人致以深刻的敬意，这也许就是真正的伟大的程序员吧）\nsudo dnf update sudo dnf install -y docker # 检查docker的运行状态 sudo systemctl status docker # 如果没有运行，需要启动 sudo systemctl start docker 接下来去sudo化配置，见官方文档\n接下来进行最重要的换源，否则镜像也无法拉取成功。目前国内还可以使用的Docker镜像见此链接\n# 修改 vi /etc/docker/daemon.json # 如果没有这个文件，自己手动新建（我当时只有一个key.json文件） # 将下面内容复制进去 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://dockerpull.org\u0026#34;, \u0026#34;https://docker.1ms.run\u0026#34;, \u0026#34;https://docker.linkedbus.com\u0026#34; ] } # 加载配置并重启 sudo systemctl daemon-reload sudo systemctl restart docker # 尝试拉取镜像 docker pull busybox docker run hello-world 看到\u0026quot;Hello from Docker\u0026quot;意味着拉取镜像成功。你也许会疑惑，为什么可以成功？镜像也应该与架构相关啊，使得，hello-world这个镜像正是适配了riscv64架构，不过其他的镜像就没有那么好运了。关于某个镜像是否支持某个架构可以在dockerhub中的具体镜像页面内查看。\n比如我们尝试一下nginx这个镜像docker pull nginx，可以发现他没有通过上面设置的仓库而是去官方找镜像，原因由chatgpt给出:\n由于您是在 RISC-V 环境（openEuler-riscv64）下运行，可能镜像加速器的缓存中并没有 RISC-V 架构的 nginx 镜像。因此 Docker 会从官方仓库拉取适配的镜像。\n就算是你强制使用换源后的仓库docker pull dockerpull.org/nginx，也会提示你no matching manifest for unknown in the manifest list entries意味着在镜像仓库中并没有合适的资源。\n目前在dockerhub中有关于riscv的镜像一共有官方的35个仓库,这里看到了redis，让我们试一试docker pull riscv64/redis，很遗憾，好像还是因为镜像站中没有收录。\n最后再尝试一个alpine，其支持riscv64docker pull alpine\n最后，关于镜像适配问题，这也是未来的工作方向之一吧，可真是任重而道远啊——总不能让我给每个镜像都来一遍riscv架构的适配吧？这是我这种鼠鼠能做的？\n配置K3s 来到k3s的官网使用 curl -sfL https://get.k3s.io | sh -官方命令安装会显示[ERROR] Unsupported architecture riscv64可以得知官方还并没有真正支持riscv架构。那么怎么办？\n谷歌了一圈发现这里的内容值得一看，专门在说k8s和k3s向riscv适配的，里面也给出了具体的操作指南。这里我们先试试SIG（兴趣小组）的源中是否存在k3s。\n其实原理上来讲，与docker一样源代码都是完全由golang编写的，而golang的官方正如我们上面所说，已经支持了riscv，所以迁移是一定可以实现的（但愿）。🧐\nsudo dnf install k3s 嘿，您猜怎么着，还真有！再次respect！让我们看看这次都下载了什么\ncontainer-selinux提供了通用的容器安全策略，而k3s-selinux提供了针对k3s的特定安全策略，确保k3s在运行时的安全性\n那么下载过后如何执行呢？按道理来说，得systemctl status k3s这样的命令（与docker类似）来操作，但是你直接操作时会发现报错unit k3s service not found并不存在。这该怎么办呢？\n在我Google了一圈发现这位大佬的适配工作后，我发现刚才下载的k3s是不是在/usr/local/bin/k3s这个路径下啊？\n于是去这个路径寻找，果然找到了，./k3执行这个文件，执行成功了。\n关于k3s的具体操作我们以后的文章再说。\n卸载k3s /usr/local/bin/k3s-uninstall.sh 总结与展望 没什么好总结的了，下一章进行k3s在riscv架构上的初步尝试，写几个golang小程序部署成节点试着管理一下。\n可真是Life\u0026rsquo;s struggle啊。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/004risc-v_deployment/","summary":"\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cp\u003e这里照例列出所用设备配置\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emacbookair\u003c/li\u003e\n\u003cli\u003eLicheePi4A,16+128\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/ys/%E6%99%9A%E4%B8%8A%E5%A5%BD%E5%85%84%E5%BC%9F%E4%BB%AC.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"基础配置\"\u003e基础配置\u003c/h2\u003e\n\u003cp\u003e如果你看完了上篇文章，那么一定成功连接到了wifi，那么我们就先用ssh从主机的终端连接到开发板（毕竟我只有一个显示器，开发板你不配）\u003c/p\u003e","title":"玩转RISCV开发板02-配置好容器化环境"},{"content":"本次实验环境 宿主机:macbook air M2 LiCheePi开发板，配置为16+128 一根USB-C线 一个拓展坞，键鼠，显示器 烧录镜像为openEuler/embedded_img/riscv64/lpi4a/ 实验步骤 1.下载fastboot 这里使用homebrew快捷安装\nbrew install android-platform-tools 2.连接开发板与宿主机 先长按开发板上的BOOT按钮，再将USB线插入到宿主机上（拓展坞上），注意这里的先后次序。这一步是为了让开发板进入USB烧录模式。\n进入终端，运行命令检查fastboot是否识别到了当前连接的开发板\nfastboot devices 注意是devices不是device,结果如下\n这里我也没弄清楚为什么是一堆问号，不过只要能够检测到设备就好。（注意本教程目前只测试了Macos下的操作）\n3.下载所需的镜像文件 来到镜像下载网站选择以下三种文件进行下载：\nu-boot文件用于引导加载程序——系统会先启动一个小型的引导加载程序（SPL），它负责加载完整的 U-Boot 镜像 base-boot.ext4.zst文件用于作为系统的启动分区——包括启动所需的文件，比如 Linux 内核、初始 RAM 磁盘（initramfs）、设备树（device tree） base-root.ext4.zst文件用于作为根文件系统——提供操作系统所需的文件 如果你安装过实打实的Linux系统（比如在windows下的双系统）的话你就会对这几个文件很熟悉。\n值得注意的是，其使用Zst格式压缩，所以在真正使用的时候需要提前进行解压，如果忘了解压，直接操作你就会遇到这个问题\n哈哈，文件格式不对当然无法成功了。\n故我们需要对zst格式结尾的文件进行解压\nzstd -d openEuler-24.03-LTS-riscv64-lpi4a-base-boot.ext4.zst zstd -d openEuler-24.03-LTS-riscv64-lpi4a-base-root.ext4.zst 4.进行烧录 首先进入上一步下载镜像所在的目录下\nfastboot flash ram u-boot-with-spl-lpi4a-16g.bin fastboot reboot # wait five seconds fastboot flash uboot u-boot-with-spl-lpi4a-16g.bin fastboot flash boot openEuler-24.03-LTS-riscv64-lpi4a-base-boot.ext4 fastboot flash root openEuler-24.03-LTS-riscv64-lpi4a-base-root.ext4 最终如果成功你会得到一堆OKAY，这里放一张我的运行截图\n5.验证 将开发板与宿主机的连接断开，将其连接到准备好的显示器上并准备好键鼠，等待其成功运行。 好的，成功发现一个神奇的问题，鼠标键盘也亮了，风扇也转起来了，结果显示器不亮？不亮？不亮？为什么！！！\n好，我先去google，来到官方文档烧录镜像的评论区发现一个遇到和我同样问题的朋友\n解决显示器不亮的问题 遇到这种问题，google并不能得到很相关的解决方案，这也是我在日常工作时觉得需要使用AI的场景之一。所以我得去询问ChatGPT让他针对于我这种问题给出解决思路。\n即使镜像烧录是成功的，为了保险起见，我们先去利用官网的SHA256文件验证一下文件是否损坏。\nsha256sum -c openEuler-24.03-LTS-riscv64-lpi4a-base-boot.ext4.zst.sha256sum sha256sum -c openEuler-24.03-LTS-riscv64-lpi4a-base-root.ext4.zst.sha256sum 结果都显示OK,那么镜像文件是对的。\n在检查ChatGPT给出的思路之前，我认为还是先来一遍对照实验，因为在烧录镜像之前自带的Debian是能够成功点亮显示器的，所以我们换一下镜像的版本来进行对照。\n对照实验1-换镜像版本 换成兴趣小组的镜像文件23.03再来一遍\n命令与上面相同，其中u-boot-with-spl-lpi4a-16g.bin这个文件是可以复用的。\n嘿，您猜怎么着，还是点不亮！\n那再换成preview版本的23.09再来一遍，这次点亮了！并且给出了一个图形化的界面！正当我准备大展拳脚，以为自己终于找到了问题的所在时，又出差错了！\n没有wifi界面，无法连接wifi，使用nmcli命令查看网络状态\nnmcli general status nmcli device status 显示WIFI是enabled,==WIFI-HW是missing的==，意味着WIFI硬件似乎没有被正确识别；只有本地环回地址连接成功了，其他都处于unmanaged状态。\n我在其他的地方也找不到连接wifi的界面，所以这又是什么问题呢？（个人目前猜测还是镜像不稳定，因为Debian操作系统可以连接wifi）\n最终，我去镜像文件中的devel部分找到23.09的镜像进行测试，结果成功了！\n成功解决 如上面所说，找到链接中的镜像后进行再次烧录,终于可以点亮屏幕（如果遇到键鼠没反应重启开发板即可），进入了一个黑色的命令行界面，接着我们要进行登录。\n用户名:openeuler 密码:openEuler12#$ 登录后我们首先需要连网，使用以下命令。\n# 检查网络状态，去报WIFI-HW,WIFI是enabled nmcli general status # 查看端口和网络，确保有wlan端口 ip a # 显示可用的wifi nmcli dev wifi # 连接具体wifi，其中SSID替换为wifi名，PASSWORD替换为密码 sudo nmcli dev wifi connect SSID password PASSWORD # 连接成功后再检查STATE是否为connected nmcli general status 至此，成功连接wifi，烧录镜像这一步骤到此结束。\n后续问题 看到这里，还有惊喜！还没完！我重启了一次开发板之后，发现wifi又没了，瞬间尴尬在原地，到底是镜像和网卡你两谁的问题!?\n又经历了两次重启，wifi又好了，所以，请看下面总结。\n总结与反思 这里总结一下为LicheePI烧录OpenEuler镜像遇到的问题。\n镜像or网卡问题 在还没烧录的时候，板子自带的Debian操作系统一切正常。接着烧录Openeuler for riscv镜像时出现了一系列的问题：\n先遇到官方最新的24.03LTS版本的专门为LiCheePI准备的镜像烧录后无法打开显示器的问题。 在SIG兴趣小组提供的链接中，preview下的镜像23.03还是没有点亮显示器。 同样，preview下的23.09虽然点亮了显示器，但是无法识别网卡，连接不了wifi 于是去develp下的23.09镜像点亮了显示器也连接到了wifi，但是存在偶尔的抽风问题——重启后wifi又没了，得不断重启尝试。 所以，综上所述，是谁的问题？\n我觉得八成原因在镜像，因为换了镜像后问题得到解决，不过如果细心的读者会发现我们对于23.03镜像并没有进行多次重启来检测是不是网卡的问题（其实我也重启了两次不过都没成功），所以还有两成原因在于WIFI网卡——看的这里你是否觉得这也太不“技术”了，一个技术博客怎么能够有点玄学成分了。\n哈哈，你的批评是对的，在这里我确实不严谨了，没有程序员深挖精神了，私密马赛！\n反思 最后的最后，我不禁想问自己一个问题：操作系统镜像和架构有关系吗？\n那这个问题答案也很简单，一定是有关系的，操作系统镜像必须与CPU指令集兼容，这也是为什么操作系统镜像页面有专门为RISC-V所设计的镜像页面。\n那么关于如何烧录OpenEuler for RISCV镜像到LiCheePi上这一主题就告一段落了，还是有点流水账了，anyway，博客永远是给自己看的。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/003li_riscv01/","summary":"\u003ch2 id=\"本次实验环境\"\u003e本次实验环境\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e宿主机:macbook air M2\u003c/li\u003e\n\u003cli\u003eLiCheePi开发板，配置为16+128\u003c/li\u003e\n\u003cli\u003e一根USB-C线\u003c/li\u003e\n\u003cli\u003e一个拓展坞，键鼠，显示器\u003c/li\u003e\n\u003cli\u003e烧录镜像为openEuler/embedded_img/riscv64/lpi4a/\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"实验步骤\"\u003e实验步骤\u003c/h2\u003e\n\u003ch3 id=\"1下载fastboot\"\u003e1.下载fastboot\u003c/h3\u003e\n\u003cp\u003e这里使用homebrew快捷安装\u003c/p\u003e","title":"玩转RISCV开发板01-烧录OpenEuler国产镜像"},{"content":"","permalink":"http://localhost:1313/posts/how_to_deployment_the_golang_microservices_on_k8s_or_k3s/","summary":"","title":"How_to_deployment_the_golang_microservices_on_K8s_or_K3s"},{"content":"Docker容器网络概述 用户定义网络 Drivers 容器网络 端口 IP地址和hostname DNS Custom hosts 代理服务 ","permalink":"http://localhost:1313/posts/docker_networking01/","summary":"\u003ch2 id=\"docker容器网络概述\"\u003eDocker容器网络概述\u003c/h2\u003e\n\u003ch2 id=\"用户定义网络\"\u003e用户定义网络\u003c/h2\u003e\n\u003ch3 id=\"drivers\"\u003eDrivers\u003c/h3\u003e\n\u003ch2 id=\"容器网络\"\u003e容器网络\u003c/h2\u003e\n\u003ch2 id=\"端口\"\u003e端口\u003c/h2\u003e\n\u003ch2 id=\"ip地址和hostname\"\u003eIP地址和hostname\u003c/h2\u003e\n\u003ch2 id=\"dns\"\u003eDNS\u003c/h2\u003e\n\u003ch3 id=\"custom-hosts\"\u003eCustom hosts\u003c/h3\u003e\n\u003ch2 id=\"代理服务\"\u003e代理服务\u003c/h2\u003e","title":"Docker_容器网络01"},{"content":"Docker_Storage 大家好我是LTX，我猜在使用docker时你或多或少都会有疑问，我的数据被存到哪里去了？（什么?你不用docker?，滚出克😡）docker化之后为什么得到的镜像这么小，比我原始的程序要小这么多？特别是如果我将Mysql容器化，那它保存的记录都去哪里了？\n今天我将进入Docker_Volumes的世界，与大家聊聊Docker是如何处理数据持久化的。\n秉持着STFW\u0026amp;RTFM的精神，我从官方的手册看起，并且辅助以测试实验来验证手册中的理论，设计实验部分使用了部分AI工具辅助。\n本机环境:macos15.1，Docker version 27.2.0, build 3ab4256\nStorage 手册中一开始就说明了默认情况下，容器内部创建的所有文件都存储在可写容器层上(详见docker基础原理)，这意味着，当该容器不存在时，数据不会持久化。\n那不会持久化可不行啊，我MYSQL关键点之一就是持久化啊，另一个容器要用我的数据结果你取不出来锁在容器里面了，这两个容器交互不就完犊子了吗？不行不行，得有个办法来实现数据的持久化，要不然我docker还怎么立足于容器生态江湖。\n并且更蛋疼的是容器的可写层与运行容器的主机紧密耦合，难以轻易地将数据移动到其他地方。这是因为可写层是容器运行时基于主机环境创建的一种临时存储结构，它的存在依赖于容器的运行状态，不像独立于容器的存储机制那样便于迁移数据。\n其实就是docker底层需要一个虚拟机运行嘛(见docker基础原理)，我们在主机上当然无法直接访问这个虚拟机，那我数据呢!?我想要让其他的人也用用啊！\ndocker设计者也当然为我想到了这种情况，于是他提供了两种解决方法：\n数据持久化到主机的磁盘上 数据持久化到主机的内存中 嘿，怎么有点Mysql的感觉了？扯远了，第一种解决方法又详细地分为了卷挂载和绑定挂载，至于这三者有何区别，且看手册中这张图片： 哈哈，你以为我要开始长篇大论了，别急，让我们先对这张图片有个印象就好，观察到三者的不同就行，我先得设计个实验来验证上面的默认不可持久化！\nlib01 首先，打开终端，(其实docker也提供了非常好用的GUI但我就想用Terminal)，运行一个简单的容器命名为test1,并进入到其中。\ndocker run -it --name test1 ubuntu:latest bash 在容器内部执行命令新建文件并写入测试内容作为测试数据。\necho \u0026#34;Test data in container layer\u0026#34; \u0026gt; /test_file.txt 检查内容是否成功写入并退出容器。\ncat /test_file.txt exit 使用同一个镜像来新建另一个容器，重复上述步骤查看指定文件是否存在\ndocker run -it --name test2 ubuntu:latest bash ls | grep test_file.txt 发现并不存在这个文件，证明当容器不存在时，在可写容器层创建的文件数据不会持久保存。\n并且我们也无法从主机上直接访问这个文件，发现无法通过常规的主机文件系统路径找到该文件。\nVolumes 那么我们来看设计师们提供的第一种方法，也是最推荐的方法——Volumes。\n如果让我用三句话来总结Volumes:在docker host中开辟一片空间，将容器使用的数据挂载到这片空间中；整个过程以及后续的操作都由docker自动管理；便于在不同的容器之间共享数据。\n形象地解释就是容器将保存的数据从容器内部移到了docker host（即宿主机）中，每次需要的时候带上（挂载）就行。\n哇哦，这么牛逼?!一次就实现了持久化和共享操作？操作一下看看怎么个事！ lib02 持久性验证 # 创建名为my_volume的卷 docker volume create --name my_volume # 运行一个简单的容器并挂载这个卷 docker run -it -v my_volume:/app/data --name test3 ubuntu:latest bash # 使检查是否能够访问这个卷中的内容 ls /app/data cat /app/data/test.txt # 退出容器，从Docker角度检查卷内容 exit docker volume inspect my_volume 显示结果为\n# cat结果 Hello from container # inspect结果 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2024-11-18T09:59:02Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/my_volume/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;my_volume\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] 重启启动一个新的容器，挂载my_volume卷并检查在不在，步骤除了容器命名不同以外都与上面一致。\n最终可以发现，这里的持久性指的就是这个my_volume永久地保存在了docker host中，想要用的时候挂载它就好了！\n多个容器共享卷验证 启动两个容器挂载同一个卷，保持着两个容器都处于运行状态，这里我就刚好拿上面两个容器做示范了。\n# 在第一个容器中修改test.txt中的内容，在第二个容器中观察内容是否发生变化 echo \u0026#34;Updated from container 1\u0026#34; \u0026gt;\u0026gt; /app/data/test.txt # 最终观察到内容为 Hello from container Updated from container 1 显然，这个卷中的内容的更改影响到了两个挂载到它的不同容器（诶，这里面是不是就会存在一些数据冲突并发问题？）\nBind mounts 哇哦，卷挂载这么厉害啊，那再看看设计者提供的第二种方案：绑定挂载。\n如果让我用三句话来总结绑定挂载：将数据文件挂载到宿主机文件系统中，使用宿主机的绝对路径并由我们自己全权管理整个过程。\n诶，这和卷挂载的区别不就很明显了嘛，卷挂载存储的文件路径是由Docker自动设置的而绑定挂载是由我们决定的。\nlib03 测试绑定挂载\n# 在本机上创建测试文件夹和测试文件 mkdir -p /host_test_dir \u0026amp;\u0026amp; cd /host_test_dir echo \u0026#34;Initial content from host\u0026#34; \u0026gt; host_file.txt # 挂载这个文件夹启动一个容器,注意，这里必须是在宿主机上的绝对路径 docker run -it -v /$HOME/host_test_dir/:/container_test_dir --name test5 ubuntu:latest bash # 检查文件 cat /container_test_dir/host_file.txt tmpfs 最后一种方法自然是将数据保存在宿主机的内存中，但众所周知保存在内存中只是一种临时的解决方案，毕竟宿主机关闭后数据就会消失。\n如何选择 那么，在卷挂载和绑定挂载之间该如何选择呢？其实从二者的特点就可以很明显看出docker的设计者想让我们用哪个，没错，一定是卷挂载——因为卷挂载的过程是由docker管理的，而绑定是由我们自己管理的，这会带来什么影响呢？\n卷挂载：\n其存储位置在 Docker 主机上是被抽象化的。这意味着数据的存储与主机的文件系统结构解耦，即使主机的文件系统布局发生变化，如系统升级或者目录结构调整，卷中的数据依然可以被容器正常访问，因为Docker会维护卷的内部结构和数据存储路径。（至于为什么，我们稍后会进行实验验证） 卷的生命周期独立于容器。当一个容器停止或被删除时，卷仍然存在，并且可以被挂载到其他容器中，数据不会丢失 总的来说就两字：隔离 绑定挂载：\n使得容器对文件的访问依赖于主机的文件系统结构。如果主机上的绑定目录被移动、重命名或者删除，容器内的文件访问就会出现问题。 反过来，容器可以直接访问主机的文件系统路径，一个被攻破的容器（例如，容器内的应用存在安全漏洞）可能会利用绑定挂载的权限对主机文件进行未经授权的修改，包括修改重要的系统文件或者其他容器可能依赖的文件 关于卷挂载中发现的问题 Hold on,Hold on,我知道看到现在你一定会存在一些小问题，没错我在看官方手册时以及配合AI进行实验验证的过程中也存在了许多问题，比如：\nmy_volume创建的路径在哪里？ 如果我不显式地设置卷挂载呢？会有什么效果？ 我们先来回答第二个问题。\n匿名卷 继续做一个模拟测试实验。\n# 使用-v 但没有指定容器外的挂载路径，只有容器内部的路径 docker run -it -v /app/data --name test6 ubuntu:lat est bash # 查看卷列表 docker volume ls 可以发现除了my_volume这个显示的卷挂载之外还有一个一长串字符组成的卷，这个卷就是匿名卷，是Docker host为我们自动设置的一个随机的、唯一的卷名称。 特别需要注意的点是:\n匿名卷是与容器的生命周期紧密绑定的 显示的卷与容器的生命周期是隔离的 所以我们可以做个实验，删除掉test6这个容器，再使用docker volume ls查看卷信息，发现刚才创建的匿名卷不见了。\n路径问题 还记得我们之前使用过的docker volume inspect my_volume命令得到的路径结果\u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/my_volume/_data\u0026quot;\n直接在宿主机上找到这个路径不就得了？这里就要引出一个困扰了我的问题——为什么默认Docker host就是宿主机？这不是与“Docker在Windows或者macos系统上运行需要一个轻量的虚拟机”这一理念冲突了吗？具体解释见下方的附录\n那么该如何找到这个路径呢？Docker在主机上实际上是基于一个小型的虚拟机来运行的，所以我们得找到这个虚拟机，经过google，我找到了一种在macos上寻找虚拟机的方法\ndocker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh cd /var/lib/docker/volumes 之后就可以在里面查看docker host中的卷以及卷内容了。\n总结 好了，那么关于Docker_Engine中的Storage的相关知识与模拟实验验证就到此结束了。\n总的来说，Docker采用了两种手段来实现存储的持久化，一种是卷挂载一种是绑定挂载，其中卷挂载更能体现出Docker的隔离性，也是设计者推荐的方法，相信看完本篇内容你会对Docker的数据存储有更深的理解！\n当然其中也挖了一些以后需要填的坑，日后再见！感谢您抽出宝贵的时间浏览这篇内容。\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n附录 Docker host 首先我们要明确两个点:\n遇到文档中Docker host这个叫法，指的就是运行Docker daemon守护进程的宿主机；比如我的macbook Docker在Linux上直接利用Linux内核来运行，在Windows和macos上运行需要一个额外的底层轻量虚拟机Hypervisor.Framwork,实现了对用户的透明化。 所以，我们使用卷挂载挂载的Volume名义上在宿主机里面，实际上真实的路径/var/lib/docker/volumes是在这个轻量的虚拟机中的。\n即使我觉得这很别扭，但是他就是这样的。总之你要知道Docker并不是独自的来到你的电脑上的，他要依靠Linux内核特性！他需要一个虚拟机来运行！（扯远了，这些需要另开一篇博客哈哈）😇\n","permalink":"http://localhost:1313/posts/002docker_volume/","summary":"\u003ch1 id=\"docker_storage\"\u003eDocker_Storage\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/ys/%E6%99%9A%E4%B8%8A%E5%A5%BD%E5%85%84%E5%BC%9F%E4%BB%AC.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e大家好我是LTX，我猜在使用docker时你或多或少都会有疑问，我的数据被存到哪里去了？（什么?你不用docker?，滚出克😡）docker化之后为什么得到的镜像这么小，比我原始的程序要小这么多？特别是如果我将Mysql容器化，那它保存的记录都去哪里了？\u003c/p\u003e","title":"玩转Docker系列——数据持久化"},{"content":"Why 起源 在上周四下午，我的导师给了我一个小任务，开发一个简单地安卓App调用相关的天气，地图API以显示当前位置的温度，由于我的安卓开发水平并不是那么出色并且当晚就要交付，所以我使用AI来帮我完成这个任务。\n我使用豆包在两个小时之内搭建好了这个简单的App，即使其中的代码逻辑原理我自己并不是那么的清楚。\n不过在外人看来，我已经出色地完成了这个任务，并且利用AI让我这样一个对安卓开发不是那么熟悉的开发者很快地就开发好了一个可以成功运行的程序。 可是在我自己看来，当我使用AI完全地开发这个程序之后，我感受到了前所未有的空洞，一种挫败感油然而生——我开始思考作为一名计算机专业的学生来说，使用AI来开发程序，完成老师所布置的作业任务是否是一件对的事情。\n基于此，我写下了这篇博客，作为我的博客集中的第一篇博客，我觉得其十分有意义。本文将从一名材料跨考计算机的非完全计算机专业学生的视角出发，讲述这两年来我与生成式AI的“爱恨情仇”。 （在此声明，本博客秉持着完全主观，从不客观的观点，极致的输出本人的个人观点与论调，欢迎任何不同的声音，所以不要拿它当做你的某些指南性的东西）\n我与生成式AI 回想起来，我第一次使用生成式AI工具是在2022年年末，也就是ChatGPT刚推出的那段时间，我还算得上比较早使用的一批用户了。\n当时用来干嘛呢？因为我是跨考生，除了学校必修的Python以外，我自学了C语言和一部分的Java，但都只是纸上谈兵，动手能力极差，解题能力就更不用说了。所以面对考研复试中的机试代码题我是比较担心的，在北林OJ里面刷题发现这些acm类型的题目和我平时偶尔刷的几次力扣不太一样，并且在我网上搜索答案时也只能找得到别人写的Cpp答案,我又不懂Cpp语法，当时铁了心以后当JavaBoy用Java写(结果现在半年没碰过Java了令人唏嘘)，我便想到了向ChatGPT询问题目的答案。\n开始时ChatGPT并不能每个题都做到完美，有我提问的因素在里面也有他当时的能力限制的因素在里面，不过有了它至少我能够开始刷OJ里的代码题了，至少有答案了。就这样来到了复试上机阶段，当时我的上机分数是69分，在上岸的人里面算是中等水平了，但我自己知道自己做的并不是那么好，其实没有一道题完整AC，都是得的过程分。但是这对当时的我也并没有起到多大的警醒，反倒是觉得上岸了就好了，也为今天的我埋下了伏笔。\n就这样，我不断地使用着ChatGPT,一路见证他从免费的3.5到收费的4，再到现在免费的4o，收费的o1，我也进行了将近一年之久的会员订阅。我发现我的生活中已经离不开他了，无论遇到什么问题我都习惯地将询问ChatGPT或者其他大语言模型例如今年开始使用的豆包，千问加入到解决问题的途径中，并且是一条优先级非常高的途径。而在如何使用AI方面我自诩也是用的不错，我的舍友们都称赞我是宿舍的Prompt工程师，但是作为一名计算机专业的学生来说，这真的对吗？\n直到最近，我突然开始反思，虽然现在生成式AI发展地如火如荼，但是如果某一天，禁止我使用AI，就让我从0开始去完成某项任务，我还能够胜任吗？我是否会手足无措？根本不知道第一行代码该从何开始写？我是否在面对报错时一筹莫展？连最基础的debug能力都要逐渐丧失？这些一连串的疑问让我不寒而栗，所以我决定使用其他的方式去寻找可能的答案。\nWhat 没有GPT时是怎样的 没有生成式AI之前，我们的解决问题的方式是什么样子？这个问题对于我而言其实很难从自身出发做出回答，因为我在没有遇到ChatGPT之前都在材料领域，与计算机领域的解决问题的思路不一样，并且大一大二两年不会遇到什么专业问题，学的都是公选课，大三分流之后也没有潜心在本专业上，甚至当时连google都没有用过，所以我只能从我所了解到的其他“纯血”计算机人那里得到答案。\n我听的最多的是从百度,从google中搜索，细说的话就是CSDN,Stackoverflow,github这些网站中寻找到解决问题的方法，毕竟有着STFW和RTFM这两个计算机领域的“圣经”。说到这里我不禁去插入有关于这两个理念的话题。\nSTFW\u0026amp;RTFM 我第一次听到这两个理念是今年的7月份，在学习一生一芯项目的预学习阶段时了解到的。为什么我想专门聊这个话题呢？因为我从对于豆包的使用中发现了一些有趣的理念。Search The Fucking Website和Read The Fucking Manual都是告诉我们要通过网络和手册独自的去解决问题，因为我所遇到的问题一定都发生过其他人的身上。而这一切都基于一件事——一个好的浏览器的好的搜索，我必须通过Google,bing去搜索才能够做到以上两点。\n而现在豆包的设计就是将自己包装为了一个浏览器，如果你下载豆包APP,你会发现他的内部完全是一个浏览器，不仅有对AI提问的页面也有直接进行网络搜索的界面(不过只有百度搜索)，相比于其他我所用过的AI那种单纯的AI问答界面，我想这或许是豆包给出的一种自己的答案——AI提问+网络搜索。\n如何看待生成式AI 生成式AI的基本原理就是预测，推理，根据语料库进行回答。不过我并不是这方面的专家，我也不会在关于原理性方面侃侃而谈，就我的观点，对于语料库中没有涉及到的知识，我认为生成式AI的回答一定是有一定的问题的。\n回到我计算机专业学生的身份，我一直在强调这一点，因为我认为对于生成式AI的使用，专业和非专业的使用习惯，使用领域应该是完全不同的。从非专业的角度来说，就像黄仁勋前几天说的那样，未来所有人都是“程序员”，是的，没错，非专业的人想要生成一段自动化的脚本来辅助自己其他领域的工作通过AI是非常便捷的一件事。\n从专业的角度来讲（虽然我并不是一名程序员，只是个学生）但我认为Coding不是一名程序员的全部，甚至Coding在他的日常工作中应该只能占到百分之五十左右，其他的应该在理解业务需求，与人沟通协作上。所以这也从侧面回答了一个问题，程序员未来会被AI取代吗？我觉得不会，我始终认为AI就像一个平静的潘多拉魔盒，它只是一个工具，我要把它当做一个工具来对待，而如果我对这个工具使用的过于深入，不小心开启了这个潘多拉魔盒，我就会在不知不觉中被其吞噬，最终失去自己的核心竞争力，沦为AI的淘汰产物。\nHow 如何面对 回到Why篇最后那一连串的提问，我当即去Google了这样一个问题：\u0026ldquo;without ai can\u0026rsquo;t code\u0026rdquo;,第一条结果就是与我感同身受的一篇来自于Reddit的帖子，他遇到了和我一模一样的问题，我意识到我思考这个问题的重要性。\n接着我回想起几天前看到的The Daily of CEO中采访前Google CEO的视频，视频里谈到如何看待AI，他告诉我当今如果你的事业不使用AI，那么你会被淘汰，但同时他也说对于程序员你得自己写代码，真正切切地自己编码。听起来像是一个悖论，但其实非常合理。\n所以最终我暂时认为，对于生成式AI的使用，我将避免在自己的专业领域以及写作中过度使用，过度依赖，因为这是我自己的核心竞争力，这是我必须要掌握的基础知识；在一些我的未知领域，比如我想快速地了解量化领域的入门知识，在一些我应该使用某个工具来完成的事情，比如写一段简单的自动化脚本，这些方面我会使用生成式AI。\n不过对于如何定义使用生成式AI这个工具的场景我个人还是十分纠结的，我目前也不清楚究竟如何才能正确对待这个强悍的工具。\n目标 基于此，我将在未来的几周之内（也许需要更久的时间）探索作为一名计算机专业的学生，到底应该如何使用生成式AI。我暂时确定一个方针，在面对自己的专业领域以及写作（未来我要完成论文的写作）我拒绝使用AI，至少两周之内完全不使用AI，面对其他领域我还是要使用AI工具。\n两周之后我将给出自己的体验，也许那时我就会将专业领域完全不使用AI这种极端的行为加以修改，精细化自己的工作流。\n最后我还想再跑个题，关于最近风靡的AI浪潮，我个人还是觉得只是狭义的AI，而非真正的AGI，也不知道达到AGI的地步需要多久的时间，但生成式AI应该也只是我心中AI的惊鸿一瞥罢了，所以到底是炒作大于实际还是未来已至呢？\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n","permalink":"http://localhost:1313/posts/001aithought/","summary":"\u003ch1 id=\"why\"\u003eWhy\u003c/h1\u003e\n\u003ch2 id=\"起源\"\u003e起源\u003c/h2\u003e\n\u003cp\u003e在上周四下午，我的导师给了我一个小任务，开发一个简单地安卓App调用相关的天气，地图API以显示当前位置的温度，由于我的安卓开发水平并不是那么出色并且当晚就要交付，所以我使用AI来帮我完成这个任务。\u003c/p\u003e","title":"AIThought01——让我尝试不依靠AI"},{"content":"这是测试\n如何创建一个新的博客\nhugo new content content/posts/xxxx.md 同时注意draft为true时是草稿，暂时不显示在网页上。\n行内数学公式：$a^2 + b^2 = c^2$。\n块公式，\n$$ a^2 + b^2 = c^2 $$\n$$ \\boldsymbol{x}{i+1}+\\boldsymbol{x}{i+2}=\\boldsymbol{x}_{i+3} $$\n","permalink":"http://localhost:1313/posts/my-first-post/","summary":"\u003cp\u003e这是测试\u003c/p\u003e\n\u003cp\u003e如何创建一个新的博客\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehugo new content content/posts/xxxx.md\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e同时注意draft为true时是草稿，暂时不显示在网页上。\u003c/p\u003e\n\u003cp\u003e行内数学公式：$a^2 + b^2 = c^2$。\u003c/p\u003e\n\u003cp\u003e块公式，\u003c/p\u003e\n\u003cp\u003e$$\na^2 + b^2 = c^2\n$$\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cp\u003e$$\n\\boldsymbol{x}\u003cem\u003e{i+1}+\\boldsymbol{x}\u003c/em\u003e{i+2}=\\boldsymbol{x}_{i+3}\n$$\u003c/p\u003e","title":"My First Post"},{"content":"注定孤独娱乐。\n","permalink":"http://localhost:1313/about/","summary":"about","title":"后花园"}]