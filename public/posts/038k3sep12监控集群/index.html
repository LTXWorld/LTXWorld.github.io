<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>K3sEP12——监控集群的两种方式 | LTX&#39;s Blog</title>
<meta name="keywords" content="K3s, RiscV, 监控">
<meta name="description" content="引子
为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 metric-server;
在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.">
<meta name="author" content="LTX">
<link rel="canonical" href="http://localhost:1313/posts/038k3sep12%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.60bb00c7b34dfc1f8b8a6f21b1fe067b40b3bc3a287fc1d4a3657a89a977208e.css" integrity="sha256-YLsAx7NN/B&#43;Lim8hsf4Ge0CzvDoof8HUo2V6ial3II4=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="http://localhost:1313/favicon.png">
<link rel="mask-icon" href="http://localhost:1313/favicon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/posts/038k3sep12%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
<script type="text/javascript"

        async

        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

MathJax.Hub.Config({

  tex2jax: {

    inlineMath: [['$','$'], ['\\(','\\)']],

    displayMath: [['$$','$$'], ['\[\[','\]\]']],

    processEscapes: true,

    processEnvironments: true,

    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],

    TeX: { equationNumbers: { autoNumber: "AMS" },

         extensions: ["AMSmath.js", "AMSsymbols.js"] }

  }

});


MathJax.Hub.Queue(function() {

    

    

    

    var all = MathJax.Hub.getAllJax(), i;

    for(i = 0; i < all.length; i += 1) {

        all[i].SourceElement().parentNode.className += ' has-jax';

    }

});

</script>


<style>

code.has-jax {

    font: inherit;

    font-size: 100%;

    background: inherit;

    border: inherit;

    color: #515151;

}

</style><meta property="og:url" content="http://localhost:1313/posts/038k3sep12%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4/">
  <meta property="og:site_name" content="LTX&#39;s Blog">
  <meta property="og:title" content="K3sEP12——监控集群的两种方式">
  <meta property="og:description" content="引子 为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 metric-server; 在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-18T18:29:58+08:00">
    <meta property="article:modified_time" content="2025-05-18T18:29:58+08:00">
    <meta property="article:tag" content="K3s">
    <meta property="article:tag" content="RiscV">
    <meta property="article:tag" content="监控">
      <meta property="og:image" content="http://localhost:1313/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png">
<meta name="twitter:title" content="K3sEP12——监控集群的两种方式">
<meta name="twitter:description" content="引子
为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 metric-server;
在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "K3sEP12——监控集群的两种方式",
      "item": "http://localhost:1313/posts/038k3sep12%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "K3sEP12——监控集群的两种方式",
  "name": "K3sEP12——监控集群的两种方式",
  "description": "引子 为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 metric-server; 在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.\n",
  "keywords": [
    "K3s", "RiscV", "监控"
  ],
  "articleBody": "引子 为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 metric-server; 在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.\nmetric-server 什么是 metric-server Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API for use by Horizontal Pod Autoscaler and Vertical Pod Autoscaler. Metrics API can also be accessed by kubectl top, making it easier to debug autoscaling pipelines.\n它会去与 kubelet 交互,作为 apiserver 的外部 api,向 kubelet 的 /metrics/resource 发起 HTTPS 请求;同时其还支持 HPA 自动扩缩容\n可以使用 kubectl get apiservices 查看其 api 状态\nNAME SERVICE AVAILABLE AGE v1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d22h 具体操作 首先,metric-server 在其 release 版本中并没有 riscv 的版本,所以我们先要进行适配工作. 不过好在之前的那位大佬也做了相关的支持,所以我们直接使用它的镜像作为我们的私有仓库镜像.\n之后写一个 yaml文件 用来部署 metric-server 到 server 上\n# metrics-server.yaml apiVersion: v1 kind: Namespace metadata: name: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: replicas: 1 selector: matchLabels: k8s-app: metrics-server template: metadata: labels: k8s-app: metrics-server spec: containers: - name: metrics-server image: jimlt.bfsmlt.top/metrics-server:v0.7.2 # 私有镜像 imagePullPolicy: IfNotPresent args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname ports: - containerPort: 4443 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: k8s-app: metrics-server ports: - port: 443 targetPort: 4443 这里包括了 Deployment 和对应的 Service,并且我们作为测试没有用到 tls --kubelet-insecure-tls\n写好之后进行部署 kubectl apply -f xxx.yaml\n部署结束后,我们进行测试 kubectl top nodes,发现此服务并没有启动成功.\n排查问题 我们自然地去查看容器的状态,即 kubectl describe,发现问题所在\nReadiness probe failed: Get \"https://10.42.0.58:10250/readyz\": dial tcp 10.42.0.58:10250: connect: connection refused 这是个什么地址呢?我们先来看看之前 yaml 文件中的这段代码 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\nInternalIP: 节点 Node 在集群内部网络中的地址 ExternalIP: 公网 IP,供集群外部访问 Hostname: 节点的计算机名称 这些与 PodIP 不同,PodIP 是集群使用 CNI 分配的,关于 CNI 在 k3s 中的知识,请看这篇文章.\n所以,这个请求其实是发往了我的 server 节点上的一个 Pod,所以这个就很奇怪了,按道理来说应该发往 InternalIP 即192.168.1.198 才对\n执行 kubectl get apiservices 可以发现 MissingEndpoints\nv1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d22h 相关的 issue 可以找到些解决办法,最多的还是添加 - --kubelet-insecure-tls,但是我们已经有了不是吗?\n如果现在遇到同样问题的你找到了其他更好的相关问题或者解决方法,请在评论区告知.\n官方文档中的问题 stackoverflow上有关问题 我们在 yaml 配置中添加 HostNetwork\nspec: hostNetwork: true # ✅ 添加 hostNetwork，避免 CNI 中 10250 访问失败 dnsPolicy: ClusterFirstWithHostNet # ✅ 搭配 hostNetwork 使用 之后再次查看容器的运行状态:\nReadiness probe failed: HTTP probe failed with statuscode: 404 通过查看日志发现:\nsudo kubectl logs -n kube-system metrics-server-6b8f575f5d-56jm8 I0518 11:31:43.086131 1 serving.go:374] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0518 11:31:45.510186 1 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager I0518 11:31:45.658709 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController I0518 11:31:45.658803 1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController I0518 11:31:45.659011 1 configmap_cafile_content.go:202] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\" I0518 11:31:45.659138 1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0518 11:31:45.659169 1 configmap_cafile_content.go:202] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\" I0518 11:31:45.659247 1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I0518 11:31:45.660929 1 dynamic_serving_content.go:132] \"Starting controller\" name=\"serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key\" I0518 11:31:45.662143 1 secure_serving.go:213] Serving securely on [::]:4443 I0518 11:31:45.662329 1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\" I0518 11:31:45.759559 1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I0518 11:31:45.759641 1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0518 11:31:45.759584 1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController 其并没有直接显示一些访问地址失败的信息.\n接着我们尝试 kubectl edit deployment -n kube-system metrics-server修改配置 将 livenessProbe 和 readinessProbe 及其有关配置删除 相关解决方案来自 CSDN\nreadinessProbe: httpGet: path: /readyz port: 4443 scheme: HTTPS initialDelaySeconds: 20 periodSeconds: 10 再来看看 Pod 结果如何\nkube-system metrics-server-6c8f74f4c4-fktpw 1/1 Running 居然成功了,查看集群信息:\nsudo kubectl top no NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% agent197 40m 1% 774Mi 9% revyos-lpi4a 136m 3% 1502Mi 9% 所以最终我们只需要去掉 readinessProbe即可,不用添加 NetWork 等字段在配置文件中. 这对整体的运行时没有影响的,只是去掉了存活探针检测机制.\nProbe及可能原因 这些 Probe 是来自于 k3s 的系统探针\nReadiness Probe:检测某个容器是否就绪(就是我们常见的 Ready 字段),是否可以接受流量,如果没有就不能加入到 Service Liveness Probe:判断容器是否“健康”，未通过探针 → 被重启容器。 Startup Probe:判断容器启动是否完成，配合前两者避免“启动未完成就被探测失败”。 所以结合之前的两种失败报错, https://10.42.0.58:10250/readyz,是 metric-server 访问 kubelet 容器,因为 metrics-server 默认会通过 kubelet API 获取节点、容器、Pod 的资源指标数据\n后来返回404:\n可能metrics-server 镜像中并没有暴露 /readyz 这个 HTTP 路径 某些版本只支持 /healthz 或 / 路径 prometheus prometheus 在 k8s 的集群监控中也是十分常见的工具,可抓取 所有组件、应用、Exporter 的细粒度指标,并且可以做到报警,可视化,长期持久化等功能.\n所以我们测试一下在我们的集群中使用情况.\n操作流程 与 metric-server 不同的是,prometheus 支持 riscv 架构,所以我们可以直接使用.\n在官网下载好安装包传到开发板 server 节点上,进行解压,并将可执行文件和配置文件都放到对应目录中\nsudo cp ~/prometheus-2.53.4.linux-riscv64/prometheus /usr/local/bin/prometheus sudo cp ~/prometheus-2.53.4.linux-riscv64/prometheus.yml /etc/prometheus/prometheus.yml 为其编写 service 文件,交由 systemd 管理\n[Unit] Description=Prometheus Monitoring Wants=network-online.target After=network-online.target [Service] User=debian ExecStart=/usr/local/bin/prometheus \\ --config.file=/etc/prometheus/prometheus.yml \\ --storage.tsdb.path=/var/lib/prometheus \\ --web.listen-address=0.0.0.0:9090 \\ --web.enable-lifecycle Restart=on-failure [Install] WantedBy=multi-user.target 这里的 User 可以选择为 prometheus 独立创建一个用户;数据存储在 /var/lib/prometheus 下\n同时创建对应的目录并为其赋予对应权限.\nsudo mkdir -p /etc/prometheus /var/lib/prometheus sudo chown -R jimlt:jimlt /etc/prometheus /var/lib/prometheus 重新加载 systemd 并启动\nsudo systemctl daemon-reexec sudo systemctl daemon-reload sudo systemctl enable prometheus sudo systemctl start prometheus 验证其状态:\nsudo systemctl status prometheus # prometheus.service - Prometheus Monitoring Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; preset: enabled) Active: active (running) since Sun 2025-05-18 20:59:08 CST; 12min ago Invocation: a99f1274a13a4ae89ecb9726ece102a4 Main PID: 5337 (prometheus) Tasks: 10 (limit: 18020) Memory: 41.2M (peak: 41.9M) CPU: 5.667s CGroup: /system.slice/prometheus.service └─5337 /usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/var/lib/prometheus --web.listen-address=0.0.0.0:9090 --web.enable-\u003e 此时我们就可以去浏览器中输入 localhost:9000 访问其监控页面.\n关于其安装部署流程就告一段落了,后续我们可以在其配置文件中添加 job,让其监控集群中的数据,关于其后续的更深入操作,见以后的文章吧!\n总结 本文尝试了 k3s 集群中两种监控方案的实现,并解决了 metric-server 运行过程中的探针问题.为后续集群的监控奠定了基础.\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n",
  "wordCount" : "2008",
  "inLanguage": "zh",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "2025-05-18T18:29:58+08:00",
  "dateModified": "2025-05-18T18:29:58+08:00",
  "author":{
    "@type": "Person",
    "name": "LTX"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/038k3sep12%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LTX's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.png"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="LTX&#39;s Blog (Alt + H)">LTX&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="LTX&#39;s Blog">
                    <span>首页</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="搜索">
                    <span>搜索</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="后花园">
                    <span>关于</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      K3sEP12——监控集群的两种方式
    </h1>
    <div class="post-meta"><span title='2025-05-18 18:29:58 +0800 CST'>五月 18, 2025</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;LTX

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%bc%95%e5%ad%90" aria-label="引子">引子</a></li>
                <li>
                    <a href="#metric-server" aria-label="metric-server">metric-server</a><ul>
                        
                <li>
                    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-metric-server" aria-label="什么是 metric-server">什么是 metric-server</a></li>
                <li>
                    <a href="#%e5%85%b7%e4%bd%93%e6%93%8d%e4%bd%9c" aria-label="具体操作">具体操作</a></li>
                <li>
                    <a href="#%e6%8e%92%e6%9f%a5%e9%97%ae%e9%a2%98" aria-label="排查问题">排查问题</a></li>
                <li>
                    <a href="#probe%e5%8f%8a%e5%8f%af%e8%83%bd%e5%8e%9f%e5%9b%a0" aria-label="Probe及可能原因">Probe及可能原因</a></li></ul>
                </li>
                <li>
                    <a href="#prometheus" aria-label="prometheus">prometheus</a><ul>
                        
                <li>
                    <a href="#%e6%93%8d%e4%bd%9c%e6%b5%81%e7%a8%8b" aria-label="操作流程">操作流程</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="引子">引子<a hidden class="anchor" aria-hidden="true" href="#引子">#</a></h2>
<p>为了更好地管理集群,我们打算使用工具监控集群, k8s 首推的是 <a href="https://github.com/kubernetes-sigs/metrics-server">metric-server</a>;
在其他书籍中我看到的使用 prometheus(普罗米修斯),所以我们对二者都进行一个测试.</p>
<h2 id="metric-server">metric-server<a hidden class="anchor" aria-hidden="true" href="#metric-server">#</a></h2>
<h3 id="什么是-metric-server">什么是 metric-server<a hidden class="anchor" aria-hidden="true" href="#什么是-metric-server">#</a></h3>
<blockquote>
<p>Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API for use by Horizontal Pod Autoscaler and Vertical Pod Autoscaler. Metrics API can also be accessed by kubectl top, making it easier to debug autoscaling pipelines.</p></blockquote>
<p>它会去与 kubelet 交互,作为 apiserver 的外部 api,向 kubelet 的 /metrics/resource 发起 HTTPS 请求;同时其还支持 HPA 自动扩缩容</p>
<p>可以使用 <code>kubectl get apiservices</code> 查看其 api 状态</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">NAME                                   SERVICE                      AVAILABLE                  AGE
</span></span><span class="line"><span class="cl">v1beta1.metrics.k8s.io                 kube-system/metrics-server   False <span class="o">(</span>MissingEndpoints<span class="o">)</span>   3d22h
</span></span></code></pre></div><h3 id="具体操作">具体操作<a hidden class="anchor" aria-hidden="true" href="#具体操作">#</a></h3>
<p>首先,metric-server 在其 release 版本中并没有 riscv 的版本,所以我们先要进行适配工作.
不过好在之前的那位大佬也做了相关的支持,所以我们直接使用它的镜像作为我们的私有仓库镜像.</p>
<p>之后写一个 yaml文件 用来部署 metric-server 到 server 上</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="c"># metrics-server.yaml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Namespace</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">jimlt.bfsmlt.top/metrics-server:v0.7.2 </span><span class="w"> </span><span class="c">#  私有镜像</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- --<span class="l">cert-dir=/tmp</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- --<span class="l">secure-port=4443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- --<span class="l">kubelet-insecure-tls</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- --<span class="l">kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">4443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">4443</span><span class="w">
</span></span></span></code></pre></div><p>这里包括了 Deployment 和对应的 Service,并且我们作为测试没有用到 tls <code>--kubelet-insecure-tls</code></p>
<p>写好之后进行部署 <code>kubectl apply -f xxx.yaml</code></p>
<p>部署结束后,我们进行测试 <code>kubectl top nodes</code>,发现此服务并没有启动成功.</p>
<h3 id="排查问题">排查问题<a hidden class="anchor" aria-hidden="true" href="#排查问题">#</a></h3>
<p>我们自然地去查看容器的状态,即 <code>kubectl describe</code>,发现问题所在</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Readiness probe failed: Get <span class="s2">&#34;https://10.42.0.58:10250/readyz&#34;</span>: dial tcp 10.42.0.58:10250: connect: connection refused    
</span></span></code></pre></div><p>这是个什么地址呢?我们先来看看之前 yaml 文件中的这段代码 <code>--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</code></p>
<ul>
<li>InternalIP: 节点 Node 在集群内部网络中的地址</li>
<li>ExternalIP: 公网 IP,供集群外部访问</li>
<li>Hostname: 节点的计算机名称</li>
</ul>
<p>这些与 PodIP 不同,PodIP 是集群使用 CNI 分配的,关于 CNI 在 k3s 中的知识,请看这篇文章.</p>
<p>所以,这个请求其实是发往了我的 server 节点上的一个 Pod,所以这个就很奇怪了,按道理来说应该发往 InternalIP 即192.168.1.198 才对</p>
<p>执行 <code>kubectl get apiservices</code> 可以发现 MissingEndpoints</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">v1beta1.metrics.k8s.io                 kube-system/metrics-server   False <span class="o">(</span>MissingEndpoints<span class="o">)</span>   3d22h
</span></span></code></pre></div><p>相关的 issue 可以找到些解决办法,最多的还是添加 <code>- --kubelet-insecure-tls</code>,但是我们已经有了不是吗?</p>
<p>如果现在遇到同样问题的你找到了其他更好的相关问题或者解决方法,请在评论区告知.</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/KNOWN_ISSUES.md#metrics-server-pod-failed-to-reach-running-status">官方文档中的问题</a></li>
<li><a href="https://stackoverflow.com/questions/71843068/metrics-server-is-currently-unable-to-handle-the-request">stackoverflow上有关问题</a></li>
</ul>
<p>我们在 yaml 配置中添加 HostNetwork</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">      hostNetwork: <span class="nb">true</span>  <span class="c1"># ✅ 添加 hostNetwork，避免 CNI 中 10250 访问失败</span>
</span></span><span class="line"><span class="cl">      dnsPolicy: ClusterFirstWithHostNet  <span class="c1"># ✅ 搭配 hostNetwork 使用</span>
</span></span></code></pre></div><p>之后再次查看容器的运行状态:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Readiness probe failed: HTTP probe failed with statuscode: <span class="m">404</span>
</span></span></code></pre></div><p>通过查看日志发现:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo kubectl logs -n kube-system metrics-server-6b8f575f5d-56jm8
</span></span><span class="line"><span class="cl">I0518 11:31:43.086131       <span class="m">1</span> serving.go:374<span class="o">]</span> Generated self-signed cert <span class="o">(</span>/tmp/apiserver.crt, /tmp/apiserver.key<span class="o">)</span>
</span></span><span class="line"><span class="cl">I0518 11:31:45.510186       <span class="m">1</span> handler.go:275<span class="o">]</span> Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
</span></span><span class="line"><span class="cl">I0518 11:31:45.658709       <span class="m">1</span> requestheader_controller.go:169<span class="o">]</span> Starting RequestHeaderAuthRequestController
</span></span><span class="line"><span class="cl">I0518 11:31:45.658803       <span class="m">1</span> shared_informer.go:311<span class="o">]</span> Waiting <span class="k">for</span> caches to sync <span class="k">for</span> RequestHeaderAuthRequestController
</span></span><span class="line"><span class="cl">I0518 11:31:45.659011       <span class="m">1</span> configmap_cafile_content.go:202<span class="o">]</span> <span class="s2">&#34;Starting controller&#34;</span> <span class="nv">name</span><span class="o">=</span><span class="s2">&#34;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file&#34;</span>
</span></span><span class="line"><span class="cl">I0518 11:31:45.659138       <span class="m">1</span> shared_informer.go:311<span class="o">]</span> Waiting <span class="k">for</span> caches to sync <span class="k">for</span> client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
</span></span><span class="line"><span class="cl">I0518 11:31:45.659169       <span class="m">1</span> configmap_cafile_content.go:202<span class="o">]</span> <span class="s2">&#34;Starting controller&#34;</span> <span class="nv">name</span><span class="o">=</span><span class="s2">&#34;client-ca::kube-system::extension-apiserver-authentication::client-ca-file&#34;</span>
</span></span><span class="line"><span class="cl">I0518 11:31:45.659247       <span class="m">1</span> shared_informer.go:311<span class="o">]</span> Waiting <span class="k">for</span> caches to sync <span class="k">for</span> client-ca::kube-system::extension-apiserver-authentication::client-ca-file
</span></span><span class="line"><span class="cl">I0518 11:31:45.660929       <span class="m">1</span> dynamic_serving_content.go:132<span class="o">]</span> <span class="s2">&#34;Starting controller&#34;</span> <span class="nv">name</span><span class="o">=</span><span class="s2">&#34;serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key&#34;</span>
</span></span><span class="line"><span class="cl">I0518 11:31:45.662143       <span class="m">1</span> secure_serving.go:213<span class="o">]</span> Serving securely on <span class="o">[</span>::<span class="o">]</span>:4443
</span></span><span class="line"><span class="cl">I0518 11:31:45.662329       <span class="m">1</span> tlsconfig.go:240<span class="o">]</span> <span class="s2">&#34;Starting DynamicServingCertificateController&#34;</span>
</span></span><span class="line"><span class="cl">I0518 11:31:45.759559       <span class="m">1</span> shared_informer.go:318<span class="o">]</span> Caches are synced <span class="k">for</span> client-ca::kube-system::extension-apiserver-authentication::client-ca-file
</span></span><span class="line"><span class="cl">I0518 11:31:45.759641       <span class="m">1</span> shared_informer.go:318<span class="o">]</span> Caches are synced <span class="k">for</span> client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
</span></span><span class="line"><span class="cl">I0518 11:31:45.759584       <span class="m">1</span> shared_informer.go:318<span class="o">]</span> Caches are synced <span class="k">for</span> RequestHeaderAuthRequestController
</span></span></code></pre></div><p>其并没有直接显示一些访问地址失败的信息.</p>
<p>接着我们尝试 <code>kubectl edit deployment -n kube-system metrics-server</code>修改配置
将 livenessProbe 和 readinessProbe 及其有关配置删除
相关解决方案来自 <a href="https://blog.csdn.net/qq_14997473/article/details/113521950">CSDN</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">readinessProbe:
</span></span><span class="line"><span class="cl">  httpGet:
</span></span><span class="line"><span class="cl">    path: /readyz
</span></span><span class="line"><span class="cl">    port: <span class="m">4443</span>
</span></span><span class="line"><span class="cl">    scheme: HTTPS
</span></span><span class="line"><span class="cl">  initialDelaySeconds: <span class="m">20</span>
</span></span><span class="line"><span class="cl">  periodSeconds: <span class="m">10</span>
</span></span></code></pre></div><p>再来看看 Pod 结果如何</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kube-system   metrics-server-6c8f74f4c4-fktpw           1/1     Running
</span></span></code></pre></div><p>居然成功了,查看集群信息:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo kubectl top no
</span></span><span class="line"><span class="cl">NAME           CPU<span class="o">(</span>cores<span class="o">)</span>   CPU%   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   MEMORY%   
</span></span><span class="line"><span class="cl">agent197       40m          1%     774Mi           9%        
</span></span><span class="line"><span class="cl">revyos-lpi4a   136m         3%     1502Mi          9%       
</span></span></code></pre></div><p>所以最终我们只需要去掉 readinessProbe即可,不用添加 NetWork 等字段在配置文件中.
这对整体的运行时没有影响的,只是去掉了存活探针检测机制.</p>
<h3 id="probe及可能原因">Probe及可能原因<a hidden class="anchor" aria-hidden="true" href="#probe及可能原因">#</a></h3>
<p>这些 Probe 是来自于 k3s 的系统探针</p>
<ul>
<li>Readiness Probe:检测某个容器是否就绪(就是我们常见的 Ready 字段),是否可以接受流量,如果没有就不能加入到 Service</li>
<li>Liveness Probe:判断容器是否“健康”，未通过探针 → 被重启容器。</li>
<li>Startup Probe:判断容器启动是否完成，配合前两者避免“启动未完成就被探测失败”。</li>
</ul>
<p>所以结合之前的两种失败报错, <code>https://10.42.0.58:10250/readyz</code>,是 metric-server 访问 kubelet 容器,因为 metrics-server 默认会通过 kubelet API 获取节点、容器、Pod 的资源指标数据</p>
<p>后来返回404:</p>
<ul>
<li>可能metrics-server 镜像中并没有暴露 /readyz 这个 HTTP 路径</li>
<li>某些版本只支持 /healthz 或 / 路径</li>
</ul>
<h2 id="prometheus">prometheus<a hidden class="anchor" aria-hidden="true" href="#prometheus">#</a></h2>
<p>prometheus 在 k8s 的集群监控中也是十分常见的工具,可抓取 所有组件、应用、Exporter 的细粒度指标,并且可以做到报警,可视化,长期持久化等功能.</p>
<p>所以我们测试一下在我们的集群中使用情况.</p>
<h3 id="操作流程">操作流程<a hidden class="anchor" aria-hidden="true" href="#操作流程">#</a></h3>
<p>与 metric-server 不同的是,prometheus 支持 riscv 架构,所以我们可以直接使用.</p>
<p>在<a href="https://prometheus.io/download">官网</a>下载好安装包传到开发板 server 节点上,进行解压,并将可执行文件和配置文件都放到对应目录中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo cp ~/prometheus-2.53.4.linux-riscv64/prometheus /usr/local/bin/prometheus
</span></span><span class="line"><span class="cl">sudo cp ~/prometheus-2.53.4.linux-riscv64/prometheus.yml /etc/prometheus/prometheus.yml
</span></span></code></pre></div><p>为其编写 service 文件,交由 systemd 管理</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Prometheus Monitoring
</span></span><span class="line"><span class="cl"><span class="nv">Wants</span><span class="o">=</span>network-online.target
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network-online.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">User</span><span class="o">=</span>debian 
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/prometheus <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --config.file<span class="o">=</span>/etc/prometheus/prometheus.yml <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --storage.tsdb.path<span class="o">=</span>/var/lib/prometheus <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --web.listen-address<span class="o">=</span>0.0.0.0:9090 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --web.enable-lifecycle
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>on-failure
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span></code></pre></div><p>这里的 User 可以选择为 prometheus 独立创建一个用户;数据存储在 <code>/var/lib/prometheus</code> 下</p>
<p>同时创建对应的目录并为其赋予对应权限.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir -p /etc/prometheus /var/lib/prometheus
</span></span><span class="line"><span class="cl">sudo chown -R jimlt:jimlt /etc/prometheus /var/lib/prometheus
</span></span></code></pre></div><p>重新加载 systemd 并启动</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl daemon-reexec
</span></span><span class="line"><span class="cl">sudo systemctl daemon-reload
</span></span><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> prometheus
</span></span><span class="line"><span class="cl">sudo systemctl start prometheus
</span></span></code></pre></div><p>验证其状态:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl status prometheus
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl">prometheus.service - Prometheus Monitoring
</span></span><span class="line"><span class="cl">     Loaded: loaded <span class="o">(</span>/etc/systemd/system/prometheus.service<span class="p">;</span> enabled<span class="p">;</span> preset: enabled<span class="o">)</span>
</span></span><span class="line"><span class="cl">     Active: active <span class="o">(</span>running<span class="o">)</span> since Sun 2025-05-18 20:59:08 CST<span class="p">;</span> 12min ago
</span></span><span class="line"><span class="cl"> Invocation: a99f1274a13a4ae89ecb9726ece102a4
</span></span><span class="line"><span class="cl">   Main PID: <span class="m">5337</span> <span class="o">(</span>prometheus<span class="o">)</span>
</span></span><span class="line"><span class="cl">      Tasks: <span class="m">10</span> <span class="o">(</span>limit: 18020<span class="o">)</span>
</span></span><span class="line"><span class="cl">     Memory: 41.2M <span class="o">(</span>peak: 41.9M<span class="o">)</span>
</span></span><span class="line"><span class="cl">        CPU: 5.667s
</span></span><span class="line"><span class="cl">     CGroup: /system.slice/prometheus.service
</span></span><span class="line"><span class="cl">             └─5337 /usr/local/bin/prometheus --config.file<span class="o">=</span>/etc/prometheus/prometheus.yml --storage.tsdb.path<span class="o">=</span>/var/lib/prometheus --web.listen-address<span class="o">=</span>0.0.0.0:9090 --web.enable-&gt;
</span></span></code></pre></div><p>此时我们就可以去浏览器中输入 <code>localhost:9000</code> 访问其监控页面.</p>
<p>关于其安装部署流程就告一段落了,后续我们可以在其配置文件中添加 job,让其监控集群中的数据,关于其后续的更深入操作,见以后的文章吧!</p>
<h2 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h2>
<p>本文尝试了 k3s 集群中两种监控方案的实现,并解决了 metric-server 运行过程中的探针问题.为后续集群的监控奠定了基础.</p>
<p><strong>这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。</strong></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/k3s/">K3s</a></li>
      <li><a href="http://localhost:1313/tags/riscv/">RiscV</a></li>
      <li><a href="http://localhost:1313/tags/%E7%9B%91%E6%8E%A7/">监控</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/039mit6.824lab1/">
    <span class="title">« 上一页</span>
    <br>
    <span>Mit6.824Lab1流程梳理</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/037%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9Cping%E5%91%BD%E4%BB%A4/">
    <span class="title">下一页 »</span>
    <br>
    <span>兴趣八股之计算机网络EP03——Ping命令</span>
  </a>
</nav>

  </footer><div id="tw-comment"></div>

<script>

    

    const getStoredTheme = () => localStorage.getItem("pref-theme") === "light" ? "light" : "dark";

    const setGiscusTheme = () => {

        const sendMessage = (message) => {

            const iframe = document.querySelector('iframe.giscus-frame');

            if (iframe) {

                iframe.contentWindow.postMessage({giscus: message}, 'https://giscus.app');

            }

        }

        sendMessage({setConfig: {theme: getStoredTheme()}})

    }


    document.addEventListener("DOMContentLoaded", () => {

        const giscusAttributes = {

            "src": "https://giscus.app/client.js",

            "data-repo": "LTXWorld\/LTXWorld.github.io",

            "data-repo-id": "R_kgDONODUuA",

            "data-category": "Announcements",

            "data-category-id": "DIC_kwDONODUuM4CkMUw",

            "data-mapping": "pathname",

            "data-strict": "0",

            "data-reactions-enabled": "1",

            "data-emit-metadata": "0",

            "data-input-position": "bottom",

            "data-theme": getStoredTheme(),

            "data-lang": "zh-CN",

            "data-loading": "lazy",

            "crossorigin": "anonymous",

        };


        

        const giscusScript = document.createElement("script");

        Object.entries(giscusAttributes).forEach(

                ([key, value]) => giscusScript.setAttribute(key, value));

        document.querySelector("#tw-comment").appendChild(giscusScript);


        

        const themeSwitcher = document.querySelector("#theme-toggle");

        if (themeSwitcher) {

            themeSwitcher.addEventListener("click", setGiscusTheme);

        }

        const themeFloatSwitcher = document.querySelector("#theme-toggle-float");

        if (themeFloatSwitcher) {

            themeFloatSwitcher.addEventListener("click", setGiscusTheme);

        }

    });

</script>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://LTXWorld.github.io/">©2025 LTX&rsquo;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
