<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>K3sEP05——开发板上k3s存在的问题02_local-path-provisioner | LTX's Blog</title>
<meta name=keywords content="K3s,local-storage,RiscV"><meta name=description content="引子
接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。
kubectl get pods -A -o wide
NAMESPACE     NAME                                      READY   STATUS             RESTARTS        AGE    IP           NODE                NOMINATED NODE   READINESS GATES
default       redis-696579c6c8-v2wns                    1/1     Running            4 (4m10s ago)   7d6h   10.42.0.17   openeuler-riscv64   <none>           <none>
kube-system   helm-install-traefik-pv4hv                0/1     ImagePullBackOff   0               14d    10.42.0.20   openeuler-riscv64   <none>           <none>
kube-system   local-path-provisioner-7b7dc8d6f5-48jjf   0/1     ImagePullBackOff   0               14d    10.42.0.19   openeuler-riscv64   <none>           <none>
kube-system   helm-install-traefik-crd-ktfth            0/1     ImagePullBackOff   0               14d    10.42.0.21   openeuler-riscv64   <none>           <none>
kube-system   metrics-server-668d979685-jthzj           0/1     ImagePullBackOff   0               14d    10.42.0.22   openeuler-riscv64   <none>           <none>
kube-system   coredns-5f8bb7cf9f-5h5kj                  0/1     Running            0               11s    10.42.0.36   openeuler-riscv64   <none>           <none>
可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running，
只是还存在网络问题没有解决,所以并没有 Ready."><meta name=author content="LTX"><link rel=canonical href=https://LTXWorld.github.io/posts/020k3sep05%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/><link crossorigin=anonymous href=/assets/css/stylesheet.725ec5b3e73dcbb0ff6543a8256cd49aa2c4c02c9e0586f006beb2e1f1dec7f2.css integrity="sha256-cl7Fs+c9y7D/ZUOoJWzUmqLEwCyeBYbwBr6y4fHex/I=" rel="preload stylesheet" as=style><link rel=icon href=https://LTXWorld.github.io/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://LTXWorld.github.io/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://LTXWorld.github.io/favicon.png><link rel=apple-touch-icon href=https://LTXWorld.github.io/favicon.png><link rel=mask-icon href=https://LTXWorld.github.io/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://LTXWorld.github.io/posts/020k3sep05%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script type=text/javascript async src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><meta property="og:url" content="https://LTXWorld.github.io/posts/020k3sep05%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/"><meta property="og:site_name" content="LTX's Blog"><meta property="og:title" content="K3sEP05——开发板上k3s存在的问题02_local-path-provisioner"><meta property="og:description" content="引子 接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。
kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default redis-696579c6c8-v2wns 1/1 Running 4 (4m10s ago) 7d6h 10.42.0.17 openeuler-riscv64 <none> <none> kube-system helm-install-traefik-pv4hv 0/1 ImagePullBackOff 0 14d 10.42.0.20 openeuler-riscv64 <none> <none> kube-system local-path-provisioner-7b7dc8d6f5-48jjf 0/1 ImagePullBackOff 0 14d 10.42.0.19 openeuler-riscv64 <none> <none> kube-system helm-install-traefik-crd-ktfth 0/1 ImagePullBackOff 0 14d 10.42.0.21 openeuler-riscv64 <none> <none> kube-system metrics-server-668d979685-jthzj 0/1 ImagePullBackOff 0 14d 10.42.0.22 openeuler-riscv64 <none> <none> kube-system coredns-5f8bb7cf9f-5h5kj 0/1 Running 0 11s 10.42.0.36 openeuler-riscv64 <none> <none> 可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running， 只是还存在网络问题没有解决,所以并没有 Ready."><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-15T16:14:08+08:00"><meta property="article:modified_time" content="2025-04-15T16:14:08+08:00"><meta property="article:tag" content="K3s"><meta property="article:tag" content="Local-Storage"><meta property="article:tag" content="RiscV"><meta property="og:image" content="https://LTXWorld.github.io/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://LTXWorld.github.io/images/papermod-cover.png"><meta name=twitter:title content="K3sEP05——开发板上k3s存在的问题02_local-path-provisioner"><meta name=twitter:description content="引子
接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。
kubectl get pods -A -o wide
NAMESPACE     NAME                                      READY   STATUS             RESTARTS        AGE    IP           NODE                NOMINATED NODE   READINESS GATES
default       redis-696579c6c8-v2wns                    1/1     Running            4 (4m10s ago)   7d6h   10.42.0.17   openeuler-riscv64   <none>           <none>
kube-system   helm-install-traefik-pv4hv                0/1     ImagePullBackOff   0               14d    10.42.0.20   openeuler-riscv64   <none>           <none>
kube-system   local-path-provisioner-7b7dc8d6f5-48jjf   0/1     ImagePullBackOff   0               14d    10.42.0.19   openeuler-riscv64   <none>           <none>
kube-system   helm-install-traefik-crd-ktfth            0/1     ImagePullBackOff   0               14d    10.42.0.21   openeuler-riscv64   <none>           <none>
kube-system   metrics-server-668d979685-jthzj           0/1     ImagePullBackOff   0               14d    10.42.0.22   openeuler-riscv64   <none>           <none>
kube-system   coredns-5f8bb7cf9f-5h5kj                  0/1     Running            0               11s    10.42.0.36   openeuler-riscv64   <none>           <none>
可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running，
只是还存在网络问题没有解决,所以并没有 Ready."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://LTXWorld.github.io/posts/"},{"@type":"ListItem","position":2,"name":"K3sEP05——开发板上k3s存在的问题02_local-path-provisioner","item":"https://LTXWorld.github.io/posts/020k3sep05%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"K3sEP05——开发板上k3s存在的问题02_local-path-provisioner","name":"K3sEP05——开发板上k3s存在的问题02_local-path-provisioner","description":"引子 接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。\nkubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default redis-696579c6c8-v2wns 1/1 Running 4 (4m10s ago) 7d6h 10.42.0.17 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system helm-install-traefik-pv4hv 0/1 ImagePullBackOff 0 14d 10.42.0.20 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system local-path-provisioner-7b7dc8d6f5-48jjf 0/1 ImagePullBackOff 0 14d 10.42.0.19 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system helm-install-traefik-crd-ktfth 0/1 ImagePullBackOff 0 14d 10.42.0.21 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system metrics-server-668d979685-jthzj 0/1 ImagePullBackOff 0 14d 10.42.0.22 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-5f8bb7cf9f-5h5kj 0/1 Running 0 11s 10.42.0.36 openeuler-riscv64 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running， 只是还存在网络问题没有解决,所以并没有 Ready.\n","keywords":["K3s","local-storage","RiscV"],"articleBody":"引子 接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。\nkubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default redis-696579c6c8-v2wns 1/1 Running 4 (4m10s ago) 7d6h 10.42.0.17 openeuler-riscv64 kube-system helm-install-traefik-pv4hv 0/1 ImagePullBackOff 0 14d 10.42.0.20 openeuler-riscv64 kube-system local-path-provisioner-7b7dc8d6f5-48jjf 0/1 ImagePullBackOff 0 14d 10.42.0.19 openeuler-riscv64 kube-system helm-install-traefik-crd-ktfth 0/1 ImagePullBackOff 0 14d 10.42.0.21 openeuler-riscv64 kube-system metrics-server-668d979685-jthzj 0/1 ImagePullBackOff 0 14d 10.42.0.22 openeuler-riscv64 kube-system coredns-5f8bb7cf9f-5h5kj 0/1 Running 0 11s 10.42.0.36 openeuler-riscv64 可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running， 只是还存在网络问题没有解决,所以并没有 Ready.\n所以这次我们来解决其他 local-path-provisioner 的问题。\n什么是local-path-provisioner 官方的介绍是:Dynamically provisioning persistent local storage with Kubernetes\n众所周知，如果我们起一个 Pod，里面的容器一定是需要挂载外部的存储目录的，否则容器一旦销毁那么相关的数据也不在了，无法做到持久化存储，而挂载外部存储目录本质上要求 PVC——持久化卷声明，举例说明：\nvolumeMounts: - mountPath: /data name: redis-storage volumes: - name: redis-storage persistentVolumeClaim: claimName: redis-data 这里我们声明了具体的 pvc persistentVolumeClaim:claimName: redis-data,这是一个真正的磁盘路径，见下方 PVC 的声明写法。\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: redis-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 一旦写了 PVC，local-path-provisioner 就会起作用：它自动为你的 PVC 创建一个目录, 如 /opt/local-path-provisioner/pvc-xxx/,将此目录映射给容器作为持久化的目录。 所以，这个镜像的修复是至关重要的一步，没有它我们就无法进行数据的持久化。\n修复问题 与CoreDNS不同的是，这次在官方的 release 界面并没有发现其针对于riscv架构的二进制文件，所以我们得尝试自己构建了。\n(后续更新,发现其已经支持了 RiscV 架构,只是没有跑通流水线部署,但是代码中已经 Commit 了,所以可以直接使用了)\n拉取到其源代码之后在我的 mac 本机进行构建\n1.构建自定义镜像 git clone git@github.com:rancher/local-path-provisioner.git cd local-path-provisioner GOOS=linux GOARCH=riscv64 CGO_ENABLED=0 go build -o local-path-provisioner 构建成功，这证明其中并没有不适配riscv的地方（例如coredns中构建时出现过的syscall不支持问题）得到一个local-path-provisioner的二进制文件，我们可以将这个二进制文件封装在一个Dockerfile中构建其自定义的镜像来代替默认的镜像。\ndockerfile如下:注意拷贝的路径选择为/usr/bin\nFROM alpine:latest # 使用阿里源（可选） RUN sed -i 's#https\\?://dl-cdn.alpinelinux.org/alpine#http://mirrors.aliyun.com/alpine#g' /etc/apk/repositories # 拷贝编译好的可执行文件 COPY local-path-provisioner-riscv64 /usr/bin/local-path-provisioner # 启动命令 ENTRYPOINT [\"local-path-provisioner\"] 如果不使用/usr/bin的话会遇到问题:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 79s default-scheduler Successfully assigned kube-system/local-path-provisioner-5cdfcc7d9c-hpqnm to openeuler-riscv64 Normal Pulled 35s (x4 over 79s) kubelet Container image \"ltx/local-path-provisioner:v0.0.21\" already present on machine Normal Created 35s (x4 over 79s) kubelet Created container local-path-provisioner Warning Failed 35s (x4 over 79s) kubelet Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"local-path-provisioner\": executable file not found in $PATH: unknown Warning BackOff 9s (x7 over 78s) kubelet Back-off restarting failed container 构建镜像并推送至本地的私有镜像仓库。\ndocker build --network host --platform=linux/riscv64 -f Dockerfile.local-path-provisioner-riscv64 -t 192.168.173.76:6000/riscv64/local-path-provisioner:1.1 . docker push 192.168.173.76:6000/riscv64/local-path-provisioner:1.1 2.拉取镜像修改配置文件 在 riscv 开发板进行拉取镜像并打标签至默认 local-storage 中指明的镜像名称。\ncrictl pull riscv64/local-path-provisioner:1.1 ctr -n k8s.io images tag docker.io/riscv64/local-path-provisioner:1.1 docker.io/rancher/local-path-provisioner:v.0.0.21 可以使用 kubectl -n kube-system edit deployment local-path-provisioner 检查其Deployment文件;\n或者与coreDNS相同,进入 /var/lib/rancher/k3s/server/manifests 目录下查看其 local-storage.yaml 文件. 这是local-path-provisioner的部署文件。\n如果未能识别到本地镜像，根据这个回答，我们只要修改前缀不是rancher的就可以识别到本地的镜像\n接下来与 coreDNS 相同，这是系统级别的镜像，为了防止每次重启都被覆盖，需要在k3s启动命令中添加--disable标签. 因为我们使用systemctl来管理k3s，所以需要修改/etc/systemd/system/k3s.service\n在此之前我们要先拷贝一份manifests目录下的yaml文件，修改为我们自定义的custom-local-storage.yaml文件，让之后的k3s识别到这个文件进行容器的构建。\ncp local-storage.yaml custom-local-storage.yaml 由于我们之前已经把标签打成了与默认 yaml 文件中相同名称的镜像，所以这个 custom 文件中不需要修改什么东西。\n回到刚才，继续修改启动命令。\nvi /etc/systemd/system/k3s.service # 在ExecStart中添加 --disable ExecStart=/usr/local/bin/k3s \\ server \\ --disable=local-storage,coredns \\ 这一方法来源于官方文档，或者这篇文章，加上这个标签会使k3s在启动时选择我们设定的自定义yaml文件并且删除默认的yaml文件与构建的容器。\n你还可以选择文章中的方法二，在manifests目录下添加 local-storage.yaml.skip 文件，这样会让k3s跳过某个容器的构建，但我没有测试其是否会去选择我们自定义的配置文件。\n接下来进行删除，检测。最好重启一下k3s,systemctl restart k3s\nkubectl delete pod -n kube-system -l app=local-path-provisioner kubectl get pods -n kube-system -l app=local-path-provisioner kubectl describe pod -n kube-system 总结 经历了 coreDNS 和 local-path-provisioner 这两个系统级别的镜像之后，我开始反思整个适配过程，我觉得应该先去看看如何从头适配，即使 OREV 的兴趣小组先进行了一个基础适配，使得我们可以先下载到 k3s。\ngoogle 后发现在官方的ISSUE中有这个适配问题,其中提到一个巨大挑战是rancher-mirrored 镜像中缺乏对 riscv 平台的支持。\n这也正是我们之前处理的那些镜像，默认是拉取的 rancher 官方维护的镜像副本 rancher-mirrored,我们也对这些镜像做了适配。(后来知道可以修改默认镜像仓库,在启动命令时加上私有的仓库即可)\nWhile we could simply add this to the arch list, this would change the multi-arch digest for any tags that we currently mirror that have this platform available.\n下面也提到虽然可以简单地将 riscv 添加到镜像的架构列表中，但会导致当前镜像的多架构 digest 发生变化。\n镜像的 digest 是镜像的唯一标识，一个哈希值 sha256:xxx 我们平时用的镜像名称，其实就是 tag，指向的是一个多架构的manifest,这个 manifest 会列出所有对应架构的镜像(见下方图像) manifest list 本身是一个 JSON 文件,他也有 digest 标识;如果添加一个 riscv 字段进去，整个 JSON 内容变化导致 digest 变化 很多生产系统,CI/CD 都依赖这个稳定可验证的 digest,这就导致牵一发而动全身, 会重新拉取镜像，打破缓存；某些系统会误以为内容被篡改或更新了，造成意外后果;不利于镜像的同步和管理 更愿意的做法是:不改变现有版本(及之前版本)的镜像 manifest,在新的版本中添加对新架构的支持. We should figure out how to add this platform to only new tags, in a way that is minimally disruptive to the list maintenance process, and can be reused when adding new platforms in the future.\n这是他们提出的解决方法，只将 RISC-V 平台添加到**新标签（新的 manifest,新的镜像）**中，这样对镜像列表的维护干扰最小。\n为了保证镜像的稳定性，K3s 团队不修改已有镜像的多架构配置，而是只给新添加的镜像构建 RISC-V 版本，靠 git blame + 时间区间控制，确保 digest 不变且流程可复用。\n但是后续的讨论就围绕着该如何支持新的架构，官方开发者更倾向于对新的镜像加入新的架构而不去改动老的镜像，这是很好理解的策略。\n接下来我们将探索其他系统级别镜像,例如 Helm,Traefik\n这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。\n后续更新 可以发现 rancher 官方提供了关于 local-path-provisioner 在 riscv 上的镜像.\n所以大佬在 commit 中只是修改了其版本到29.\n故我们只需要将其拉取到本地并推送到私有镜像仓库即可.\n","wordCount":"2675","inLanguage":"zh","image":"https://LTXWorld.github.io/images/papermod-cover.png","datePublished":"2025-04-15T16:14:08+08:00","dateModified":"2025-04-15T16:14:08+08:00","author":{"@type":"Person","name":"LTX"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://LTXWorld.github.io/posts/020k3sep05%E5%BC%80%E5%8F%91%E6%9D%BF%E4%B8%8Ak3s%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%9802/"},"publisher":{"@type":"Organization","name":"LTX's Blog","logo":{"@type":"ImageObject","url":"https://LTXWorld.github.io/favicon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://LTXWorld.github.io/ accesskey=h title="LTX's Blog (Alt + H)">LTX's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://LTXWorld.github.io/ title="LTX's Blog"><span>首页</span></a></li><li><a href=https://LTXWorld.github.io/archives/ title=归档><span>归档</span></a></li><li><a href=https://LTXWorld.github.io/categories/ title=Categories><span>分类</span></a></li><li><a href=https://LTXWorld.github.io/tags/ title=Tags><span>标签</span></a></li><li><a href=https://LTXWorld.github.io/search/ title=搜索><span>搜索</span></a></li><li><a href=https://LTXWorld.github.io/about/ title=后花园><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">K3sEP05——开发板上k3s存在的问题02_local-path-provisioner</h1><div class=post-meta><span title='2025-04-15 16:14:08 +0800 CST'>四月 15, 2025</span>&nbsp;·&nbsp;6 分钟&nbsp;·&nbsp;LTX</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e5%ad%90 aria-label=引子>引子</a></li><li><a href=#%e4%bb%80%e4%b9%88%e6%98%aflocal-path-provisioner aria-label=什么是local-path-provisioner>什么是local-path-provisioner</a></li><li><a href=#%e4%bf%ae%e5%a4%8d%e9%97%ae%e9%a2%98 aria-label=修复问题>修复问题</a><ul><li><a href=#1%e6%9e%84%e5%bb%ba%e8%87%aa%e5%ae%9a%e4%b9%89%e9%95%9c%e5%83%8f aria-label=1.构建自定义镜像>1.构建自定义镜像</a></li><li><a href=#2%e6%8b%89%e5%8f%96%e9%95%9c%e5%83%8f%e4%bf%ae%e6%94%b9%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6 aria-label=2.拉取镜像修改配置文件>2.拉取镜像修改配置文件</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e5%90%8e%e7%bb%ad%e6%9b%b4%e6%96%b0 aria-label=后续更新>后续更新</a></li></ul></div></details></div><div class=post-content><h2 id=引子>引子<a hidden class=anchor aria-hidden=true href=#引子>#</a></h2><p>接着上一篇文章，我们继续解决开发板上的k3s集群遇到的问题，回顾一下之前遇到的问题，使用命令查看当前所有 Pods。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get pods -A -o wide
</span></span><span class=line><span class=cl>NAMESPACE     NAME                                      READY   STATUS             RESTARTS        AGE    IP           NODE                NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>default       redis-696579c6c8-v2wns                    1/1     Running            <span class=m>4</span> <span class=o>(</span>4m10s ago<span class=o>)</span>   7d6h   10.42.0.17   openeuler-riscv64   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system   helm-install-traefik-pv4hv                0/1     ImagePullBackOff   <span class=m>0</span>               14d    10.42.0.20   openeuler-riscv64   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system   local-path-provisioner-7b7dc8d6f5-48jjf   0/1     ImagePullBackOff   <span class=m>0</span>               14d    10.42.0.19   openeuler-riscv64   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system   helm-install-traefik-crd-ktfth            0/1     ImagePullBackOff   <span class=m>0</span>               14d    10.42.0.21   openeuler-riscv64   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system   metrics-server-668d979685-jthzj           0/1     ImagePullBackOff   <span class=m>0</span>               14d    10.42.0.22   openeuler-riscv64   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system   coredns-5f8bb7cf9f-5h5kj                  0/1     Running            <span class=m>0</span>               11s    10.42.0.36   openeuler-riscv64   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><p>可以发现在 kube-system 命名空间下的这些 Pod 还是处于 ImagePullBackOff 阶段，只有我们上次解决的 CoreDNS 是 Running，
只是还存在网络问题没有解决,所以并没有 Ready.</p><p>所以这次我们来解决其他 local-path-provisioner 的问题。</p><h2 id=什么是local-path-provisioner>什么是local-path-provisioner<a hidden class=anchor aria-hidden=true href=#什么是local-path-provisioner>#</a></h2><p>官方的介绍是:<strong>Dynamically provisioning persistent local storage with Kubernetes</strong></p><p>众所周知，如果我们起一个 Pod，里面的容器一定是需要挂载外部的存储目录的，否则容器一旦销毁那么相关的数据也不在了，无法做到持久化存储，而挂载外部存储目录本质上要求 PVC——持久化卷声明，举例说明：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>volumeMounts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=l>/data</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>redis-storage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>redis-storage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>persistentVolumeClaim</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>claimName</span><span class=p>:</span><span class=w> </span><span class=l>redis-data</span><span class=w>
</span></span></span></code></pre></div><p>这里我们声明了具体的 pvc <code>persistentVolumeClaim:claimName: redis-data</code>,这是一个真正的磁盘路径，见下方 PVC 的声明写法。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>PersistentVolumeClaim</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>redis-data</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>accessModes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>ReadWriteOnce</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>storage</span><span class=p>:</span><span class=w> </span><span class=l>1Gi</span><span class=w>
</span></span></span></code></pre></div><p>一旦写了 PVC，local-path-provisioner 就会起作用：它自动为你的 PVC 创建一个目录,
如 <code>/opt/local-path-provisioner/pvc-xxx/</code>,将此目录映射给容器作为持久化的目录。
所以，这个镜像的修复是至关重要的一步，没有它我们就无法进行数据的持久化。</p><h2 id=修复问题>修复问题<a hidden class=anchor aria-hidden=true href=#修复问题>#</a></h2><p>与CoreDNS不同的是，这次在官方的 release 界面并没有发现其针对于riscv架构的二进制文件，所以我们得尝试自己构建了。</p><p>(后续更新,发现其已经支持了 RiscV 架构,只是没有跑通流水线部署,但是代码中已经 Commit 了,所以可以直接使用了)</p><p>拉取到其源代码之后在我的 mac 本机进行构建</p><h3 id=1构建自定义镜像>1.构建自定义镜像<a hidden class=anchor aria-hidden=true href=#1构建自定义镜像>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone git@github.com:rancher/local-path-provisioner.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> local-path-provisioner
</span></span><span class=line><span class=cl><span class=nv>GOOS</span><span class=o>=</span>linux <span class=nv>GOARCH</span><span class=o>=</span>riscv64 <span class=nv>CGO_ENABLED</span><span class=o>=</span><span class=m>0</span> go build -o local-path-provisioner
</span></span></code></pre></div><p>构建成功，这证明其中并没有不适配riscv的地方（例如coredns中构建时出现过的syscall不支持问题）得到一个local-path-provisioner的二进制文件，我们可以将这个二进制文件封装在一个Dockerfile中构建其自定义的镜像来代替默认的镜像。</p><p>dockerfile如下:注意拷贝的路径选择为<code>/usr/bin</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=s> alpine:latest</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 使用阿里源（可选）</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> sed -i <span class=s1>&#39;s#https\?://dl-cdn.alpinelinux.org/alpine#http://mirrors.aliyun.com/alpine#g&#39;</span> /etc/apk/repositories<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 拷贝编译好的可执行文件</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> local-path-provisioner-riscv64 /usr/bin/local-path-provisioner<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 启动命令</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENTRYPOINT</span> <span class=p>[</span><span class=s2>&#34;local-path-provisioner&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><p>如果不使用<code>/usr/bin</code>的话会遇到问题:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Events:
</span></span><span class=line><span class=cl>  Type     Reason     Age                From               Message
</span></span><span class=line><span class=cl>  ----     ------     ----               ----               -------
</span></span><span class=line><span class=cl>  Normal   Scheduled  79s                default-scheduler  Successfully assigned kube-system/local-path-provisioner-5cdfcc7d9c-hpqnm to openeuler-riscv64
</span></span><span class=line><span class=cl>  Normal   Pulled     35s <span class=o>(</span>x4 over 79s<span class=o>)</span>  kubelet            Container image <span class=s2>&#34;ltx/local-path-provisioner:v0.0.21&#34;</span> already present on machine
</span></span><span class=line><span class=cl>  Normal   Created    35s <span class=o>(</span>x4 over 79s<span class=o>)</span>  kubelet            Created container local-path-provisioner
</span></span><span class=line><span class=cl>  Warning  Failed     35s <span class=o>(</span>x4 over 79s<span class=o>)</span>  kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: <span class=s2>&#34;local-path-provisioner&#34;</span>: executable file not found in <span class=nv>$PATH</span>: unknown
</span></span><span class=line><span class=cl>  Warning  BackOff    9s <span class=o>(</span>x7 over 78s<span class=o>)</span>   kubelet            Back-off restarting failed container
</span></span></code></pre></div><p>构建镜像并推送至本地的私有镜像仓库。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker build --network host --platform<span class=o>=</span>linux/riscv64 -f Dockerfile.local-path-provisioner-riscv64 -t 192.168.173.76:6000/riscv64/local-path-provisioner:1.1 .
</span></span><span class=line><span class=cl>docker push 192.168.173.76:6000/riscv64/local-path-provisioner:1.1
</span></span></code></pre></div><h3 id=2拉取镜像修改配置文件>2.拉取镜像修改配置文件<a hidden class=anchor aria-hidden=true href=#2拉取镜像修改配置文件>#</a></h3><p>在 riscv 开发板进行拉取镜像并打标签至默认 local-storage 中指明的镜像名称。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>crictl pull riscv64/local-path-provisioner:1.1
</span></span><span class=line><span class=cl>ctr -n k8s.io images tag docker.io/riscv64/local-path-provisioner:1.1 docker.io/rancher/local-path-provisioner:v.0.0.21
</span></span></code></pre></div><p>可以使用 <code>kubectl -n kube-system edit deployment local-path-provisioner</code> 检查其Deployment文件;</p><p>或者与coreDNS相同,进入 <code>/var/lib/rancher/k3s/server/manifests</code> 目录下查看其 <code>local-storage.yaml</code> 文件.
这是local-path-provisioner的部署文件。</p><p>如果未能识别到本地镜像，根据这个<a href=https://github.com/k3s-io/k3s/issues/7015>回答</a>，我们只要修改前缀不是rancher的就可以识别到本地的镜像</p><p>接下来与 coreDNS 相同，这是系统级别的镜像，为了防止每次重启都被覆盖，需要在k3s启动命令中添加<code>--disable</code>标签.
因为我们使用systemctl来管理k3s，所以需要修改<code>/etc/systemd/system/k3s.service</code></p><p>在此之前我们要先拷贝一份manifests目录下的yaml文件，修改为我们自定义的custom-local-storage.yaml文件，让之后的k3s识别到这个文件进行容器的构建。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cp local-storage.yaml custom-local-storage.yaml
</span></span></code></pre></div><p>由于我们之前已经把标签打成了与默认 yaml 文件中相同名称的镜像，所以这个 custom 文件中不需要修改什么东西。</p><p>回到刚才，继续修改启动命令。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vi /etc/systemd/system/k3s.service
</span></span><span class=line><span class=cl><span class=c1># 在ExecStart中添加 --disable</span>
</span></span><span class=line><span class=cl><span class=nv>ExecStart</span><span class=o>=</span>/usr/local/bin/k3s <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    server <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --disable<span class=o>=</span>local-storage,coredns <span class=se>\
</span></span></span></code></pre></div><p>这一方法来源于官方文档，或者<a href=https://medium.com/@bilsted/how-to-disable-traefik-and-metrics-server-on-k3s-b7dab4ca3aae>这篇文章</a>，加上这个标签会使k3s在启动时选择我们设定的自定义yaml文件并且删除默认的yaml文件与构建的容器。</p><p>你还可以选择文章中的方法二，在manifests目录下添加 <code>local-storage.yaml.skip</code> 文件，这样会让k3s跳过某个容器的构建，但我没有测试其是否会去选择我们自定义的配置文件。</p><p>接下来进行删除，检测。最好重启一下k3s,<code>systemctl restart k3s</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl delete pod -n kube-system -l <span class=nv>app</span><span class=o>=</span>local-path-provisioner
</span></span><span class=line><span class=cl>kubectl get pods -n kube-system -l <span class=nv>app</span><span class=o>=</span>local-path-provisioner
</span></span><span class=line><span class=cl>kubectl describe pod &lt;PodName&gt; -n kube-system
</span></span></code></pre></div><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>经历了 coreDNS 和 local-path-provisioner 这两个系统级别的镜像之后，我开始反思整个适配过程，我觉得应该先去看看如何从头适配，即使 OREV 的兴趣小组先进行了一个基础适配，使得我们可以先下载到 k3s。</p><p>google 后发现在官方的ISSUE中有这个<a href=https://github.com/rancher/image-mirror/issues/502>适配问题</a>,其中提到一个巨大挑战是rancher-mirrored 镜像中缺乏对 riscv 平台的支持。</p><p>这也正是我们之前处理的那些镜像，默认是拉取的 rancher 官方维护的镜像副本 <strong>rancher-mirrored</strong>,我们也对这些镜像做了适配。(后来知道可以修改默认镜像仓库,在启动命令时加上私有的仓库即可)</p><blockquote><p>While we could simply add this to the arch list, this would change the multi-arch digest for any tags that we currently mirror that have this platform available.</p></blockquote><p>下面也提到虽然可以简单地将 riscv 添加到镜像的架构列表中，但会导致当前镜像的多架构 digest 发生变化。</p><ul><li>镜像的 digest 是镜像的唯一标识，一个哈希值 <code>sha256:xxx</code></li><li>我们平时用的镜像名称，其实就是 tag，指向的是一个<strong>多架构的manifest</strong>,这个 manifest 会列出所有对应架构的镜像(见下方图像)</li><li>manifest list 本身是一个 JSON 文件,他也有 digest 标识;如果添加一个 riscv 字段进去，整个 JSON 内容变化导致 digest 变化</li><li>很多生产系统,CI/CD 都依赖这个稳定可验证的 digest,这就导致牵一发而动全身, 会重新拉取镜像，打破缓存；某些系统会误以为内容被篡改或更新了，造成意外后果;不利于镜像的同步和管理</li><li>更愿意的做法是:<strong>不改变现有版本(及之前版本)的镜像 manifest,在新的版本中添加对新架构的支持</strong>.</li></ul><p><img alt=2 loading=lazy src=/img/riscv/manifest.png></p><blockquote><p>We should figure out how to add this platform to only new tags, in a way that is minimally disruptive to the list maintenance process, and can be reused when adding new platforms in the future.</p></blockquote><p>这是他们提出的解决方法，只将 RISC-V 平台添加到**新标签（新的 manifest,新的镜像）**中，这样对镜像列表的维护干扰最小。</p><p>为了保证镜像的稳定性，K3s 团队不修改已有镜像的多架构配置，而是只给新添加的镜像构建 RISC-V 版本，靠 git blame + 时间区间控制，确保 digest 不变且流程可复用。</p><p>但是后续的讨论就围绕着该如何支持新的架构，官方开发者更倾向于对新的镜像加入新的架构而不去改动老的镜像，这是很好理解的策略。</p><p>接下来我们将探索其他系统级别镜像,例如 Helm,Traefik</p><p><strong>这里是LTX，感谢您阅读这篇博客，人生海海，和自己对话，像只蝴蝶纵横四海。</strong></p><h2 id=后续更新>后续更新<a hidden class=anchor aria-hidden=true href=#后续更新>#</a></h2><p>可以发现 rancher 官方提供了关于 local-path-provisioner 在 riscv 上的镜像.</p><p><img alt=3 loading=lazy src=/img/riscv/local1.png></p><p>所以大佬在 commit 中只是修改了其版本到29.</p><p>故我们只需要将其拉取到本地并推送到私有镜像仓库即可.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://LTXWorld.github.io/tags/k3s/>K3s</a></li><li><a href=https://LTXWorld.github.io/tags/local-storage/>Local-Storage</a></li><li><a href=https://LTXWorld.github.io/tags/riscv/>RiscV</a></li></ul><nav class=paginav><a class=prev href=https://LTXWorld.github.io/posts/021%E5%A5%87%E6%80%9D%E4%B9%B1%E6%83%B3ep02%E6%B6%88%E8%B4%B9%E8%A7%82%E4%B8%8E%E9%98%85%E8%AF%BB/><span class=title>« 上一页</span><br><span>LT的奇思乱想EP02精气神——消费观与阅读</span>
</a><a class=next href=https://LTXWorld.github.io/posts/019golangep03_slice%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/><span class=title>下一页 »</span><br><span>GolangEP03_slice底层原理及注意事项</span></a></nav></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"LTXWorld/LTXWorld.github.io","data-repo-id":"R_kgDONODUuA","data-category":"Announcements","data-category-id":"DIC_kwDONODUuM4CkMUw","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"zh-CN","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span><a href=https://LTXWorld.github.io/>©2025 LTX&rsquo;s Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div class=ltx-music-player id=ltx-music-player role=group aria-label=背景音乐播放器><audio id=ltx-music-audio preload=metadata></audio><div class=ltx-drag-handle data-role=drag-handle title=上下拖动调整位置></div><div class=ltx-music-header><div class=ltx-music-visual><div class=ltx-disc data-role=disc aria-hidden=true><img data-role=cover alt loading=lazy decoding=async><div class=ltx-disc-center></div></div></div><div class=ltx-music-meta><span class=ltx-music-title data-role=title></span>
<span class=ltx-music-artist data-role=artist></span></div><button type=button class=ltx-music-hide data-action=collapse aria-label=隐藏播放器>&#215;</button></div><div class=ltx-music-progress data-role=progress aria-label=进度条 role=slider><div class=ltx-music-progress-fill data-role=progress-fill></div></div><div class=ltx-music-time><span data-role=current>0:00</span>
<span data-role=total>--:--</span></div><div class=ltx-music-controls><button type=button data-action=prev aria-label=上一首>⏮</button>
<button type=button data-action=toggle aria-label=播放或暂停>
<span data-icon=play>▶</span>
<span data-icon=pause>⏸</span>
</button>
<button type=button data-action=next aria-label=下一首>⏭</button>
<button type=button data-action=mute aria-label=静音或取消静音>
<span data-icon=unmuted>🔊</span>
<span data-icon=muted>🔇</span></button></div><div class=ltx-autoplay-tip data-role=tip aria-live=polite></div></div><button type=button class=ltx-music-fab id=ltx-music-fab aria-label=显示音乐播放器>&#9835;</button>
<script>window.LTX_MUSIC_CONFIG={autoplay:!0,defaultCover:"",loopPlaylist:!0,startVolume:.6,tracks:[{artist:"王力宏",file:"/audio/02 - 依然爱你.mp3",title:"依然爱你"},{artist:"王力宏",file:"/audio/05 - 改变自己.mp3",title:"改变自己"},{artist:"王力宏",file:"/audio/11 - 爱错.mp3",title:"爱错"}]},function(){const a=window.LTX_MUSIC_CONFIG||{};if(!a.tracks||!a.tracks.length)return;const e=document.getElementById("ltx-music-player"),t=e.querySelector("audio"),x=e.querySelector('[data-role="title"]'),O=e.querySelector('[data-role="artist"]'),b=e.querySelector('[data-role="progress"]'),A=e.querySelector('[data-role="progress-fill"]'),F=e.querySelector('[data-role="current"]'),M=e.querySelector('[data-role="total"]'),c=e.querySelector('[data-role="tip"]'),r=e.querySelector('[data-role="cover"]'),S=e.querySelector('[data-action="toggle"]'),y=e.querySelector('[data-action="prev"]'),k=e.querySelector('[data-action="next"]'),E=e.querySelector('[data-action="mute"]'),g=e.querySelector('[data-action="collapse"]'),i=e.querySelector('[data-role="drag-handle"]'),o=document.getElementById("ltx-music-fab"),f=(e,t,n)=>Math.min(Math.max(e,t),n),j=e=>{if(!Number.isFinite(e))return"--:--";const t=Math.floor(e/60),n=Math.floor(e%60);return`${String(t).padStart(1,"0")}:${String(n).padStart(2,"0")}`},C=-120,_=220,w=180,s={active:!1,startY:0,startOffset:0,pointerId:null},n={index:0,tracks:a.tracks,autoplay:a.autoplay!==!1,loopPlaylist:a.loopPlaylist!==!1,offset:0,collapsed:!1},h=t=>{n.offset=f(t,C,_),e.style.setProperty("--ltx-offset",`${n.offset}px`)},l=t=>{const s=Boolean(t);n.collapsed=s,e.classList.toggle("ltx-collapsed",s),s?(e.classList.remove("ltx-dragging"),n.offset=0,e.style.removeProperty("--ltx-offset"),o&&(o.classList.add("ltx-visible"),o.setAttribute("aria-hidden","false"),o.tabIndex=0)):(h(0),o&&(o.classList.remove("ltx-visible"),o.setAttribute("aria-hidden","true"),o.tabIndex=-1))},m=()=>{const n=t.muted||t.volume===0;e.classList.toggle("ltx-muted",n)};t.volume=f(a.startVolume??.6,0,1),m(),l(!1);const d=()=>{const e=t.duration||0,n=t.currentTime||0,s=e?n/e*100:0;A.style.width=`${s}%`,F.textContent=j(n),M.textContent=j(e)},u=(s,o={})=>{const l=n.tracks.length;n.index=(s%l+l)%l;const i=n.tracks[n.index];if(!i||!i.file){console.warn("LTX music player: 音频文件缺失",i),c.textContent="音频文件缺失，请检查配置";return}const u=i.cover||a.defaultCover||"";r&&(u?(r.src=u,r.alt=i.title?`${i.title} 封面`:"音乐封面",e.classList.add("ltx-has-cover")):(r.removeAttribute("src"),r.alt="",e.classList.remove("ltx-has-cover"))),x.textContent=i.title||`Track ${n.index+1}`,O.textContent=i.artist||"",t.src=i.file,e.dataset.trackIndex=n.index,c.textContent="",e.classList.remove("ltx-autoplay-blocked"),d(),o.play&&p()},p=()=>{t.play().then(()=>{e.classList.remove("ltx-autoplay-blocked"),c.textContent=""}).catch(()=>{e.classList.add("ltx-autoplay-blocked"),c.textContent="浏览器阻止了自动播放，请点击播放按钮"})},v=(e=!1)=>{const o=!t.paused||e,s=n.index+1;if(s>=n.tracks.length&&!n.loopPlaylist){t.currentTime=t.duration||0,t.pause();return}u(s,{play:o})},T=()=>{const e=!t.paused;u(n.index-1,{play:e})};if(g&&g.addEventListener("click",()=>l(!0)),o&&(o.addEventListener("click",()=>l(!1)),o.setAttribute("aria-hidden","true"),o.tabIndex=-1),i){i.style.touchAction="none",i.addEventListener("pointerdown",t=>{if(n.collapsed)return;if(s.active=!0,s.pointerId=t.pointerId,s.startY=t.clientY,s.startOffset=n.offset,i.setPointerCapture)try{i.setPointerCapture(t.pointerId)}catch{}e.classList.add("ltx-dragging")}),i.addEventListener("pointermove",e=>{if(!s.active)return;const t=e.clientY-s.startY;h(s.startOffset+t)});const t=()=>{if(!s.active)return;if(s.pointerId!==null&&i.releasePointerCapture)try{i.releasePointerCapture(s.pointerId)}catch{}s.active=!1,s.pointerId=null,e.classList.remove("ltx-dragging"),n.offset>w?l(!0):h(n.offset)};["pointerup","pointercancel"].forEach(e=>{i.addEventListener(e,t)}),i.addEventListener("pointerleave",()=>{if(!s.active)return;t()})}S.addEventListener("click",()=>{t.paused?p():t.pause()}),y.addEventListener("click",T),k.addEventListener("click",()=>v(!1)),E.addEventListener("click",()=>{t.muted=!t.muted,m()}),b.addEventListener("click",e=>{const n=b.getBoundingClientRect(),s=f((e.clientX-n.left)/n.width,0,1);Number.isFinite(t.duration)&&(t.currentTime=s*t.duration)}),t.addEventListener("timeupdate",d),t.addEventListener("loadedmetadata",d),t.addEventListener("play",()=>{e.classList.add("ltx-playing"),e.classList.remove("ltx-autoplay-blocked"),c.textContent=""}),t.addEventListener("pause",()=>{e.classList.remove("ltx-playing")}),t.addEventListener("ended",()=>{v(!0)}),t.addEventListener("volumechange",m),u(0,{play:n.autoplay})}()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>